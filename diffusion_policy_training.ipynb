{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Imports**\n",
    "# diffusion policy import\n",
    "from typing import Tuple, Sequence, Dict, Union, Optional\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import zarr\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# env import\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import pymunk\n",
    "import pymunk.pygame_util\n",
    "from pymunk.space_debug_draw_options import SpaceDebugColor\n",
    "from pymunk.vec2d import Vec2d\n",
    "import shapely.geometry as sg\n",
    "import cv2\n",
    "import skimage.transform as st\n",
    "from skvideo.io import vwrite\n",
    "from IPython.display import Video\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import mani_skill2.envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment PickCube doesn't exist. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@markdown Standard Gym Env (0.21.0 API)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 0. create env object\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#env = PushTEnv()\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPickCube-v0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgbd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpd_joint_delta_pos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 1. seed env for initial state.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Seed 0-200 are used for the demonstration dataset.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#env.seed(1000)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 2. must reset before use\u001b[39;00m\n\u001b[1;32m     12\u001b[0m obs, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m/project/dl2024s/lottv/conda_envs/dl/lib/python3.9/site-packages/gym/envs/registration.py:569\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    564\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m         )\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m         \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    572\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m spec_\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/project/dl2024s/lottv/conda_envs/dl/lib/python3.9/site-packages/gym/envs/registration.py:219\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_env_id(ns, name, version) \u001b[38;5;129;01min\u001b[39;00m registry:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m \u001b[43m_check_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/project/dl2024s/lottv/conda_envs/dl/lib/python3.9/site-packages/gym/envs/registration.py:197\u001b[0m, in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    194\u001b[0m namespace_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in namespace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m suggestion_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you mean: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestion \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mNameNotFound(\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnamespace_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m )\n",
      "\u001b[0;31mNameNotFound\u001b[0m: Environment PickCube doesn't exist. "
     ]
    }
   ],
   "source": [
    "\n",
    "#@markdown Standard Gym Env (0.21.0 API)\n",
    "\n",
    "# 0. create env object\n",
    "#env = PushTEnv()\n",
    "env = gym.make(\"PickCube-v0\", obs_mode=\"rgbd\", control_mode=\"pd_joint_delta_pos\")\n",
    "\n",
    "# 1. seed env for initial state.\n",
    "# Seed 0-200 are used for the demonstration dataset.\n",
    "#env.seed(1000)\n",
    "\n",
    "# 2. must reset before use\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# 3. 2D positional action space [0,512]\n",
    "action = env.action_space.sample()\n",
    "\n",
    "# 4. Standard gym step method\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# prints and explains each dimension of the observation and action vectors\n",
    "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
    "    print(\"Obs: \", repr(obs))\n",
    "    print(\"Obs:        [agent_x,  agent_y,  block_x,  block_y,    block_angle]\")\n",
    "    print(\"Action: \", repr(action))\n",
    "    print(\"Action:   [target_agent_x, target_agent_y]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Dataset**\n",
    "#@markdown\n",
    "#@markdown Defines `PushTStateDataset` and helper functions\n",
    "#@markdown\n",
    "#@markdown The dataset class\n",
    "#@markdown - Load data (obs, action) from a zarr storage\n",
    "#@markdown - Normalizes each dimension of obs and action to [-1,1]\n",
    "#@markdown - Returns\n",
    "#@markdown  - All possible segments with length `pred_horizon`\n",
    "#@markdown  - Pads the beginning and the end of each episode with repetition\n",
    "#@markdown  - key `obs`: shape (obs_horizon, obs_dim)\n",
    "#@markdown  - key `action`: shape (pred_horizon, action_dim)\n",
    "\n",
    "def create_sample_indices(\n",
    "        episode_ends:np.ndarray, sequence_length:int,\n",
    "        pad_before: int=0, pad_after: int=0):\n",
    "    indices = list()\n",
    "    for i in range(len(episode_ends)):\n",
    "        start_idx = 0\n",
    "        if i > 0:\n",
    "            start_idx = episode_ends[i-1]\n",
    "        end_idx = episode_ends[i]\n",
    "        episode_length = end_idx - start_idx\n",
    "\n",
    "        min_start = -pad_before\n",
    "        max_start = episode_length - sequence_length + pad_after\n",
    "\n",
    "        # range stops one idx before end\n",
    "        for idx in range(min_start, max_start+1):\n",
    "            buffer_start_idx = max(idx, 0) + start_idx\n",
    "            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n",
    "            start_offset = buffer_start_idx - (idx+start_idx)\n",
    "            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n",
    "            sample_start_idx = 0 + start_offset\n",
    "            sample_end_idx = sequence_length - end_offset\n",
    "            indices.append([\n",
    "                buffer_start_idx, buffer_end_idx,\n",
    "                sample_start_idx, sample_end_idx])\n",
    "    indices = np.array(indices)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def sample_sequence(train_data, sequence_length,\n",
    "                    buffer_start_idx, buffer_end_idx,\n",
    "                    sample_start_idx, sample_end_idx):\n",
    "    result = dict()\n",
    "    for key, input_arr in train_data.items():\n",
    "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
    "        data = sample\n",
    "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
    "            data = np.zeros(\n",
    "                shape=(sequence_length,) + input_arr.shape[1:],\n",
    "                dtype=input_arr.dtype)\n",
    "            if sample_start_idx > 0:\n",
    "                data[:sample_start_idx] = sample[0]\n",
    "            if sample_end_idx < sequence_length:\n",
    "                data[sample_end_idx:] = sample[-1]\n",
    "            data[sample_start_idx:sample_end_idx] = sample\n",
    "        result[key] = data\n",
    "    return result\n",
    "\n",
    "# normalize data\n",
    "def get_data_stats(data):\n",
    "    data = data.reshape(-1,data.shape[-1])\n",
    "    stats = {\n",
    "        'min': np.min(data, axis=0),\n",
    "        'max': np.max(data, axis=0)\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "def normalize_data(data, stats):\n",
    "    # nomalize to [0,1]\n",
    "    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n",
    "    # normalize to [-1, 1]\n",
    "    ndata = ndata * 2 - 1\n",
    "    return ndata\n",
    "\n",
    "def unnormalize_data(ndata, stats):\n",
    "    ndata = (ndata + 1) / 2\n",
    "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
    "    return data\n",
    "\n",
    "# dataset\n",
    "class PushTStateDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path,\n",
    "                 pred_horizon, obs_horizon, action_horizon):\n",
    "\n",
    "        # read from zarr dataset\n",
    "        dataset_root = zarr.open(dataset_path, 'r')\n",
    "        # All demonstration episodes are concatinated in the first dimension N\n",
    "        train_data = {\n",
    "            # (N, action_dim)\n",
    "            'action': dataset_root['data']['action'][:],\n",
    "            # (N, obs_dim)\n",
    "            'obs': dataset_root['data']['state'][:]\n",
    "        }\n",
    "        # Marks one-past the last index for each episode\n",
    "        episode_ends = dataset_root['meta']['episode_ends'][:]\n",
    "\n",
    "        # compute start and end of each state-action sequence\n",
    "        # also handles padding\n",
    "        indices = create_sample_indices(\n",
    "            episode_ends=episode_ends,\n",
    "            sequence_length=pred_horizon,\n",
    "            # add padding such that each timestep in the dataset are seen\n",
    "            pad_before=obs_horizon-1,\n",
    "            pad_after=action_horizon-1)\n",
    "\n",
    "        # compute statistics and normalized data to [-1,1]\n",
    "        stats = dict()\n",
    "        normalized_train_data = dict()\n",
    "        for key, data in train_data.items():\n",
    "            stats[key] = get_data_stats(data)\n",
    "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
    "\n",
    "        self.indices = indices\n",
    "        self.stats = stats\n",
    "        self.normalized_train_data = normalized_train_data\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.action_horizon = action_horizon\n",
    "        self.obs_horizon = obs_horizon\n",
    "\n",
    "    def __len__(self):\n",
    "        # all possible segments of the dataset\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the start/end indices for this datapoint\n",
    "        buffer_start_idx, buffer_end_idx, \\\n",
    "            sample_start_idx, sample_end_idx = self.indices[idx]\n",
    "\n",
    "        # get nomralized data using these indices\n",
    "        nsample = sample_sequence(\n",
    "            train_data=self.normalized_train_data,\n",
    "            sequence_length=self.pred_horizon,\n",
    "            buffer_start_idx=buffer_start_idx,\n",
    "            buffer_end_idx=buffer_end_idx,\n",
    "            sample_start_idx=sample_start_idx,\n",
    "            sample_end_idx=sample_end_idx\n",
    "        )\n",
    "\n",
    "        # discard unused observations\n",
    "        nsample['obs'] = nsample['obs'][:self.obs_horizon,:]\n",
    "        return nsample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch['obs'].shape: torch.Size([256, 2, 5])\n",
      "batch['action'].shape torch.Size([256, 16, 2])\n"
     ]
    }
   ],
   "source": [
    "#@markdown ### **Dataset Demo**\n",
    "\n",
    "# download demonstration data from Google Drive\n",
    "#dataset_path = \"pusht_cchi_v7_replay.zarr.zip\"\n",
    "#if not os.path.isfile(dataset_path):\n",
    "#    id = \"1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\"\n",
    "#    gdown.download(id=id, output=dataset_path, quiet=False)\n",
    "\n",
    "# parameters\n",
    "pred_horizon = 16\n",
    "obs_horizon = 2\n",
    "action_horizon = 8\n",
    "#|o|o|                             observations: 2\n",
    "#| |a|a|a|a|a|a|a|a|               actions executed: 8\n",
    "#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n",
    "\n",
    "# create dataset from file\n",
    "dataset = PushTStateDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    pred_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    "    action_horizon=action_horizon\n",
    ")\n",
    "# save training data statistics (min, max) for each dim\n",
    "stats = dataset.stats\n",
    "\n",
    "# create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    # accelerate cpu-gpu transfer\n",
    "    pin_memory=True,\n",
    "    # don't kill worker process afte each epoch\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# visualize data in batch\n",
    "batch = next(iter(dataloader))\n",
    "print(\"batch['obs'].shape:\", batch['obs'].shape)\n",
    "print(\"batch['action'].shape\", batch['action'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Network**\n",
    "#@markdown\n",
    "#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n",
    "#@markdown as the noies prediction network\n",
    "#@markdown\n",
    "#@markdown Components\n",
    "#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n",
    "#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n",
    "#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n",
    "#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n",
    "#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n",
    "#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.\n",
    "#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class Downsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    '''\n",
    "        Conv1d --> GroupNorm --> Mish\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.GroupNorm(n_groups, out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ConditionalResidualBlock1D(nn.Module):\n",
    "    def __init__(self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            cond_dim,\n",
    "            kernel_size=3,\n",
    "            n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "        ])\n",
    "\n",
    "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
    "        # predicts per-channel scale and bias\n",
    "        cond_channels = out_channels * 2\n",
    "        self.out_channels = out_channels\n",
    "        self.cond_encoder = nn.Sequential(\n",
    "            nn.Mish(),\n",
    "            nn.Linear(cond_dim, cond_channels),\n",
    "            nn.Unflatten(-1, (-1, 1))\n",
    "        )\n",
    "\n",
    "        # make sure dimensions compatible\n",
    "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        '''\n",
    "            x : [ batch_size x in_channels x horizon ]\n",
    "            cond : [ batch_size x cond_dim]\n",
    "\n",
    "            returns:\n",
    "            out : [ batch_size x out_channels x horizon ]\n",
    "        '''\n",
    "        out = self.blocks[0](x)\n",
    "        embed = self.cond_encoder(cond)\n",
    "\n",
    "        embed = embed.reshape(\n",
    "            embed.shape[0], 2, self.out_channels, 1)\n",
    "        scale = embed[:,0,...]\n",
    "        bias = embed[:,1,...]\n",
    "        out = scale * out + bias\n",
    "\n",
    "        out = self.blocks[1](out)\n",
    "        out = out + self.residual_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConditionalUnet1D(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim,\n",
    "        global_cond_dim,\n",
    "        diffusion_step_embed_dim=256,\n",
    "        down_dims=[256,512,1024],\n",
    "        kernel_size=5,\n",
    "        n_groups=8\n",
    "        ):\n",
    "        \"\"\"\n",
    "        input_dim: Dim of actions.\n",
    "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
    "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
    "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
    "        down_dims: Channel size for each UNet level.\n",
    "          The length of this array determines numebr of levels.\n",
    "        kernel_size: Conv kernel size\n",
    "        n_groups: Number of groups for GroupNorm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        all_dims = [input_dim] + list(down_dims)\n",
    "        start_dim = down_dims[0]\n",
    "\n",
    "        dsed = diffusion_step_embed_dim\n",
    "        diffusion_step_encoder = nn.Sequential(\n",
    "            SinusoidalPosEmb(dsed),\n",
    "            nn.Linear(dsed, dsed * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(dsed * 4, dsed),\n",
    "        )\n",
    "        cond_dim = dsed + global_cond_dim\n",
    "\n",
    "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
    "        mid_dim = all_dims[-1]\n",
    "        self.mid_modules = nn.ModuleList([\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        down_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            down_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        up_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            up_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        final_conv = nn.Sequential(\n",
    "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
    "            nn.Conv1d(start_dim, input_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.diffusion_step_encoder = diffusion_step_encoder\n",
    "        self.up_modules = up_modules\n",
    "        self.down_modules = down_modules\n",
    "        self.final_conv = final_conv\n",
    "\n",
    "        print(\"number of parameters: {:e}\".format(\n",
    "            sum(p.numel() for p in self.parameters()))\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "            sample: torch.Tensor,\n",
    "            timestep: Union[torch.Tensor, float, int],\n",
    "            global_cond=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,input_dim)\n",
    "        timestep: (B,) or int, diffusion step\n",
    "        global_cond: (B,global_cond_dim)\n",
    "        output: (B,T,input_dim)\n",
    "        \"\"\"\n",
    "        # (B,T,C)\n",
    "        sample = sample.moveaxis(-1,-2)\n",
    "        # (B,C,T)\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        global_feature = self.diffusion_step_encoder(timesteps)\n",
    "\n",
    "        if global_cond is not None:\n",
    "            global_feature = torch.cat([\n",
    "                global_feature, global_cond\n",
    "            ], axis=-1)\n",
    "\n",
    "        x = sample\n",
    "        h = []\n",
    "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        for mid_module in self.mid_modules:\n",
    "            x = mid_module(x, global_feature)\n",
    "\n",
    "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # (B,C,T)\n",
    "        x = x.moveaxis(-1,-2)\n",
    "        # (B,T,C)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 6.535322e+07\n"
     ]
    }
   ],
   "source": [
    "#@markdown ### **Network Demo**\n",
    "\n",
    "# observation and action dimensions corrsponding to\n",
    "# the output of PushTEnv\n",
    "obs_dim = 5\n",
    "action_dim = 2\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim*obs_horizon\n",
    ")\n",
    "\n",
    "# example inputs\n",
    "noised_action = torch.randn((1, pred_horizon, action_dim))\n",
    "obs = torch.zeros((1, obs_horizon, obs_dim))\n",
    "diffusion_iter = torch.zeros((1,))\n",
    "\n",
    "# the noise prediction network\n",
    "# takes noisy action, diffusion iteration and observation as input\n",
    "# predicts the noise added to action\n",
    "noise = noise_pred_net(\n",
    "    sample=noised_action,\n",
    "    timestep=diffusion_iter,\n",
    "    global_cond=obs.flatten(start_dim=1))\n",
    "\n",
    "# illustration of removing noise\n",
    "# the actual noise removal is performed by NoiseScheduler\n",
    "# and is dependent on the diffusion noise schedule\n",
    "denoised_action = noised_action - noise\n",
    "\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "num_diffusion_iters = 100\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    # the choise of beta schedule has big impact on performance\n",
    "    # we found squared cosine works the best\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    # clip output to [-1,1] to improve stability\n",
    "    clip_sample=True,\n",
    "    # our network predicts noise (instead of denoised action)\n",
    "    prediction_type='epsilon'\n",
    ")\n",
    "\n",
    "# device transfer\n",
    "device = torch.device('cuda')\n",
    "_ = noise_pred_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1%|          | 1/100 [02:27<4:03:50, 147.78s/it, loss=0.708]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# optimize\u001b[39;00m\n\u001b[1;32m     70\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 71\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# step lr scheduler every batch\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# this is different from standard pytorch behavior\u001b[39;00m\n",
      "File \u001b[0;32m/project/dl2024s/lottv/conda_envs/dl/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/dl2024s/lottv/conda_envs/dl/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/project/dl2024s/lottv/conda_envs/dl/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/dl2024s/lottv/conda_envs/dl/lib/python3.9/site-packages/torch/optim/adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m             max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    160\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m          \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m          \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m          \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m          \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m          \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/project/dl2024s/lottv/conda_envs/dl/lib/python3.9/site-packages/torch/optim/adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 219\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/dl2024s/lottv/conda_envs/dl/lib/python3.9/site-packages/torch/optim/adamw.py:274\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    273\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 274\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable:\n\u001b[1;32m    277\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@markdown ### **Training**\n",
    "#@markdown\n",
    "#@markdown Takes about an hour. If you don't want to wait, skip to the next cell\n",
    "#@markdown to load pre-trained weights\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# Exponential Moving Average\n",
    "# accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "ema = EMAModel(\n",
    "    parameters=noise_pred_net.parameters(),\n",
    "    power=0.75)\n",
    "\n",
    "# Standard ADAM optimizer\n",
    "# Note that EMA parametesr are not optimized\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=noise_pred_net.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    # epoch loop\n",
    "    for epoch_idx in tglobal:\n",
    "        epoch_loss = list()\n",
    "        # batch loop\n",
    "        with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
    "            for nbatch in tepoch:\n",
    "                # data normalized in dataset\n",
    "                # device transfer\n",
    "                nobs = nbatch['obs'].to(device)\n",
    "                naction = nbatch['action'].to(device)\n",
    "                B = nobs.shape[0]\n",
    "\n",
    "                # observation as FiLM conditioning\n",
    "                # (B, obs_horizon, obs_dim)\n",
    "                obs_cond = nobs[:,:obs_horizon,:]\n",
    "                # (B, obs_horizon * obs_dim)\n",
    "                obs_cond = obs_cond.flatten(start_dim=1)\n",
    "\n",
    "                # sample noise to add to actions\n",
    "                noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "                # sample a diffusion iteration for each data point\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps,\n",
    "                    (B,), device=device\n",
    "                ).long()\n",
    "\n",
    "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_actions = noise_scheduler.add_noise(\n",
    "                    naction, noise, timesteps)\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = noise_pred_net(\n",
    "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "                # L2 loss\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "                # optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # step lr scheduler every batch\n",
    "                # this is different from standard pytorch behavior\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # update Exponential Moving Average of the model weights\n",
    "                ema.step(noise_pred_net.parameters())\n",
    "\n",
    "                # logging\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
    "\n",
    "# Weights of the EMA model\n",
    "# is used for inference\n",
    "ema_noise_pred_net = noise_pred_net\n",
    "ema.copy_to(ema_noise_pred_net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code from Diffusion Policy for ENvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Environment**\n",
    "#@markdown Defines a PyMunk-based Push-T environment `PushTEnv`.\n",
    "#@markdown\n",
    "#@markdown **Goal**: push the gray T-block into the green area.\n",
    "#@markdown\n",
    "#@markdown Adapted from [Implicit Behavior Cloning](https://implicitbc.github.io/)\n",
    "\n",
    "\n",
    "positive_y_is_up: bool = False\n",
    "\"\"\"Make increasing values of y point upwards.\n",
    "\n",
    "When True::\n",
    "\n",
    "    y\n",
    "    ^\n",
    "    |      . (3, 3)\n",
    "    |\n",
    "    |   . (2, 2)\n",
    "    |\n",
    "    +------ > x\n",
    "\n",
    "When False::\n",
    "\n",
    "    +------ > x\n",
    "    |\n",
    "    |   . (2, 2)\n",
    "    |\n",
    "    |      . (3, 3)\n",
    "    v\n",
    "    y\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def to_pygame(p: Tuple[float, float], surface: pygame.Surface) -> Tuple[int, int]:\n",
    "    \"\"\"Convenience method to convert pymunk coordinates to pygame surface\n",
    "    local coordinates.\n",
    "\n",
    "    Note that in case positive_y_is_up is False, this function wont actually do\n",
    "    anything except converting the point to integers.\n",
    "    \"\"\"\n",
    "    if positive_y_is_up:\n",
    "        return round(p[0]), surface.get_height() - round(p[1])\n",
    "    else:\n",
    "        return round(p[0]), round(p[1])\n",
    "\n",
    "\n",
    "def light_color(color: SpaceDebugColor):\n",
    "    color = np.minimum(1.2 * np.float32([color.r, color.g, color.b, color.a]), np.float32([255]))\n",
    "    color = SpaceDebugColor(r=color[0], g=color[1], b=color[2], a=color[3])\n",
    "    return color\n",
    "\n",
    "class DrawOptions(pymunk.SpaceDebugDrawOptions):\n",
    "    def __init__(self, surface: pygame.Surface) -> None:\n",
    "        \"\"\"Draw a pymunk.Space on a pygame.Surface object.\n",
    "\n",
    "        Typical usage::\n",
    "\n",
    "        >>> import pymunk\n",
    "        >>> surface = pygame.Surface((10,10))\n",
    "        >>> space = pymunk.Space()\n",
    "        >>> options = pymunk.pygame_util.DrawOptions(surface)\n",
    "        >>> space.debug_draw(options)\n",
    "\n",
    "        You can control the color of a shape by setting shape.color to the color\n",
    "        you want it drawn in::\n",
    "\n",
    "        >>> c = pymunk.Circle(None, 10)\n",
    "        >>> c.color = pygame.Color(\"pink\")\n",
    "\n",
    "        See pygame_util.demo.py for a full example\n",
    "\n",
    "        Since pygame uses a coordiante system where y points down (in contrast\n",
    "        to many other cases), you either have to make the physics simulation\n",
    "        with Pymunk also behave in that way, or flip everything when you draw.\n",
    "\n",
    "        The easiest is probably to just make the simulation behave the same\n",
    "        way as Pygame does. In that way all coordinates used are in the same\n",
    "        orientation and easy to reason about::\n",
    "\n",
    "        >>> space = pymunk.Space()\n",
    "        >>> space.gravity = (0, -1000)\n",
    "        >>> body = pymunk.Body()\n",
    "        >>> body.position = (0, 0) # will be positioned in the top left corner\n",
    "        >>> space.debug_draw(options)\n",
    "\n",
    "        To flip the drawing its possible to set the module property\n",
    "        :py:data:`positive_y_is_up` to True. Then the pygame drawing will flip\n",
    "        the simulation upside down before drawing::\n",
    "\n",
    "        >>> positive_y_is_up = True\n",
    "        >>> body = pymunk.Body()\n",
    "        >>> body.position = (0, 0)\n",
    "        >>> # Body will be position in bottom left corner\n",
    "\n",
    "        :Parameters:\n",
    "                surface : pygame.Surface\n",
    "                    Surface that the objects will be drawn on\n",
    "        \"\"\"\n",
    "        self.surface = surface\n",
    "        super(DrawOptions, self).__init__()\n",
    "\n",
    "    def draw_circle(\n",
    "        self,\n",
    "        pos: Vec2d,\n",
    "        angle: float,\n",
    "        radius: float,\n",
    "        outline_color: SpaceDebugColor,\n",
    "        fill_color: SpaceDebugColor,\n",
    "    ) -> None:\n",
    "        p = to_pygame(pos, self.surface)\n",
    "\n",
    "        pygame.draw.circle(self.surface, fill_color.as_int(), p, round(radius), 0)\n",
    "        pygame.draw.circle(self.surface, light_color(fill_color).as_int(), p, round(radius-4), 0)\n",
    "\n",
    "        circle_edge = pos + Vec2d(radius, 0).rotated(angle)\n",
    "        p2 = to_pygame(circle_edge, self.surface)\n",
    "        line_r = 2 if radius > 20 else 1\n",
    "        # pygame.draw.lines(self.surface, outline_color.as_int(), False, [p, p2], line_r)\n",
    "\n",
    "    def draw_segment(self, a: Vec2d, b: Vec2d, color: SpaceDebugColor) -> None:\n",
    "        p1 = to_pygame(a, self.surface)\n",
    "        p2 = to_pygame(b, self.surface)\n",
    "\n",
    "        pygame.draw.aalines(self.surface, color.as_int(), False, [p1, p2])\n",
    "\n",
    "    def draw_fat_segment(\n",
    "        self,\n",
    "        a: Tuple[float, float],\n",
    "        b: Tuple[float, float],\n",
    "        radius: float,\n",
    "        outline_color: SpaceDebugColor,\n",
    "        fill_color: SpaceDebugColor,\n",
    "    ) -> None:\n",
    "        p1 = to_pygame(a, self.surface)\n",
    "        p2 = to_pygame(b, self.surface)\n",
    "\n",
    "        r = round(max(1, radius * 2))\n",
    "        pygame.draw.lines(self.surface, fill_color.as_int(), False, [p1, p2], r)\n",
    "        if r > 2:\n",
    "            orthog = [abs(p2[1] - p1[1]), abs(p2[0] - p1[0])]\n",
    "            if orthog[0] == 0 and orthog[1] == 0:\n",
    "                return\n",
    "            scale = radius / (orthog[0] * orthog[0] + orthog[1] * orthog[1]) ** 0.5\n",
    "            orthog[0] = round(orthog[0] * scale)\n",
    "            orthog[1] = round(orthog[1] * scale)\n",
    "            points = [\n",
    "                (p1[0] - orthog[0], p1[1] - orthog[1]),\n",
    "                (p1[0] + orthog[0], p1[1] + orthog[1]),\n",
    "                (p2[0] + orthog[0], p2[1] + orthog[1]),\n",
    "                (p2[0] - orthog[0], p2[1] - orthog[1]),\n",
    "            ]\n",
    "            pygame.draw.polygon(self.surface, fill_color.as_int(), points)\n",
    "            pygame.draw.circle(\n",
    "                self.surface,\n",
    "                fill_color.as_int(),\n",
    "                (round(p1[0]), round(p1[1])),\n",
    "                round(radius),\n",
    "            )\n",
    "            pygame.draw.circle(\n",
    "                self.surface,\n",
    "                fill_color.as_int(),\n",
    "                (round(p2[0]), round(p2[1])),\n",
    "                round(radius),\n",
    "            )\n",
    "\n",
    "    def draw_polygon(\n",
    "        self,\n",
    "        verts: Sequence[Tuple[float, float]],\n",
    "        radius: float,\n",
    "        outline_color: SpaceDebugColor,\n",
    "        fill_color: SpaceDebugColor,\n",
    "    ) -> None:\n",
    "        ps = [to_pygame(v, self.surface) for v in verts]\n",
    "        ps += [ps[0]]\n",
    "\n",
    "        radius = 2\n",
    "        pygame.draw.polygon(self.surface, light_color(fill_color).as_int(), ps)\n",
    "\n",
    "        if radius > 0:\n",
    "            for i in range(len(verts)):\n",
    "                a = verts[i]\n",
    "                b = verts[(i + 1) % len(verts)]\n",
    "                self.draw_fat_segment(a, b, radius, fill_color, fill_color)\n",
    "\n",
    "    def draw_dot(\n",
    "        self, size: float, pos: Tuple[float, float], color: SpaceDebugColor\n",
    "    ) -> None:\n",
    "        p = to_pygame(pos, self.surface)\n",
    "        pygame.draw.circle(self.surface, color.as_int(), p, round(size), 0)\n",
    "\n",
    "\n",
    "def pymunk_to_shapely(body, shapes):\n",
    "    geoms = list()\n",
    "    for shape in shapes:\n",
    "        if isinstance(shape, pymunk.shapes.Poly):\n",
    "            verts = [body.local_to_world(v) for v in shape.get_vertices()]\n",
    "            verts += [verts[0]]\n",
    "            geoms.append(sg.Polygon(verts))\n",
    "        else:\n",
    "            raise RuntimeError(f'Unsupported shape type {type(shape)}')\n",
    "    geom = sg.MultiPolygon(geoms)\n",
    "    return geom\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
