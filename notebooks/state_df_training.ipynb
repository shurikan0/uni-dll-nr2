{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pip Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the package\n",
    "%pip install --upgrade mani_skill\n",
    "# install a version of torch that is compatible with your system\n",
    "%pip install torch torchvision torchaudio numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import h5py\n",
    "from torch.utils.data import IterableDataset\n",
    "from tqdm import tqdm\n",
    "from helper import *\n",
    "from mani_skill.utils import common\n",
    "from mani_skill.utils.io_utils import load_json\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from mani_skill.utils.common import flatten_state_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads h5 data into memory for faster access\n",
    "def load_h5_data(data):\n",
    "    out = dict()\n",
    "    for k in data.keys():\n",
    "        if isinstance(data[k], h5py.Dataset):\n",
    "            out[k] = data[k][:]\n",
    "        else:\n",
    "            out[k] = load_h5_data(data[k])\n",
    "    return out\n",
    "\n",
    "\n",
    "def create_sample_indices(episode_ends: np.ndarray, sequence_length: int, pad_before: int = 0, pad_after: int = 0):\n",
    "    # Currently uses truncated as episode ends which is the end of the episode and not the end of the trajectory\n",
    "    # TODO: What to use as episode ends?\n",
    "    indices = list()\n",
    "    episode_length = 0\n",
    "    episode_index = 1 # Start 1 for human readability\n",
    "    for i in range(len(episode_ends)):\n",
    "        episode_length += 1\n",
    "        if episode_ends[i]:\n",
    "            start_idx = 0 if i <= 0 else i - episode_length + 1\n",
    "            min_start = -pad_before\n",
    "            max_start = episode_length - sequence_length + pad_after\n",
    "            \n",
    "            # Create indices for each possible sequence in the episode\n",
    "            for idx in range(min_start, max_start + 1):\n",
    "                buffer_start_idx = max(idx, 0) + start_idx\n",
    "                buffer_end_idx = min(idx + sequence_length, episode_length) + start_idx\n",
    "                start_offset = buffer_start_idx - (idx + start_idx)\n",
    "                end_offset = (idx + sequence_length + start_idx) - buffer_end_idx\n",
    "                sample_start_idx = 0 + start_offset\n",
    "                sample_end_idx = sequence_length - end_offset\n",
    "                indices.append([buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx])\n",
    "            episode_length = 0\n",
    "            episode_index += 1\n",
    "    return np.array(indices)\n",
    "\n",
    "\n",
    "def sample_sequence(train_data, sequence_length, buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx):\n",
    "    result = dict()\n",
    "    for key, input_arr in train_data.items():\n",
    "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
    "        data = sample\n",
    "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
    "            if isinstance(input_arr, torch.Tensor):\n",
    "                data = torch.zeros((sequence_length,) + input_arr.shape[1:], dtype=input_arr.dtype)\n",
    "            else: \n",
    "                data = np.zeros(shape=(sequence_length,) + input_arr.shape[1:], dtype=input_arr.dtype)\n",
    "            if sample_start_idx > 0:\n",
    "                data[:sample_start_idx] = sample[0]\n",
    "            if sample_end_idx < sequence_length:\n",
    "                data[sample_end_idx:] = sample[-1]\n",
    "            data[sample_start_idx:sample_end_idx] = sample\n",
    "        result[key] = data\n",
    "    return result\n",
    "\n",
    "def remove_np_uint16(x: Union[np.ndarray, dict]):\n",
    "            if isinstance(x, dict):\n",
    "                for k in x.keys():\n",
    "                    x[k] = remove_np_uint16(x[k])\n",
    "                return x\n",
    "            else:\n",
    "                if x.dtype == np.uint16:\n",
    "                    return x.astype(np.int32)\n",
    "                return x\n",
    "            \n",
    "\n",
    "def flatten_observation(observation):\n",
    "    # flattens the original observation by flattening the state dictionaries\n",
    "    # and combining the rgb and depth images\n",
    "\n",
    "    # we provide a simple tool to flatten dictionaries with state data\n",
    "\n",
    "    \n",
    "    obs_dict = dict()\n",
    "    for k in observation.keys():\n",
    "        if k != \"actions\":\n",
    "            flatten_state_dict(observation[k]),\n",
    "           \n",
    "\n",
    "    return observation\n",
    "\n",
    "def convert_observation(observation):\n",
    "    # flattens the original observation by flattening the state dictionaries\n",
    "    # and combining the rgb and depth images\n",
    "\n",
    "    # we provide a simple tool to flatten dictionaries with state data\n",
    "    new_observation = dict()\n",
    "    for k in observation.keys():\n",
    "        if k != \"actions\":\n",
    "            new_observation[k] = flatten_state_dict(observation[k])\n",
    "    state = np.hstack(\n",
    "        new_observation.values()\n",
    "    )\n",
    "\n",
    "    return {\"obs\": state, \"actions\": observation[\"actions\"]}\n",
    "\n",
    "class StateStreamingDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    A general torch Dataset you can drop in and use immediately with just about any trajectory .h5 data generated from ManiSkill.\n",
    "    This class simply is a simple starter code to load trajectory data easily, but does not do any data transformation or anything\n",
    "    advanced. We recommend you to copy this code directly and modify it for more advanced use cases\n",
    "    Implements the IterableDataset class for PyTorch to allow for streaming data loading. \n",
    "    Currently only supports PointCloud and RGBD data.\n",
    "    \n",
    "    Args:\n",
    "        dataset_file (str): path to the .h5 file containing the data you want to load\n",
    "        pred_horizon (int): the number of steps to predict into the future\n",
    "        obs_horizon (int): the number of steps to observe in the past\n",
    "        action_horizon (int): the number of steps to execute actions in the future\n",
    "        device: The location to save data to. If None will store as numpy (the default), otherwise will move data to that device\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataset_file: str, pred_horizon: int, obs_horizon: int, action_horizon:int, device=None\n",
    "    ) -> None:\n",
    "        self.dataset_file = dataset_file\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.obs_horizon = obs_horizon\n",
    "        self.action_horizon = action_horizon\n",
    "        self.device = device\n",
    "        self.data = h5py.File(dataset_file, \"r\")\n",
    "        json_path = dataset_file.replace(\".h5\", \".json\")\n",
    "        self.json_data = load_json(json_path)\n",
    "        self.episodes = self.json_data[\"episodes\"]\n",
    "        self.env_info = self.json_data[\"env_info\"]\n",
    "        self.env_id = self.env_info[\"env_id\"]\n",
    "        self.env_kwargs = self.env_info[\"env_kwargs\"]\n",
    "        self.current_episode = 0\n",
    "\n",
    "    def __iter__(self):    \n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:  # Single-process\n",
    "            episode_indices = range(len(self.episodes))\n",
    "        else:\n",
    "            num_episodes = len(self.episodes)\n",
    "            per_worker = int(math.ceil(num_episodes / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            episode_indices = range(worker_id * per_worker, min((worker_id + 1) * per_worker, num_episodes))\n",
    "\n",
    "        for eps_id in episode_indices:\n",
    "            eps = self.episodes[eps_id]\n",
    "            with h5py.File(self.dataset_file, \"r\") as data:  # Context manager\n",
    "                trajectory = data[f\"traj_{eps['episode_id']}\"]\n",
    "                trajectory = load_h5_data(trajectory)  # Load data\n",
    "                eps_len = len(trajectory[\"actions\"])\n",
    "\n",
    "                # exclude the final observation as most learning workflows do not use it\n",
    "                obs = common.index_dict_array(trajectory[\"obs\"], slice(eps_len))\n",
    "                if self.current_episode > 0:\n",
    "                    obs = common.append_dict_array(obs, obs)\n",
    "\n",
    "                actions = trajectory[\"actions\"]\n",
    "                terminated = trajectory[\"terminated\"]\n",
    "                truncated = trajectory[\"truncated\"]\n",
    "                rewards = trajectory.get(\"rewards\", None)\n",
    "                success = trajectory.get(\"success\", None)\n",
    "                fail = trajectory.get(\"fail\", None)\n",
    "         \n",
    "            \n",
    "                # uint16 dtype is used to conserve disk space and memory\n",
    "                # you can optimize this dataset code to keep it as uint16 and process that\n",
    "                # dtype of data yourself. for simplicity we simply cast to a int32 so\n",
    "                # it can automatically be converted to torch tensors without complaint\n",
    "                obs = remove_np_uint16(obs)\n",
    "            \n",
    "                if self.device is not None:\n",
    "                    actions = common.to_tensor(actions, device=self.device)\n",
    "                    obs = common.to_tensor(obs, device=self.device)\n",
    "                    terminated = common.to_tensor(terminated, device=self.device)\n",
    "                    truncated = common.to_tensor(truncated, device=self.device)\n",
    "                    if rewards is not None:\n",
    "                        rewards = common.to_tensor(rewards, device=self.device)\n",
    "                    if success is not None:\n",
    "                        success = common.to_tensor(terminated, device=self.device)\n",
    "                    if fail is not None:\n",
    "                        fail = common.to_tensor(truncated, device=self.device)\n",
    "            \n",
    "                # Added code for diffusion policy\n",
    "\n",
    "                # Initialize index lists and stat dicts\n",
    "                indices = create_sample_indices(\n",
    "                    episode_ends=truncated, \n",
    "                    sequence_length=self.pred_horizon,\n",
    "                    pad_before=self.obs_horizon - 1,\n",
    "                    pad_after=self.action_horizon - 1\n",
    "                )\n",
    "\n",
    "                for idx in indices:\n",
    "                    buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx = idx\n",
    "\n",
    "                    print(obs.keys())\n",
    "                    print(obs[\"extra\"].keys())\n",
    "                    print(obs[\"agent\"].keys())\n",
    "                    train_data = dict(\n",
    "                        tcp_pose=obs[\"extra\"][\"tcp_pose\"],\n",
    "                        obj_pose=obs[\"extra\"][\"obj_pose\"],\n",
    "                        goal_pos=obs[\"extra\"][\"goal_pos\"],\n",
    "                        #is_grasped=obs[\"extra\"][\"is_grasped\"],\n",
    "                        #tcp_to_obj_pos=obs[\"extra\"][\"tcp_to_obj_pos\"],\n",
    "                        #obj_to_goal_pos=obs[\"extra\"][\"obj_to_goal_pos\"],\n",
    "                        qpos=obs[\"agent\"][\"qpos\"],\n",
    "                        qvel=obs[\"agent\"][\"qvel\"],\n",
    "                        actions=actions,\n",
    "                        )\n",
    "                   \n",
    "                    sampled = sample_sequence(\n",
    "                        train_data=train_data, \n",
    "                        sequence_length=self.pred_horizon,\n",
    "                        buffer_start_idx=buffer_start_idx,\n",
    "                        buffer_end_idx=buffer_end_idx,\n",
    "                        sample_start_idx=sample_start_idx,\n",
    "                        sample_end_idx=sample_end_idx\n",
    "                    )\n",
    "                    for k in sampled.keys():\n",
    "                        if k != \"actions\":\n",
    "                            # discard unused observations in the sequence\n",
    "                            sampled[k] = sampled[k][:self.obs_horizon,:]\n",
    "                    sampled[k] = common.to_tensor(sampled[k], device=self.device)\n",
    "            \n",
    "                    sampled = convert_observation(sampled)\n",
    "\n",
    "                    yield sampled\n",
    "                    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Network**\n",
    "#@markdown\n",
    "#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n",
    "#@markdown as the noies prediction network\n",
    "#@markdown\n",
    "#@markdown Components\n",
    "#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n",
    "#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n",
    "#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n",
    "#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n",
    "#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n",
    "#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.\n",
    "#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class Downsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    '''\n",
    "        Conv1d --> GroupNorm --> Mish\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.GroupNorm(n_groups, out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ConditionalResidualBlock1D(nn.Module):\n",
    "    def __init__(self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            cond_dim,\n",
    "            kernel_size=3,\n",
    "            n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "        ])\n",
    "\n",
    "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
    "        # predicts per-channel scale and bias\n",
    "        cond_channels = out_channels * 2\n",
    "        self.out_channels = out_channels\n",
    "        self.cond_encoder = nn.Sequential(\n",
    "            nn.Mish(),\n",
    "            nn.Linear(cond_dim, cond_channels),\n",
    "            nn.Unflatten(-1, (-1, 1))\n",
    "        )\n",
    "\n",
    "        # make sure dimensions compatible\n",
    "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        '''\n",
    "            x : [ batch_size x in_channels x horizon ]\n",
    "            cond : [ batch_size x cond_dim]\n",
    "\n",
    "            returns:\n",
    "            out : [ batch_size x out_channels x horizon ]\n",
    "        '''\n",
    "        out = self.blocks[0](x)\n",
    "        embed = self.cond_encoder(cond)\n",
    "\n",
    "        embed = embed.reshape(\n",
    "            embed.shape[0], 2, self.out_channels, 1)\n",
    "        scale = embed[:,0,...]\n",
    "        bias = embed[:,1,...]\n",
    "        out = scale * out + bias\n",
    "\n",
    "        out = self.blocks[1](out)\n",
    "        out = out + self.residual_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConditionalUnet1D(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim,\n",
    "        global_cond_dim,\n",
    "        diffusion_step_embed_dim=256,\n",
    "        down_dims=[256,512,1024],\n",
    "        kernel_size=5,\n",
    "        n_groups=8\n",
    "        ):\n",
    "        \"\"\"\n",
    "        input_dim: Dim of actions.\n",
    "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
    "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
    "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
    "        down_dims: Channel size for each UNet level.\n",
    "          The length of this array determines numebr of levels.\n",
    "        kernel_size: Conv kernel size\n",
    "        n_groups: Number of groups for GroupNorm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        all_dims = [input_dim] + list(down_dims)\n",
    "        start_dim = down_dims[0]\n",
    "\n",
    "        dsed = diffusion_step_embed_dim\n",
    "        diffusion_step_encoder = nn.Sequential(\n",
    "            SinusoidalPosEmb(dsed),\n",
    "            nn.Linear(dsed, dsed * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(dsed * 4, dsed),\n",
    "        )\n",
    "        cond_dim = dsed + global_cond_dim\n",
    "\n",
    "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
    "        mid_dim = all_dims[-1]\n",
    "        self.mid_modules = nn.ModuleList([\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        down_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            down_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        up_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            up_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        final_conv = nn.Sequential(\n",
    "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
    "            nn.Conv1d(start_dim, input_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.diffusion_step_encoder = diffusion_step_encoder\n",
    "        self.up_modules = up_modules\n",
    "        self.down_modules = down_modules\n",
    "        self.final_conv = final_conv\n",
    "\n",
    "        print(\"number of parameters: {:e}\".format(\n",
    "            sum(p.numel() for p in self.parameters()))\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "            sample: torch.Tensor,\n",
    "            timestep: Union[torch.Tensor, float, int],\n",
    "            global_cond=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,input_dim)\n",
    "        timestep: (B,) or int, diffusion step\n",
    "        global_cond: (B,global_cond_dim)\n",
    "        output: (B,T,input_dim)\n",
    "        \"\"\"\n",
    "        # (B,T,C)\n",
    "        sample = sample.moveaxis(-1,-2)\n",
    "        # (B,C,T)\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        global_feature = self.diffusion_step_encoder(timesteps)\n",
    "\n",
    "        if global_cond is not None:\n",
    "            global_feature = torch.cat([\n",
    "                global_feature, global_cond\n",
    "            ], axis=-1)\n",
    "\n",
    "        x = sample\n",
    "        h = []\n",
    "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        for mid_module in self.mid_modules:\n",
    "            x = mid_module(x, global_feature)\n",
    "\n",
    "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # (B,C,T)\n",
    "        x = x.moveaxis(-1,-2)\n",
    "        # (B,T,C)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Dataset Demo**\n",
    "\n",
    "# download demonstration data from Google Drive\n",
    "dataset_path = '/content/drive/MyDrive/Data/Training/demos/PickCube-v1/motionplanning/staterawdata.state_dict.pd_ee_delta_pos.h5'\n",
    "\n",
    "# parameters\n",
    "pred_horizon = 16\n",
    "obs_horizon = 2\n",
    "action_horizon = 8\n",
    "#|o|o|                             observations: 2\n",
    "#| |a|a|a|a|a|a|a|a|               actions executed: 8\n",
    "#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n",
    "\n",
    "# create dataset from file\n",
    "dataset = StateStreamingDataset(\n",
    "    dataset_file=dataset_path,\n",
    "    pred_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    "    action_horizon=action_horizon,\n",
    "    device=None\n",
    ")\n",
    "\n",
    "# save training data statistics (min, max) for each dim\n",
    "stats = dataset.stats\n",
    "\n",
    "# create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    # accelerate cpu-gpu transfer\n",
    "    pin_memory=True,\n",
    "    # don't kill worker process afte each epoch\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# visualize data in batch\n",
    "batch = next(iter(dataloader))\n",
    "print(batch.keys())\n",
    "print(\"observations:\", batch['obs'].shape, batch['obs'].dtype)\n",
    "print(\"actions:\", batch['actions'].shape, batch['actions'].dtype)\n",
    "    \n",
    "\n",
    "# observation and action dimensions corrsponding to the dataset\n",
    "obs_dim = batch['obs'].shape[-1]\n",
    "action_dim = batch['actions'].shape[-1]\n",
    "print(\"obs_dim:\", obs_dim)\n",
    "print(\"action_dim:\", action_dim)\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim*obs_horizon\n",
    ")\n",
    "\n",
    "# example inputs\n",
    "noised_action = torch.randn((1, pred_horizon, action_dim))\n",
    "obs = torch.zeros((1, obs_horizon, obs_dim))\n",
    "diffusion_iter = torch.zeros((1,))\n",
    "\n",
    "# the noise prediction network\n",
    "# takes noisy action, diffusion iteration and observation as input\n",
    "# predicts the noise added to action\n",
    "noise = noise_pred_net(\n",
    "    sample=noised_action,\n",
    "    timestep=diffusion_iter,\n",
    "    global_cond=obs.flatten(start_dim=1))\n",
    "\n",
    "# illustration of removing noise\n",
    "# the actual noise removal is performed by NoiseScheduler\n",
    "# and is dependent on the diffusion noise schedule\n",
    "denoised_action = noised_action - noise\n",
    "\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "num_diffusion_iters = 100\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    # the choise of beta schedule has big impact on performance\n",
    "    # we found squared cosine works the best\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    # clip output to [-1,1] to improve stability\n",
    "    clip_sample=True,\n",
    "    # our network predicts noise (instead of denoised action)\n",
    "    prediction_type='epsilon'\n",
    ")\n",
    "\n",
    "# device transfer\n",
    "device = torch.device('cuda')\n",
    "_ = noise_pred_net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "# Exponential Moving Average\n",
    "# accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "ema = EMAModel(\n",
    "    parameters=noise_pred_net.parameters(),\n",
    "    power=0.75)\n",
    "\n",
    "# Standard ADAM optimizer\n",
    "# Note that EMA parametesr are not optimized\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=noise_pred_net.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    # epoch loop\n",
    "    for epoch_idx in tglobal:\n",
    "        epoch_loss = list()\n",
    "        # batch loop\n",
    "        with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
    "            for nbatch in tepoch:\n",
    "                # data normalized in dataset\n",
    "                # device transfer\n",
    "                nobs = nbatch['obs'].to(device)\n",
    "                naction = nbatch['action'].to(device)\n",
    "                B = nobs.shape[0]\n",
    "\n",
    "                # observation as FiLM conditioning\n",
    "                # (B, obs_horizon, obs_dim)\n",
    "                obs_cond = nobs[:,:obs_horizon,:]\n",
    "                # (B, obs_horizon * obs_dim)\n",
    "                obs_cond = obs_cond.flatten(start_dim=1)\n",
    "\n",
    "                # sample noise to add to actions\n",
    "                noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "                # sample a diffusion iteration for each data point\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps,\n",
    "                    (B,), device=device\n",
    "                ).long()\n",
    "\n",
    "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_actions = noise_scheduler.add_noise(\n",
    "                    naction, noise, timesteps)\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = noise_pred_net(\n",
    "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "                # L2 loss\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "                # optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # step lr scheduler every batch\n",
    "                # this is different from standard pytorch behavior\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # update Exponential Moving Average of the model weights\n",
    "                ema.step(noise_pred_net.parameters())\n",
    "\n",
    "                # logging\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
    "\n",
    "# Weights of the EMA model\n",
    "# is used for inference\n",
    "ema_noise_pred_net = noise_pred_net\n",
    "ema.copy_to(ema_noise_pred_net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained = False\n",
    "if load_pretrained:\n",
    "  ckpt_path = \"pusht_state_100ep.ckpt\"\n",
    "  if not os.path.isfile(ckpt_path):\n",
    "      id = \"1mHDr_DEZSdiGo9yecL50BBQYzR8Fjhl_&confirm=t\"\n",
    "      gdown.download(id=id, output=ckpt_path, quiet=False)\n",
    "\n",
    "  state_dict = torch.load(ckpt_path, map_location='cuda')\n",
    "  ema_noise_pred_net = noise_pred_net\n",
    "  ema_noise_pred_net.load_state_dict(state_dict)\n",
    "  print('Pretrained weights loaded.')\n",
    "else:\n",
    "  print(\"Skipped pretrained weight loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# limit enviornment interaction to 200 steps before termination\n",
    "max_steps = 200\n",
    "env = PushTEnv()\n",
    "# use a seed >200 to avoid initial states seen in the training dataset\n",
    "env.seed(100000)\n",
    "\n",
    "# get first observation\n",
    "obs, info = env.reset()\n",
    "\n",
    "# keep a queue of last 2 steps of observations\n",
    "obs_deque = collections.deque(\n",
    "    [obs] * obs_horizon, maxlen=obs_horizon)\n",
    "# save visualization and rewards\n",
    "imgs = [env.render(mode='rgb_array')]\n",
    "rewards = list()\n",
    "done = False\n",
    "step_idx = 0\n",
    "\n",
    "with tqdm(total=max_steps, desc=\"Eval PushTStateEnv\") as pbar:\n",
    "    while not done:\n",
    "        B = 1\n",
    "        # stack the last obs_horizon (2) number of observations\n",
    "        obs_seq = np.stack(obs_deque)\n",
    "        # normalize observation\n",
    "        nobs = normalize_data(obs_seq, stats=stats['obs'])\n",
    "        # device transfer\n",
    "        nobs = torch.from_numpy(nobs).to(device, dtype=torch.float32)\n",
    "\n",
    "        # infer action\n",
    "        with torch.no_grad():\n",
    "            # reshape observation to (B,obs_horizon*obs_dim)\n",
    "            obs_cond = nobs.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "            # initialize action from Guassian noise\n",
    "            noisy_action = torch.randn(\n",
    "                (B, pred_horizon, action_dim), device=device)\n",
    "            naction = noisy_action\n",
    "\n",
    "            # init scheduler\n",
    "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "\n",
    "            for k in noise_scheduler.timesteps:\n",
    "                # predict noise\n",
    "                noise_pred = ema_noise_pred_net(\n",
    "                    sample=naction,\n",
    "                    timestep=k,\n",
    "                    global_cond=obs_cond\n",
    "                )\n",
    "\n",
    "                # inverse diffusion step (remove noise)\n",
    "                naction = noise_scheduler.step(\n",
    "                    model_output=noise_pred,\n",
    "                    timestep=k,\n",
    "                    sample=naction\n",
    "                ).prev_sample\n",
    "\n",
    "        # unnormalize action\n",
    "        naction = naction.detach().to('cpu').numpy()\n",
    "        # (B, pred_horizon, action_dim)\n",
    "        naction = naction[0]\n",
    "        action_pred = unnormalize_data(naction, stats=stats['action'])\n",
    "\n",
    "        # only take action_horizon number of actions\n",
    "        start = obs_horizon - 1\n",
    "        end = start + action_horizon\n",
    "        action = action_pred[start:end,:]\n",
    "        # (action_horizon, action_dim)\n",
    "\n",
    "        # execute action_horizon number of steps\n",
    "        # without replanning\n",
    "        for i in range(len(action)):\n",
    "            # stepping env\n",
    "            obs, reward, done, _, info = env.step(action[i])\n",
    "            # save observations\n",
    "            obs_deque.append(obs)\n",
    "            # and reward/vis\n",
    "            rewards.append(reward)\n",
    "            imgs.append(env.render(mode='rgb_array'))\n",
    "\n",
    "            # update progress bar\n",
    "            step_idx += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(reward=reward)\n",
    "            if step_idx > max_steps:\n",
    "                done = True\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "# print out the maximum target coverage\n",
    "print('Score: ', max(rewards))\n",
    "\n",
    "# visualize\n",
    "from IPython.display import Video\n",
    "vwrite('vis.mp4', imgs)\n",
    "Video('vis.mp4', embed=True, width=256, height=256)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
