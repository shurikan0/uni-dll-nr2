{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHPE4zWJVeiD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --upgrade mani_skill tyro diffusers sk-video\n",
        "\n",
        "import torch\n",
        "\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATA CLASS"
      ],
      "metadata": {
        "id": "lNU4OJ9Ufmte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union\n",
        "import h5py\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from mani_skill.utils import common\n",
        "from mani_skill.utils.io_utils import load_json\n",
        "\n",
        "def load_h5_data(data):\n",
        "    out = dict()\n",
        "    for k in data.keys():\n",
        "        if isinstance(data[k], h5py.Dataset):\n",
        "            out[k] = data[k][:]\n",
        "        else:\n",
        "            out[k] = load_h5_data(data[k])\n",
        "    return out\n",
        "\n",
        "\n",
        "def create_sample_indices(episode_ends: np.ndarray, sequence_length: int, pad_before: int = 0, pad_after: int = 0):\n",
        "    # Currently uses truncated as episode ends which is the end of the episode and not the end of the trajectory\n",
        "    # TODO: What to use as episode ends?\n",
        "    indices = list()\n",
        "    episode_length = 0\n",
        "    episode_index = 1 # Start 1 for human readability\n",
        "    for i in range(len(episode_ends)):\n",
        "        episode_length += 1\n",
        "        if episode_ends[i]:\n",
        "            start_idx = 0 if i <= 0 else i - episode_length + 1\n",
        "            min_start = -pad_before\n",
        "            max_start = episode_length - sequence_length + pad_after\n",
        "\n",
        "            # Create indices for each possible sequence in the episode\n",
        "            for idx in range(min_start, max_start + 1):\n",
        "                buffer_start_idx = max(idx, 0) + start_idx\n",
        "                buffer_end_idx = min(idx + sequence_length, episode_length) + start_idx\n",
        "                start_offset = buffer_start_idx - (idx + start_idx)\n",
        "                end_offset = (idx + sequence_length + start_idx) - buffer_end_idx\n",
        "                sample_start_idx = 0 + start_offset\n",
        "                sample_end_idx = sequence_length - end_offset\n",
        "                indices.append([buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx])\n",
        "            episode_length = 0\n",
        "            episode_index += 1\n",
        "    return np.array(indices)\n",
        "\n",
        "\n",
        "def sample_sequence(train_data, sequence_length, buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx):\n",
        "    result = dict()\n",
        "    for key, input_arr in train_data.items():\n",
        "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
        "        data = sample\n",
        "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
        "            data = np.zeros(shape=(sequence_length,) + input_arr.shape[1:], dtype=input_arr.dtype)\n",
        "            if sample_start_idx > 0:\n",
        "                data[:sample_start_idx] = sample[0]\n",
        "            if sample_end_idx < sequence_length:\n",
        "                data[sample_end_idx:] = sample[-1]\n",
        "            data[sample_start_idx:sample_end_idx] = sample\n",
        "        result[key] = data\n",
        "    return result\n",
        "\n",
        "class PointCloudManiSkillTrajectoryDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A general torch Dataset you can drop in and use immediately with just about any trajectory .h5 data generated from ManiSkill.\n",
        "    This class simply is a simple starter code to load trajectory data easily, but does not do any data transformation or anything\n",
        "    advanced. We recommend you to copy this code directly and modify it for more advanced use cases\n",
        "\n",
        "    Args:\n",
        "        dataset_file (str): path to the .h5 file containing the data you want to load\n",
        "        load_count (int): the number of trajectories from the dataset to load into memory. If -1, will load all into memory\n",
        "        success_only (bool): whether to skip trajectories that are not successful in the end. Default is false\n",
        "        device: The location to save data to. If None will store as numpy (the default), otherwise will move data to that device\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, dataset_file: str, pred_horizon: int, obs_horizon: int, action_horizon:int, load_count=-1, success_only: bool = False, normalize: bool = False, device=None\n",
        "    ) -> None:\n",
        "        self.dataset_file = dataset_file\n",
        "        self.pred_horizon = pred_horizon\n",
        "        self.obs_horizon = obs_horizon\n",
        "        self.action_horizon = action_horizon\n",
        "        self.normalize = normalize\n",
        "        self.device = device\n",
        "        self.data = h5py.File(dataset_file, \"r\")\n",
        "        json_path = dataset_file.replace(\".h5\", \".json\")\n",
        "        self.json_data = load_json(json_path)\n",
        "        self.episodes = self.json_data[\"episodes\"]\n",
        "        self.env_info = self.json_data[\"env_info\"]\n",
        "        self.env_id = self.env_info[\"env_id\"]\n",
        "        self.env_kwargs = self.env_info[\"env_kwargs\"]\n",
        "\n",
        "        self.obs = None\n",
        "        self.actions = []\n",
        "        self.terminated = []\n",
        "        self.truncated = []\n",
        "        self.success, self.fail, self.rewards = None, None, None\n",
        "        if load_count == -1:\n",
        "            load_count = len(self.episodes)\n",
        "        for eps_id in tqdm(range(load_count), desc=\"Loading Episodes\", colour=\"green\"):\n",
        "            eps = self.episodes[eps_id]\n",
        "            if success_only:\n",
        "                assert (\n",
        "                    \"success\" in eps\n",
        "                ), \"episodes in this dataset do not have the success attribute, cannot load dataset with success_only=True\"\n",
        "                if not eps[\"success\"]:\n",
        "                    continue\n",
        "            trajectory = self.data[f\"traj_{eps['episode_id']}\"]\n",
        "            trajectory = load_h5_data(trajectory)\n",
        "            eps_len = len(trajectory[\"actions\"])\n",
        "\n",
        "            # exclude the final observation as most learning workflows do not use it\n",
        "            obs = common.index_dict_array(trajectory[\"obs\"], slice(eps_len))\n",
        "            if eps_id == 0:\n",
        "                self.obs = obs\n",
        "            else:\n",
        "                self.obs = common.append_dict_array(self.obs, obs)\n",
        "\n",
        "            self.actions.append(trajectory[\"actions\"])\n",
        "            self.terminated.append(trajectory[\"terminated\"])\n",
        "            self.truncated.append(trajectory[\"truncated\"])\n",
        "\n",
        "            # handle data that might optionally be in the trajectory\n",
        "            if \"rewards\" in trajectory:\n",
        "                if self.rewards is None:\n",
        "                    self.rewards = [trajectory[\"rewards\"]]\n",
        "                else:\n",
        "                    self.rewards.append(trajectory[\"rewards\"])\n",
        "            if \"success\" in trajectory:\n",
        "                if self.success is None:\n",
        "                    self.success = [trajectory[\"success\"]]\n",
        "                else:\n",
        "                    self.success.append(trajectory[\"success\"])\n",
        "            if \"fail\" in trajectory:\n",
        "                if self.fail is None:\n",
        "                    self.fail = [trajectory[\"fail\"]]\n",
        "                else:\n",
        "                    self.fail.append(trajectory[\"fail\"])\n",
        "\n",
        "        self.actions = np.vstack(self.actions)\n",
        "        self.terminated = np.concatenate(self.terminated)\n",
        "        self.truncated = np.concatenate(self.truncated)\n",
        "\n",
        "\n",
        "        if self.rewards is not None:\n",
        "            self.rewards = np.concatenate(self.rewards)\n",
        "        if self.success is not None:\n",
        "            self.success = np.concatenate(self.success)\n",
        "        if self.fail is not None:\n",
        "            self.fail = np.concatenate(self.fail)\n",
        "\n",
        "        def remove_np_uint16(x: Union[np.ndarray, dict]):\n",
        "            if isinstance(x, dict):\n",
        "                for k in x.keys():\n",
        "                    x[k] = remove_np_uint16(x[k])\n",
        "                return x\n",
        "            else:\n",
        "                if x.dtype == np.uint16:\n",
        "                    return x.astype(np.int32)\n",
        "                return x\n",
        "\n",
        "        # uint16 dtype is used to conserve disk space and memory\n",
        "        # you can optimize this dataset code to keep it as uint16 and process that\n",
        "        # dtype of data yourself. for simplicity we simply cast to a int32 so\n",
        "        # it can automatically be converted to torch tensors without complaint\n",
        "        self.obs = remove_np_uint16(self.obs)\n",
        "\n",
        "        if device is not None:\n",
        "            self.actions = common.to_tensor(self.actions, device=device)\n",
        "            self.obs = common.to_tensor(self.obs, device=device)\n",
        "            self.terminated = common.to_tensor(self.terminated, device=device)\n",
        "            self.truncated = common.to_tensor(self.truncated, device=device)\n",
        "            if self.rewards is not None:\n",
        "                self.rewards = common.to_tensor(self.rewards, device=device)\n",
        "            if self.success is not None:\n",
        "                self.success = common.to_tensor(self.terminated, device=device)\n",
        "            if self.fail is not None:\n",
        "                self.fail = common.to_tensor(self.truncated, device=device)\n",
        "\n",
        "\n",
        "\n",
        "        # Added code for diffusion policy\n",
        "\n",
        "         # Initialize index lists and stat dicts\n",
        "        self.indices = create_sample_indices(\n",
        "            episode_ends=self.truncated,\n",
        "            sequence_length=self.pred_horizon,\n",
        "            pad_before=self.obs_horizon - 1,\n",
        "            pad_after=self.action_horizon - 1\n",
        "        )\n",
        "\n",
        "        # normalize observations between -1 and 1\n",
        "        if self.normalize:\n",
        "            self.obs = normalize_data(self.obs, self.terminated)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # all possible sequenzes of the dataset\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Change data to fit diffusion policy\n",
        "        buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx = self.indices[idx]\n",
        "\n",
        "        train_data = dict(\n",
        "            obs_agent_qpos=self.obs[\"agent\"][\"qpos\"],\n",
        "            obs_agent_qvel=self.obs[\"agent\"][\"qvel\"],\n",
        "            obs_xyzw=self.obs[\"pointcloud\"][\"xyzw\"],\n",
        "            obs_rgb=self.obs[\"pointcloud\"][\"rgb\"],\n",
        "            obs_segmentation=self.obs[\"pointcloud\"][\"segmentation\"],\n",
        "            actions=self.actions,\n",
        "        )\n",
        "\n",
        "\n",
        "        sampled = sample_sequence(\n",
        "            train_data=train_data,\n",
        "            sequence_length=self.pred_horizon,\n",
        "            buffer_start_idx=buffer_start_idx,\n",
        "            buffer_end_idx=buffer_end_idx,\n",
        "            sample_start_idx=sample_start_idx,\n",
        "            sample_end_idx=sample_end_idx\n",
        "        )\n",
        "\n",
        "        # discard unused observations in the sequence\n",
        "        sampled['obs_agent_qpos'] = sampled['obs_agent_qpos'][:self.obs_horizon,:]\n",
        "        sampled['obs_agent_qvel'] = sampled['obs_agent_qvel'][:self.obs_horizon,:]\n",
        "        sampled['obs_xyzw'] = sampled['obs_xyzw'][:self.obs_horizon,:]\n",
        "        sampled['obs_rgb'] = sampled['obs_rgb'][:self.obs_horizon,:]\n",
        "        sampled['obs_segmentation'] = sampled['obs_segmentation'][:self.obs_horizon,:]\n",
        "\n",
        "        sampled['obs_agent_qpos'] = common.to_tensor(sampled['obs_agent_qpos'], device=self.device)\n",
        "        sampled['obs_agent_qvel'] = common.to_tensor(sampled['obs_agent_qvel'], device=self.device)\n",
        "        sampled['obs_xyzw'] = common.to_tensor(sampled['obs_xyzw'], device=self.device)\n",
        "        sampled['obs_rgb'] = common.to_tensor(sampled['obs_rgb'], device=self.device)\n",
        "        sampled['obs_segmentation'] = common.to_tensor(sampled['obs_segmentation'], device=self.device)\n",
        "        sampled['actions'] = common.to_tensor(sampled['actions'], device=self.device)\n",
        "        return sampled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RITg5zc3fKlX",
        "outputId": "5901cf42-51e3-47f5-9f5c-5f666b4c3808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sapien/_vulkan_tricks.py:21: UserWarning: Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\n",
            "  warn(\"Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\")\n",
            "/usr/local/lib/python3.10/dist-packages/sapien/_vulkan_tricks.py:37: UserWarning: Failed to find Vulkan ICD file. This is probably due to an incorrect or partial installation of the NVIDIA driver. SAPIEN will attempt to provide an ICD file anyway but it may not work.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sapien/_vulkan_tricks.py:59: UserWarning: Failed to find glvnd ICD file. This is probably due to an incorrect or partial installation of the NVIDIA driver. SAPIEN will attempt to provide an ICD file anyway but it may not work.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrx63qFSGXo0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import scipy.spatial.distance\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from tqdm import tqdm\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQhXd4KWU1Ga"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python -m mani_skill.utils.download_demo all\n",
        "!python -m mani_skill.trajectory.replay_trajectory \\\n",
        "  --traj-path /root/.maniskill/demos/PickCube-v1/rl/trajectory.h5 \\\n",
        "  --save-traj \\\n",
        "  --obs-mode pointcloud\n",
        "!cp -r /root/.maniskill/demos /content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O0eqSuJHKt8",
        "outputId": "99f2d84a-55bb-4c59-992e-cc057ffd122e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejrPf8cg5GV8",
        "outputId": "4be9b667-2bac-4d8b-cc13-813bea52a448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading Episodes: 100%|\u001b[32m██████████\u001b[0m| 700/700 [25:21<00:00,  2.17s/it]\n"
          ]
        }
      ],
      "source": [
        "#%cd /content/uni-dll-nr2-jan\n",
        "#import data_classes\n",
        "#from data_classes.helper import *\n",
        "#from data_classes import pointCloudDataClass\n",
        "#from data_classes.pointCloudDataClass import PointCloudManiSkillTrajectoryDataset\n",
        "\n",
        "pred_horizon = 16\n",
        "obs_horizon = 2\n",
        "action_horizon = 2\n",
        "\n",
        "data = PointCloudManiSkillTrajectoryDataset('/content/demos/PickCube-v1/rl/trajectory.pointcloud.pd_joint_delta_pos.h5',\n",
        "                                            pred_horizon, obs_horizon, action_horizon, load_count = 700)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx8lDCBp8HHI"
      },
      "outputs": [],
      "source": [
        "import torch, torch_cluster\n",
        "from torch_geometric.nn import fps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalize(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        assert len(pointcloud.shape)==2\n",
        "\n",
        "        norm_pointcloud = pointcloud - torch.mean(pointcloud, axis=0)\n",
        "        norm_pointcloud /= torch.max(torch.norm(norm_pointcloud, p=2, dim=1))\n",
        "\n",
        "        return  norm_pointcloud"
      ],
      "metadata": {
        "id": "0ZcMIaEYStzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lSodsizhtI3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title POINT NET IMPLEMENTATION\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Tnet(nn.Module):\n",
        "   def __init__(self, k=3):\n",
        "      super().__init__()\n",
        "      self.k=k\n",
        "      self.conv1 = nn.Conv1d(k,64,1)\n",
        "      self.conv2 = nn.Conv1d(64,128,1)\n",
        "      self.conv3 = nn.Conv1d(128,1024,1)\n",
        "      self.fc1 = nn.Linear(1024,512)\n",
        "      self.fc2 = nn.Linear(512,256)\n",
        "      self.fc3 = nn.Linear(256,k*k)\n",
        "\n",
        "      self.bn1 = nn.BatchNorm1d(64)\n",
        "      self.bn2 = nn.BatchNorm1d(128)\n",
        "      self.bn3 = nn.BatchNorm1d(1024)\n",
        "      self.bn4 = nn.BatchNorm1d(512)\n",
        "      self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "\n",
        "   def forward(self, input):\n",
        "      # input.shape == (bs,n,3)\n",
        "      bs = input.size(0)\n",
        "      xb = F.relu(self.bn1(self.conv1(input)))\n",
        "      xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "      xb = F.relu(self.bn3(self.conv3(xb)))\n",
        "      pool = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "      flat = nn.Flatten(1)(pool)\n",
        "      xb = F.relu(self.bn4(self.fc1(flat)))\n",
        "      xb = F.relu(self.bn5(self.fc2(xb)))\n",
        "\n",
        "      #initialize as identity\n",
        "      init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n",
        "      if xb.is_cuda:\n",
        "        init=init.cuda()\n",
        "      matrix = self.fc3(xb).view(-1,self.k,self.k) + init\n",
        "      return matrix\n",
        "\n",
        "\n",
        "class Transform(nn.Module):\n",
        "   def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input_transform = Tnet(k=3)\n",
        "        self.feature_transform = Tnet(k=64)\n",
        "        self.conv1 = nn.Conv1d(3,64,1)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64,128,1)\n",
        "        self.conv3 = nn.Conv1d(128,1024,1)\n",
        "\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "\n",
        "   def forward(self, input):\n",
        "        matrix3x3 = self.input_transform(input)\n",
        "        # batch matrix multiplication\n",
        "        xb = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)\n",
        "\n",
        "        xb = F.relu(self.bn1(self.conv1(xb)))\n",
        "\n",
        "        matrix64x64 = self.feature_transform(xb)\n",
        "        xb = torch.bmm(torch.transpose(xb,1,2), matrix64x64).transpose(1,2)\n",
        "\n",
        "        xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "        xb = self.bn3(self.conv3(xb))\n",
        "        xb = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "        output = nn.Flatten(1)(xb)\n",
        "        return output, matrix3x3, matrix64x64\n",
        "\n",
        "class PointNet(nn.Module):\n",
        "    def __init__(self, classes = 10):\n",
        "        super().__init__()\n",
        "        self.transform = Transform()\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, classes)\n",
        "\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        xb, matrix3x3, matrix64x64 = self.transform(input)\n",
        "        xb = F.relu(self.bn1(self.fc1(xb)))\n",
        "        xb = F.relu(self.bn2(self.dropout(self.fc2(xb))))\n",
        "        #output = self.fc3(xb)\n",
        "        output = xb\n",
        "        return self.logsoftmax(output) #, matrix3x3, matrix64x64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1p8PJdcYHlNF"
      },
      "outputs": [],
      "source": [
        "#@title 1D CONDITIONAL U-NET\n",
        "\n",
        "from typing import Tuple, Sequence, Dict, Union, Optional, Callable\n",
        "#import zarr\n",
        "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
        "from diffusers.training_utils import EMAModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class Downsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Upsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Conv1dBlock(nn.Module):\n",
        "    '''\n",
        "        Conv1d --> GroupNorm --> Mish\n",
        "    '''\n",
        "\n",
        "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
        "            nn.GroupNorm(n_groups, out_channels),\n",
        "            nn.Mish(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class ConditionalResidualBlock1D(nn.Module):\n",
        "    def __init__(self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            cond_dim,\n",
        "            kernel_size=3,\n",
        "            n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "        ])\n",
        "\n",
        "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
        "        # predicts per-channel scale and bias\n",
        "        cond_channels = out_channels * 2\n",
        "        self.out_channels = out_channels\n",
        "        self.cond_encoder = nn.Sequential(\n",
        "            nn.Mish(),\n",
        "            nn.Linear(cond_dim, cond_channels),\n",
        "            nn.Unflatten(-1, (-1, 1))\n",
        "        )\n",
        "\n",
        "        # make sure dimensions compatible\n",
        "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
        "            if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        '''\n",
        "            x : [ batch_size x in_channels x horizon ]\n",
        "            cond : [ batch_size x cond_dim]\n",
        "\n",
        "            returns:\n",
        "            out : [ batch_size x out_channels x horizon ]\n",
        "        '''\n",
        "        out = self.blocks[0](x)\n",
        "        embed = self.cond_encoder(cond)\n",
        "\n",
        "        embed = embed.reshape(\n",
        "            embed.shape[0], 2, self.out_channels, 1)\n",
        "        scale = embed[:,0,...]\n",
        "        bias = embed[:,1,...]\n",
        "        out = scale * out + bias\n",
        "\n",
        "        out = self.blocks[1](out)\n",
        "        out = out + self.residual_conv(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConditionalUnet1D(nn.Module):\n",
        "    def __init__(self,\n",
        "        input_dim,\n",
        "        global_cond_dim,\n",
        "        diffusion_step_embed_dim=256,\n",
        "        down_dims=[256,512,1024],\n",
        "        kernel_size=5,\n",
        "        n_groups=8\n",
        "        ):\n",
        "        \"\"\"\n",
        "        input_dim: Dim of actions.\n",
        "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
        "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
        "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
        "        down_dims: Channel size for each UNet level.\n",
        "          The length of this array determines numebr of levels.\n",
        "        kernel_size: Conv kernel size\n",
        "        n_groups: Number of groups for GroupNorm\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        all_dims = [input_dim] + list(down_dims)\n",
        "        start_dim = down_dims[0]\n",
        "\n",
        "        dsed = diffusion_step_embed_dim\n",
        "        diffusion_step_encoder = nn.Sequential(\n",
        "            SinusoidalPosEmb(dsed),\n",
        "            nn.Linear(dsed, dsed * 4),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(dsed * 4, dsed),\n",
        "        )\n",
        "        cond_dim = dsed + global_cond_dim\n",
        "\n",
        "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
        "        mid_dim = all_dims[-1]\n",
        "        self.mid_modules = nn.ModuleList([\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        down_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            down_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_out, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out, dim_out, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        up_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            up_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        final_conv = nn.Sequential(\n",
        "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
        "            nn.Conv1d(start_dim, input_dim, 1),\n",
        "        )\n",
        "\n",
        "        self.diffusion_step_encoder = diffusion_step_encoder\n",
        "        self.up_modules = up_modules\n",
        "        self.down_modules = down_modules\n",
        "        self.final_conv = final_conv\n",
        "\n",
        "        print(\"number of parameters: {:e}\".format(\n",
        "            sum(p.numel() for p in self.parameters()))\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "            sample: torch.Tensor,\n",
        "            timestep: Union[torch.Tensor, float, int],\n",
        "            global_cond=None):\n",
        "        \"\"\"\n",
        "        x: (B,T,input_dim)\n",
        "        timestep: (B,) or int, diffusion step\n",
        "        global_cond: (B,global_cond_dim)\n",
        "        output: (B,T,input_dim)\n",
        "        \"\"\"\n",
        "        # (B,T,C)\n",
        "        sample = sample.moveaxis(-1,-2)\n",
        "        # (B,C,T)\n",
        "\n",
        "        # 1. time\n",
        "        timesteps = timestep\n",
        "        if not torch.is_tensor(timesteps):\n",
        "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
        "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
        "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
        "            timesteps = timesteps[None].to(sample.device)\n",
        "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
        "        timesteps = timesteps.expand(sample.shape[0])\n",
        "\n",
        "        global_feature = self.diffusion_step_encoder(timesteps)\n",
        "\n",
        "        if global_cond is not None:\n",
        "            global_feature = torch.cat([\n",
        "                global_feature, global_cond\n",
        "            ], axis=-1)\n",
        "\n",
        "        x = sample\n",
        "        h = []\n",
        "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        for mid_module in self.mid_modules:\n",
        "            x = mid_module(x, global_feature)\n",
        "\n",
        "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        # (B,C,T)\n",
        "        x = x.moveaxis(-1,-2)\n",
        "        # (B,T,C)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9duBdf6oWFW7"
      },
      "source": [
        "### SETTING UP THE NETWORKS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7i2rtDmHQyZ",
        "outputId": "82d663c3-2918-43d6-c8af-b86b7dc95a29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 7.281870e+07\n"
          ]
        }
      ],
      "source": [
        "# if you have multiple camera views, use seperate encoder weights for each view.\n",
        "encoder = PointNet()\n",
        "\n",
        "# PointNet has output dim of 256\n",
        "encoded_feature_dim = 256\n",
        "# agent_pos is 9 dimensional\n",
        "lowdim_obs_dim = 9\n",
        "# observation feature has 265 dims in total per step\n",
        "obs_dim = encoded_feature_dim + lowdim_obs_dim\n",
        "action_dim = 8\n",
        "\n",
        "# create network object\n",
        "noise_pred_net = ConditionalUnet1D(\n",
        "    input_dim=action_dim,\n",
        "    global_cond_dim=obs_dim*obs_horizon\n",
        ")\n",
        "\n",
        "# the final arch has 2 parts\n",
        "nets = nn.ModuleDict({\n",
        "    'encoder': encoder,\n",
        "    'noise_pred_net': noise_pred_net\n",
        "})\n",
        "nets['encoder'].eval()\n",
        "\n",
        "# demo\n",
        "with torch.no_grad():\n",
        "    # example inputs\n",
        "    cloud = torch.zeros((2, 1024,3))\n",
        "    agent_pos = torch.zeros((1, obs_horizon, 9))\n",
        "    # vision encoder\n",
        "    image_features = nets['encoder'](\n",
        "        cloud.transpose(1,2))\n",
        "    # (2,512)\n",
        "    image_features = image_features.reshape(1,2,256)\n",
        "    # (1,2,512)\n",
        "    obs = torch.cat([image_features, agent_pos],dim=-1)\n",
        "    # (1,2,514)\n",
        "\n",
        "    noised_action = torch.randn((1, pred_horizon, action_dim))\n",
        "    diffusion_iter = torch.zeros((1,))\n",
        "\n",
        "    # the noise prediction network\n",
        "    # takes noisy action, diffusion iteration and observation as input\n",
        "    # predicts the noise added to action\n",
        "    noise = nets['noise_pred_net'](\n",
        "        sample=noised_action,\n",
        "        timestep=diffusion_iter,\n",
        "        global_cond=obs.flatten(start_dim=1))\n",
        "\n",
        "    # illustration of removing noise\n",
        "    # the actual noise removal is performed by NoiseScheduler\n",
        "    # and is dependent on the diffusion noise schedule\n",
        "    denoised_action = noised_action - noise\n",
        "\n",
        "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
        "num_diffusion_iters = 100\n",
        "noise_scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=num_diffusion_iters,\n",
        "    # the choise of beta schedule has big impact on performance\n",
        "    # we found squared cosine works the best\n",
        "    beta_schedule='squaredcos_cap_v2',\n",
        "    # clip output to [-1,1] to improve stability\n",
        "    clip_sample=True,\n",
        "    # our network predicts noise (instead of denoised action)\n",
        "    prediction_type='epsilon'\n",
        ")\n",
        "\n",
        "# device transfer\n",
        "device = torch.device('cuda')\n",
        "_ = nets.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj3QHz8pIVGG",
        "outputId": "61739655-3cf3-4eb1-8a71-e25158373430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Batch:   0%|          | 0/414 [00:00<?, ?it/s]\u001b[A\n",
            "Batch:   0%|          | 0/414 [00:02<?, ?it/s, loss=0.0802]\u001b[A\n",
            "Batch:   0%|          | 1/414 [00:02<18:01,  2.62s/it, loss=0.0802]\u001b[A\n",
            "Batch:   0%|          | 1/414 [00:05<18:01,  2.62s/it, loss=0.0879]\u001b[A\n",
            "Batch:   0%|          | 2/414 [00:05<17:20,  2.53s/it, loss=0.0879]\u001b[A\n",
            "Batch:   0%|          | 2/414 [00:07<17:20,  2.53s/it, loss=0.075] \u001b[A\n",
            "Batch:   1%|          | 3/414 [00:07<17:07,  2.50s/it, loss=0.075]\u001b[A\n",
            "Batch:   1%|          | 3/414 [00:10<17:07,  2.50s/it, loss=0.138]\u001b[A\n",
            "Batch:   1%|          | 4/414 [00:10<17:00,  2.49s/it, loss=0.138]\u001b[A\n",
            "Batch:   1%|          | 4/414 [00:12<17:00,  2.49s/it, loss=0.0808]\u001b[A\n",
            "Batch:   1%|          | 5/414 [00:12<16:54,  2.48s/it, loss=0.0808]\u001b[A\n",
            "Batch:   1%|          | 5/414 [00:14<16:54,  2.48s/it, loss=0.0701]\u001b[A\n",
            "Batch:   1%|▏         | 6/414 [00:14<16:49,  2.47s/it, loss=0.0701]\u001b[A\n",
            "Batch:   1%|▏         | 6/414 [00:17<16:49,  2.47s/it, loss=0.064] \u001b[A\n",
            "Batch:   2%|▏         | 7/414 [00:17<16:45,  2.47s/it, loss=0.064]\u001b[A\n",
            "Batch:   2%|▏         | 7/414 [00:19<16:45,  2.47s/it, loss=0.0821]\u001b[A\n",
            "Batch:   2%|▏         | 8/414 [00:19<16:43,  2.47s/it, loss=0.0821]\u001b[A\n",
            "Batch:   2%|▏         | 8/414 [00:22<16:43,  2.47s/it, loss=0.0633]\u001b[A\n",
            "Batch:   2%|▏         | 9/414 [00:22<16:41,  2.47s/it, loss=0.0633]\u001b[A\n",
            "Batch:   2%|▏         | 9/414 [00:24<16:41,  2.47s/it, loss=0.0698]\u001b[A\n",
            "Batch:   2%|▏         | 10/414 [00:24<16:37,  2.47s/it, loss=0.0698]\u001b[A\n",
            "Batch:   2%|▏         | 10/414 [00:27<16:37,  2.47s/it, loss=0.0624]\u001b[A\n",
            "Batch:   3%|▎         | 11/414 [00:27<16:34,  2.47s/it, loss=0.0624]\u001b[A\n",
            "Batch:   3%|▎         | 11/414 [00:29<16:34,  2.47s/it, loss=0.0482]\u001b[A\n",
            "Batch:   3%|▎         | 12/414 [00:29<16:31,  2.47s/it, loss=0.0482]\u001b[A\n",
            "Batch:   3%|▎         | 12/414 [00:32<16:31,  2.47s/it, loss=0.081] \u001b[A\n",
            "Batch:   3%|▎         | 13/414 [00:32<16:29,  2.47s/it, loss=0.081]\u001b[A\n",
            "Batch:   3%|▎         | 13/414 [00:34<16:29,  2.47s/it, loss=0.0624]\u001b[A\n",
            "Batch:   3%|▎         | 14/414 [00:34<16:26,  2.47s/it, loss=0.0624]\u001b[A\n",
            "Batch:   3%|▎         | 14/414 [00:37<16:26,  2.47s/it, loss=0.0487]\u001b[A\n",
            "Batch:   4%|▎         | 15/414 [00:37<16:23,  2.47s/it, loss=0.0487]\u001b[A\n",
            "Batch:   4%|▎         | 15/414 [00:39<16:23,  2.47s/it, loss=0.0689]\u001b[A\n",
            "Batch:   4%|▍         | 16/414 [00:39<16:20,  2.46s/it, loss=0.0689]\u001b[A\n",
            "Batch:   4%|▍         | 16/414 [00:42<16:20,  2.46s/it, loss=0.0893]\u001b[A\n",
            "Batch:   4%|▍         | 17/414 [00:42<16:19,  2.47s/it, loss=0.0893]\u001b[A\n",
            "Batch:   4%|▍         | 17/414 [00:44<16:19,  2.47s/it, loss=0.078] \u001b[A\n",
            "Batch:   4%|▍         | 18/414 [00:44<16:18,  2.47s/it, loss=0.078]\u001b[A\n",
            "Batch:   4%|▍         | 18/414 [00:47<16:18,  2.47s/it, loss=0.0882]\u001b[A\n",
            "Batch:   5%|▍         | 19/414 [00:47<16:14,  2.47s/it, loss=0.0882]\u001b[A\n",
            "Batch:   5%|▍         | 19/414 [00:49<16:14,  2.47s/it, loss=0.0973]\u001b[A\n",
            "Batch:   5%|▍         | 20/414 [00:49<16:11,  2.46s/it, loss=0.0973]\u001b[A\n",
            "Batch:   5%|▍         | 20/414 [00:51<16:11,  2.46s/it, loss=0.0755]\u001b[A\n",
            "Batch:   5%|▌         | 21/414 [00:51<16:08,  2.46s/it, loss=0.0755]\u001b[A\n",
            "Batch:   5%|▌         | 21/414 [00:54<16:08,  2.46s/it, loss=0.0932]\u001b[A\n",
            "Batch:   5%|▌         | 22/414 [00:54<16:06,  2.46s/it, loss=0.0932]\u001b[A\n",
            "Batch:   5%|▌         | 22/414 [00:56<16:06,  2.46s/it, loss=0.0703]\u001b[A\n",
            "Batch:   6%|▌         | 23/414 [00:56<16:03,  2.46s/it, loss=0.0703]\u001b[A\n",
            "Batch:   6%|▌         | 23/414 [00:59<16:03,  2.46s/it, loss=0.0899]\u001b[A\n",
            "Batch:   6%|▌         | 24/414 [00:59<15:59,  2.46s/it, loss=0.0899]\u001b[A\n",
            "Batch:   6%|▌         | 24/414 [01:01<15:59,  2.46s/it, loss=0.0717]\u001b[A\n",
            "Batch:   6%|▌         | 25/414 [01:01<15:56,  2.46s/it, loss=0.0717]\u001b[A\n",
            "Batch:   6%|▌         | 25/414 [01:04<15:56,  2.46s/it, loss=0.049] \u001b[A\n",
            "Batch:   6%|▋         | 26/414 [01:04<15:54,  2.46s/it, loss=0.049]\u001b[A\n",
            "Batch:   6%|▋         | 26/414 [01:06<15:54,  2.46s/it, loss=0.0706]\u001b[A\n",
            "Batch:   7%|▋         | 27/414 [01:06<15:52,  2.46s/it, loss=0.0706]\u001b[A\n",
            "Batch:   7%|▋         | 27/414 [01:09<15:52,  2.46s/it, loss=0.0674]\u001b[A\n",
            "Batch:   7%|▋         | 28/414 [01:09<15:49,  2.46s/it, loss=0.0674]\u001b[A\n",
            "Batch:   7%|▋         | 28/414 [01:11<15:49,  2.46s/it, loss=0.104] \u001b[A\n",
            "Batch:   7%|▋         | 29/414 [01:11<15:45,  2.46s/it, loss=0.104]\u001b[A"
          ]
        }
      ],
      "source": [
        "num_epochs = 2\n",
        "\n",
        "train_data = DataLoader(data, batch_size = 64)\n",
        "\n",
        "# Exponential Moving Average\n",
        "# accelerates training and improves stability\n",
        "# holds a copy of the model weights\n",
        "ema = EMAModel(\n",
        "    parameters=nets.parameters(),\n",
        "    power=0.75)\n",
        "\n",
        "# Standard ADAM optimizer\n",
        "# Note that EMA parametesr are not optimized\n",
        "optimizer = torch.optim.AdamW(\n",
        "    params=nets.parameters(),\n",
        "    lr=1e-4, weight_decay=1e-6)\n",
        "\n",
        "# Cosine LR schedule with linear warmup\n",
        "lr_scheduler = get_scheduler(\n",
        "    name='cosine',\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=500,\n",
        "    num_training_steps=len(train_data) * num_epochs\n",
        ")\n",
        "\n",
        "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
        "    # epoch loop\n",
        "    for epoch_idx in tglobal:\n",
        "        epoch_loss = list()\n",
        "        # batch loop\n",
        "        with tqdm(train_data, desc='Batch', leave=False) as tepoch:\n",
        "            for nbatch in tepoch:\n",
        "                # data normalized in dataset\n",
        "                # device transfer\n",
        "                ncloud = nbatch['obs_xyzw'][:, :, :, :3].to(device)\n",
        "\n",
        "                for cloud in range(ncloud.shape[0]):\n",
        "                  for seq in range(ncloud.shape[1]):\n",
        "                    ncloud[cloud][seq] = Normalize()(ncloud[cloud][seq])\n",
        "\n",
        "                nagent_pos = nbatch['obs_agent_qpos'].to(device)\n",
        "                naction = nbatch['actions'].to(device)\n",
        "                B = nagent_pos.shape[0]\n",
        "\n",
        "                indeces = []\n",
        "                down_sampled_clouds = torch.zeros(B, obs_horizon, 1024, 3)\n",
        "\n",
        "                for seq in range(ncloud.shape[0]):\n",
        "                    indeces.append(fps(ncloud[seq][0], ratio = 1024/ncloud[seq].shape[1]))\n",
        "\n",
        "                for seq in range(B):\n",
        "                  for frame in range(obs_horizon):\n",
        "                    down_sampled_clouds[seq][frame] = ncloud[seq][frame][indeces[seq]]\n",
        "\n",
        "\n",
        "                # encoder vision features\n",
        "                image_features = torch.zeros(B, obs_horizon, 256).to(device)\n",
        "\n",
        "                for frame in range(obs_horizon):\n",
        "                  image_features[:, frame, :] = nets['encoder'](down_sampled_clouds[:,frame,:,:].transpose(1,2).to(device))\n",
        "\n",
        "                # (B,obs_horizon,D)\n",
        "\n",
        "                # concatenate vision feature and low-dim obs\n",
        "                obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n",
        "                obs_cond = obs_features.flatten(start_dim=1)\n",
        "                # (B, obs_horizon * obs_dim)\n",
        "\n",
        "                # sample noise to add to actions\n",
        "                noise = torch.randn(naction.shape, device=device)\n",
        "\n",
        "                # sample a diffusion iteration for each data point\n",
        "                timesteps = torch.randint(\n",
        "                    0, noise_scheduler.config.num_train_timesteps,\n",
        "                    (B,), device=device\n",
        "                ).long()\n",
        "\n",
        "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
        "                # (this is the forward diffusion process)\n",
        "                noisy_actions = noise_scheduler.add_noise(\n",
        "                    naction, noise, timesteps)\n",
        "\n",
        "                # predict the noise residual\n",
        "                noise_pred = noise_pred_net(\n",
        "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
        "\n",
        "                # L2 loss\n",
        "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
        "\n",
        "                # optimize\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                # step lr scheduler every batch\n",
        "                # this is different from standard pytorch behavior\n",
        "                lr_scheduler.step()\n",
        "\n",
        "                # update Exponential Moving Average of the model weights\n",
        "                ema.step(nets.parameters())\n",
        "\n",
        "                # logging\n",
        "                loss_cpu = loss.item()\n",
        "                epoch_loss.append(loss_cpu)\n",
        "                tepoch.set_postfix(loss=loss_cpu)\n",
        "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
        "\n",
        "# Weights of the EMA model\n",
        "# is used for inference\n",
        "ema_nets = nets\n",
        "ema.copy_to(ema_nets.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ema_nets = nets\n",
        "ema.copy_to(ema_nets.parameters())"
      ],
      "metadata": {
        "id": "ZEXCPa8CLHQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EVALUATION"
      ],
      "metadata": {
        "id": "JGgYb292Diht"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9undJKEHUQgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c627c10-64bb-4a0e-a837-51bad1963fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -------------------------------------------------------------------------- #\n",
            "Task ID: PickCube-v1, 1 parallel environments, sim_backend=cpu\n",
            "obs_mode=pointcloud, control_mode=pd_joint_delta_pos\n",
            "render_mode=None, sensor_details=RGBD(128x128)\n",
            "sim_freq=100, control_freq=20\n",
            "observation space: Dict('agent': Dict('qpos': Box(-inf, inf, (1, 9), float32), 'qvel': Box(-inf, inf, (1, 9), float32)), 'extra': Dict('is_grasped': Box(False, True, (1,), bool), 'tcp_pose': Box(-inf, inf, (1, 7), float32), 'goal_pos': Box(-inf, inf, (1, 3), float32)), 'sensor_param': Dict('base_camera': Dict('extrinsic_cv': Box(-inf, inf, (1, 3, 4), float32), 'cam2world_gl': Box(-inf, inf, (1, 4, 4), float32), 'intrinsic_cv': Box(-inf, inf, (1, 3, 3), float32))), 'sensor_data': Dict(), 'pointcloud': Dict('xyzw': Box(-inf, inf, (1, 16384, 4), float32), 'rgb': Box(0, 255, (1, 16384, 3), uint8), 'segmentation': Box(-32768, 32767, (1, 16384, 1), int16)))\n",
            "(single) action space: Box(-1.0, 1.0, (8,), float32)\n",
            "# -------------------------------------------------------------------------- #\n"
          ]
        }
      ],
      "source": [
        "# limit enviornment interaction to 200 steps before termination\n",
        "import gymnasium as gym\n",
        "import collections\n",
        "import mani_skill.envs\n",
        "\n",
        "\n",
        "max_steps = 80\n",
        "# use a seed >200 to avoid initial states seen in the training dataset\n",
        "env = gym.make(\"PickCube-v1\", obs_mode=\"pointcloud\")\n",
        "env.unwrapped.print_sim_details()\n",
        "#env.seed(100000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get first observation\n",
        "obs, info = env.reset()\n",
        "\n",
        "# keep a queue of last 2 steps of observations\n",
        "obs_deque = collections.deque(\n",
        "    [obs] * obs_horizon, maxlen=obs_horizon)\n",
        "# save visualization and rewards\n",
        "#imgs = [env.render()]\n",
        "rewards = list()\n",
        "done = False\n",
        "step_idx = 0\n",
        "nets['encoder'].eval()\n",
        "\n",
        "with tqdm(total=max_steps, desc=\"Eval PickCube\") as pbar:\n",
        "    while not done:\n",
        "        B = 1\n",
        "        # stack the last obs_horizon number of observations\n",
        "\n",
        "        clouds = [x['pointcloud']['xyzw'].squeeze()[:, :3] for x in obs_deque]\n",
        "        nagent_poses = np.vstack([x['agent']['qpos'] for x in obs_deque])\n",
        "        # normalize observation\n",
        "        clouds = [x[fps(x, ratio = 1024/x.shape[0])] for x in clouds]\n",
        "        clouds = np.stack([Normalize()(x) for x in clouds])\n",
        "        #nagent_poses = normalize_data(agent_poses, stats=stats['agent_pos'])\n",
        "        # images are already normalized to [0,1]\n",
        "\n",
        "\n",
        "        # device transfer\n",
        "        nclouds = torch.from_numpy(clouds).to(device, dtype=torch.float32)\n",
        "        # (2,3,96,96)\n",
        "        nagent_poses = torch.from_numpy(nagent_poses).to(device, dtype=torch.float32).unsqueeze(0)\n",
        "        # (2,2)\n",
        "\n",
        "\n",
        "        # infer action\n",
        "        with torch.no_grad():\n",
        "            # get image features\n",
        "            image_features = torch.zeros(1, obs_horizon, 256).to(device)\n",
        "            for frame in range(obs_horizon):\n",
        "                  image_features[0, frame] = nets['encoder'](nclouds[frame].T.unsqueeze(0))\n",
        "\n",
        "            image_features = image_features.to(device)\n",
        "\n",
        "            # (2,256)\n",
        "\n",
        "            # concat with low-dim observations\n",
        "            obs_features = torch.cat([image_features, nagent_poses], dim=-1)\n",
        "\n",
        "            # reshape observation to (B,obs_horizon*obs_dim)\n",
        "            obs_cond = obs_features.unsqueeze(0).flatten(start_dim=1)\n",
        "\n",
        "            # initialize action from Guassian noise\n",
        "            noisy_action = torch.randn(\n",
        "                (B, pred_horizon, action_dim), device=device)\n",
        "            naction = noisy_action\n",
        "\n",
        "            # init scheduler\n",
        "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
        "\n",
        "            for k in noise_scheduler.timesteps:\n",
        "                # predict noise\n",
        "                noise_pred = ema_nets['noise_pred_net'](\n",
        "                    sample=naction,\n",
        "                    timestep=k,\n",
        "                    global_cond=obs_cond\n",
        "                )\n",
        "\n",
        "                # inverse diffusion step (remove noise)\n",
        "                naction = noise_scheduler.step(\n",
        "                    model_output=noise_pred,\n",
        "                    timestep=k,\n",
        "                    sample=naction\n",
        "                ).prev_sample\n",
        "\n",
        "        # unnormalize action\n",
        "        naction = naction.detach().to('cpu').numpy()\n",
        "        # (B, pred_horizon, action_dim)\n",
        "        naction = naction[0]\n",
        "        action_pred = naction\n",
        "\n",
        "        # only take action_horizon number of actions\n",
        "        start = obs_horizon - 1\n",
        "        end = start + action_horizon\n",
        "        action = action_pred[start:end,:]\n",
        "        # (action_horizon, action_dim)\n",
        "\n",
        "        # execute action_horizon number of steps\n",
        "        # without replanning\n",
        "        for i in range(len(action)):\n",
        "            # stepping env\n",
        "            obs, reward, done, _, info = env.step(action[i])\n",
        "            # save observations\n",
        "            obs_deque.append(obs)\n",
        "            # and reward/vis\n",
        "            rewards.append(reward)\n",
        "            #imgs.append(env.render())\n",
        "\n",
        "            # update progress bar\n",
        "            step_idx += 1\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix(reward=reward)\n",
        "            if step_idx > max_steps:\n",
        "                done = True\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "# print out the maximum target coverage\n",
        "print('Score: ', max(rewards))\n",
        "import skimage.transform as st\n",
        "from skvideo.io import vwrite\n",
        "from IPython.display import Video\n",
        "# visualize\n",
        "from IPython.display import Video\n",
        "vwrite('vis.mp4', imgs)\n",
        "Video('vis.mp4', embed=True, width=256, height=256)"
      ],
      "metadata": {
        "id": "APTTrRQvEmCQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "20c253af-c9cf-4126-e729-f08ec926c15c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Eval PushTImageEnv: 81it [01:01,  1.31it/s, reward=tensor([0.0437])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score:  tensor([0.0914])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Improper data input",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-cbc17932eb5a>\u001b[0m in \u001b[0;36m<cell line: 114>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVideo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mvwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vis.mp4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0mVideo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vis.mp4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skvideo/io/io.py\u001b[0m in \u001b[0;36mvwrite\u001b[0;34m(fname, videodata, inputdict, outputdict, backend, verbosity)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0moutputdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mvideodata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideodata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# check that the appropriate videodata size was passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skvideo/utils/__init__.py\u001b[0m in \u001b[0;36mvshape\u001b[0;34m(videodata)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvideodata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Improper data input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrgb2gray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideodata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Improper data input"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SIhA6vkZ2T_1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}