{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHveeBiogmYh"
      },
      "source": [
        "### Pip Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "88703405b6c0416384800b1478a2f906",
            "72b64855bf9c4af991b69b02bead5a4e",
            "52881d861c5d414eb62de2201e7d3ead",
            "eb46ef7dc03b435ead00a36ae45893b6",
            "bf79027210bb4d96a4a922becb7ed04b",
            "4841e65e86d34f33b1b2b13def6cbd73",
            "ea2199014d944cfb844f2f4733b0a117",
            "d12eb65ecaee437884b165364b9ac0a9",
            "b389f174941b44ac868118d1f64a5a42",
            "93083b1ddc4c4d6ba485df10d86db3ee",
            "80c9ea2a67e24c2ba3a7da4e022e09fb"
          ]
        },
        "id": "t-f7_a6zgmYi",
        "outputId": "552a02ac-fc36-4996-b7d9-1286658f648c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mani_skill\n",
            "  Downloading mani_skill-3.0.0b5-py3-none-any.whl (58.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from mani_skill) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mani_skill) (1.11.4)\n",
            "Collecting dacite (from mani_skill)\n",
            "  Downloading dacite-1.8.1-py3-none-any.whl (14 kB)\n",
            "Collecting gymnasium==0.29.1 (from mani_skill)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sapien==3.0.0.b1 (from mani_skill)\n",
            "  Downloading sapien-3.0.0b1-cp310-cp310-manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from mani_skill) (3.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mani_skill) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mani_skill) (4.66.4)\n",
            "Collecting GitPython (from mani_skill)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from mani_skill) (0.9.0)\n",
            "Collecting transforms3d (from mani_skill)\n",
            "  Downloading transforms3d-0.4.2-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trimesh (from mani_skill)\n",
            "  Downloading trimesh-4.4.3-py3-none-any.whl (695 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m695.9/695.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rtree (from mani_skill)\n",
            "  Downloading Rtree-1.3.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (543 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m543.2/543.2 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from mani_skill) (2.31.6)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from mani_skill) (7.34.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from mani_skill) (0.23.5)\n",
            "Collecting mplib>=0.1.1 (from mani_skill)\n",
            "  Downloading mplib-0.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fast-kinematics==0.2.2 (from mani_skill)\n",
            "  Downloading fast_kinematics-0.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (623 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m623.8/623.8 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1->mani_skill) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1->mani_skill) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium==0.29.1->mani_skill)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: requests>=2.22 in /usr/local/lib/python3.10/dist-packages (from sapien==3.0.0.b1->mani_skill) (2.31.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sapien==3.0.0.b1->mani_skill) (4.9.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from sapien==3.0.0.b1->mani_skill) (3.3)\n",
            "Requirement already satisfied: pyperclip in /usr/local/lib/python3.10/dist-packages (from sapien==3.0.0.b1->mani_skill) (1.9.0)\n",
            "Requirement already satisfied: opencv-python>=4.0 in /usr/local/lib/python3.10/dist-packages (from sapien==3.0.0.b1->mani_skill) (4.8.0.76)\n",
            "Collecting toppra>=0.4.0 (from mplib>=0.1.1->mani_skill)\n",
            "  Downloading toppra-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (638 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m638.2/638.2 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from GitPython->mani_skill)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->mani_skill) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->mani_skill) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->mani_skill) (24.1)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio->mani_skill) (9.4.0)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from imageio->mani_skill) (0.5.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from imageio->mani_skill) (5.9.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->mani_skill) (67.7.2)\n",
            "Collecting jedi>=0.16 (from IPython->mani_skill)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython->mani_skill) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->mani_skill) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->mani_skill) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->mani_skill) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->mani_skill) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->mani_skill) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->mani_skill) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->mani_skill) (4.9.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython->mani_skill)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->mani_skill) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->mani_skill) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->mani_skill) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22->sapien==3.0.0.b1->mani_skill) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22->sapien==3.0.0.b1->mani_skill) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22->sapien==3.0.0.b1->mani_skill) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22->sapien==3.0.0.b1->mani_skill) (2024.7.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from toppra>=0.4.0->mplib>=0.1.1->mani_skill) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->toppra>=0.4.0->mplib>=0.1.1->mani_skill) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->toppra>=0.4.0->mplib>=0.1.1->mani_skill) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->toppra>=0.4.0->mplib>=0.1.1->mani_skill) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->toppra>=0.4.0->mplib>=0.1.1->mani_skill) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->toppra>=0.4.0->mplib>=0.1.1->mani_skill) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->toppra>=0.4.0->mplib>=0.1.1->mani_skill) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->toppra>=0.4.0->mplib>=0.1.1->mani_skill) (1.16.0)\n",
            "Installing collected packages: farama-notifications, trimesh, transforms3d, smmap, rtree, jedi, gymnasium, fast-kinematics, dacite, sapien, gitdb, toppra, GitPython, mplib, mani_skill\n",
            "Successfully installed GitPython-3.1.43 dacite-1.8.1 farama-notifications-0.0.4 fast-kinematics-0.2.2 gitdb-4.0.11 gymnasium-0.29.1 jedi-0.19.1 mani_skill-3.0.0b5 mplib-0.1.1 rtree-1.3.0 sapien-3.0.0b1 smmap-5.0.1 toppra-0.6.0 transforms3d-0.4.2 trimesh-4.4.3\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting diffusers\n",
            "  Downloading diffusers-0.29.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.0.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.23.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.19.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, diffusers\n",
            "Successfully installed diffusers-0.29.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sapien/_vulkan_tricks.py:21: UserWarning: Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\n",
            "  warn(\"Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\")\n",
            "/usr/local/lib/python3.10/dist-packages/sapien/_vulkan_tricks.py:37: UserWarning: Failed to find Vulkan ICD file. This is probably due to an incorrect or partial installation of the NVIDIA driver. SAPIEN will attempt to provide an ICD file anyway but it may not work.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sapien/_vulkan_tricks.py:59: UserWarning: Failed to find glvnd ICD file. This is probably due to an incorrect or partial installation of the NVIDIA driver. SAPIEN will attempt to provide an ICD file anyway but it may not work.\n",
            "  warn(\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88703405b6c0416384800b1478a2f906"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# install the package\n",
        "%pip install --upgrade mani_skill\n",
        "# install a version of torch that is compatible with your system\n",
        "%pip install torch torchvision torchaudio numpy diffusers\n",
        "\n",
        "import math\n",
        "from typing import Union\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from mani_skill.utils import common\n",
        "from mani_skill.utils.io_utils import load_json\n",
        "from mani_skill.utils.common import flatten_state_dict\n",
        "import mani_skill.envs\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import IterableDataset, Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Tuple, Sequence, Dict, Union, Optional\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import collections\n",
        "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
        "from diffusers.training_utils import EMAModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# env import\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXw0guO5gmYj",
        "outputId": "b049d277-91cf-417c-956b-880190fa1ddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# File names\n",
        "original_h5_file = '/content/drive/MyDrive/data3/trajectory.state_dict.pd_joint_pos.h5'\n",
        "original_json_file = '/content/drive/MyDrive/data3/trajectory.state_dict.pd_joint_pos.json'\n",
        "training_h5_file = '03_pd_joint_pos_training_set.h5'\n",
        "validation_h5_file = '03_pd_joint_pos_validation_set.h5'\n",
        "training_json_file = '03_pd_joint_pos_training_set.json'\n",
        "validation_json_file = '03_pd_joint_pos_validation_set.json'\n",
        "\n",
        "with open(original_json_file, 'r') as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "with h5py.File(original_h5_file, 'r') as f:\n",
        "    traj_datasets = [key for key in f.keys() if key.startswith('traj_')]\n",
        "\n",
        "    np.random.shuffle(traj_datasets)\n",
        "\n",
        "    split_point = int(0.75 * len(traj_datasets))\n",
        "\n",
        "    training_datasets = traj_datasets[:split_point]\n",
        "    validation_datasets = traj_datasets[split_point:]\n",
        "\n",
        "    with h5py.File(training_h5_file, 'w') as f_train:\n",
        "        for key in training_datasets:\n",
        "            f.copy(key, f_train)\n",
        "\n",
        "    with h5py.File(validation_h5_file, 'w') as f_val:\n",
        "        for key in validation_datasets:\n",
        "            f.copy(key, f_val)\n",
        "\n",
        "dataset_indices = {f'traj_{i}': i for i in range(len(json_data['episodes']))}\n",
        "\n",
        "training_episodes = [json_data['episodes'][dataset_indices[key]] for key in training_datasets]\n",
        "validation_episodes = [json_data['episodes'][dataset_indices[key]] for key in validation_datasets]\n",
        "\n",
        "training_json = {\n",
        "    'env_info': json_data['env_info'],\n",
        "    'episodes': training_episodes\n",
        "}\n",
        "\n",
        "validation_json = {\n",
        "    'env_info': json_data['env_info'],\n",
        "    'episodes': validation_episodes\n",
        "}\n",
        "\n",
        "with open(training_json_file, 'w') as f:\n",
        "    json.dump(training_json, f, indent=4)\n",
        "\n",
        "with open(validation_json_file, 'w') as f:\n",
        "    json.dump(validation_json, f, indent=4)"
      ],
      "metadata": {
        "id": "tGs4aXUTp9C3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGNhfrb0gmYj"
      },
      "source": [
        "### Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_Mv05dogmYj"
      },
      "source": [
        "#### Dataset Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cnTNAtp9gmYj"
      },
      "outputs": [],
      "source": [
        "def load_h5_data(data):\n",
        "    out = dict()\n",
        "    for k in data.keys():\n",
        "        if isinstance(data[k], h5py.Dataset):\n",
        "            out[k] = data[k][:]\n",
        "        else:\n",
        "            out[k] = load_h5_data(data[k])\n",
        "    return out\n",
        "\n",
        "\n",
        "def create_sample_indices(episode_ends: np.ndarray, sequence_length: int, pad_before: int = 0, pad_after: int = 0):\n",
        "    # Currently uses truncated as episode ends which is the end of the episode and not the end of the trajectory\n",
        "    indices = list()\n",
        "    episode_length = 0\n",
        "    episode_index = 1 # Start 1 for human readability\n",
        "    for i in range(len(episode_ends)):\n",
        "        episode_length += 1\n",
        "        if episode_ends[i]:\n",
        "            start_idx = 0 if i <= 0 else i - episode_length + 1\n",
        "            min_start = -pad_before\n",
        "            max_start = episode_length - sequence_length + pad_after\n",
        "\n",
        "            # Create indices for each possible sequence in the episode\n",
        "            for idx in range(min_start, max_start + 1):\n",
        "                buffer_start_idx = max(idx, 0) + start_idx\n",
        "                buffer_end_idx = min(idx + sequence_length, episode_length) + start_idx\n",
        "                start_offset = buffer_start_idx - (idx + start_idx)\n",
        "                end_offset = (idx + sequence_length + start_idx) - buffer_end_idx\n",
        "                sample_start_idx = 0 + start_offset\n",
        "                sample_end_idx = sequence_length - end_offset\n",
        "                indices.append([buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx])\n",
        "            episode_length = 0\n",
        "            episode_index += 1\n",
        "    return np.array(indices)\n",
        "\n",
        "\n",
        "def sample_sequence(train_data, sequence_length, buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx):\n",
        "    result = dict()\n",
        "    for key, input_arr in train_data.items():\n",
        "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
        "        data = sample\n",
        "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
        "            if isinstance(input_arr, torch.Tensor):\n",
        "                data = torch.zeros((sequence_length,) + input_arr.shape[1:], dtype=input_arr.dtype)\n",
        "            else:\n",
        "                data = np.zeros(shape=(sequence_length,) + input_arr.shape[1:], dtype=input_arr.dtype)\n",
        "            if sample_start_idx > 0:\n",
        "                data[:sample_start_idx] = sample[0]\n",
        "            if sample_end_idx < sequence_length:\n",
        "                data[sample_end_idx:] = sample[-1]\n",
        "            data[sample_start_idx:sample_end_idx] = sample\n",
        "        result[key] = data\n",
        "    return result\n",
        "\n",
        "def remove_np_uint16(x: Union[np.ndarray, dict]):\n",
        "            if isinstance(x, dict):\n",
        "                for k in x.keys():\n",
        "                    x[k] = remove_np_uint16(x[k])\n",
        "                return x\n",
        "            else:\n",
        "                if x.dtype == np.uint16:\n",
        "                    return x.astype(np.int32)\n",
        "                return x\n",
        "\n",
        "def convert_observation(obs, task_id):\n",
        "    # adds task_id to the observation\n",
        "    values = list(obs.values())\n",
        "    example = values[0]\n",
        "    if isinstance(example, torch.Tensor):\n",
        "          example = example.numpy()\n",
        "\n",
        "    # add task_id to the observation\n",
        "    task_id_array = np.full((example.shape[0], 1), task_id, dtype=example.dtype)\n",
        "    values.append(task_id_array)\n",
        "    # concatenate all the values\n",
        "    return np.concatenate(values, axis=-1)\n",
        "\n",
        "def get_observations(obs):\n",
        "    #ensoure that the observations are in the correct format\n",
        "    #and ordered correctly across tasks\n",
        "\n",
        "    cleaned_obs = OrderedDict()\n",
        "    cleaned_obs[\"qpos\"] = obs[\"agent\"][\"qpos\"]\n",
        "    cleaned_obs[\"qvel\"] = obs[\"agent\"][\"qvel\"]\n",
        "    cleaned_obs[\"tcp_pose\"] = obs[\"extra\"][\"tcp_pose\"]\n",
        "    obs[\"extra\"].pop(\"tcp_pose\")\n",
        "\n",
        "    #this code is not generic and only works for the specific observation spaces we have\n",
        "    # Handle different goal position formats gracefully\n",
        "    goal_pose_keys = [\"goal_pose\", \"goal_pos\", \"box_hole_pose\", \"cubeB_pose\"]\n",
        "    for key in goal_pose_keys:\n",
        "        if key in obs[\"extra\"]:\n",
        "            pos = obs[\"extra\"][key]\n",
        "\n",
        "            # Ensure 'pos' is 2D with the correct number of columns\n",
        "            if pos.ndim == 1:\n",
        "                pos = pos.reshape(1, -1)  # Reshape to 2D if necessary\n",
        "            elif pos.ndim > 2:\n",
        "                raise ValueError(f\"Unexpected dimensions for '{key}': {pos.shape}\")\n",
        "\n",
        "            # Pad or truncate 'pos' to have 7 columns\n",
        "            pos = np.pad(pos[:, :7], ((0, 0), (0, 7 - pos.shape[1])), mode='constant')\n",
        "            if isinstance(cleaned_obs[\"tcp_pose\"], torch.Tensor):\n",
        "                pos = torch.tensor(pos, dtype=cleaned_obs[\"tcp_pose\"].dtype)\n",
        "\n",
        "            cleaned_obs[\"goal_pose\"] = pos\n",
        "            obs[\"extra\"].pop(key)\n",
        "            break  # Stop once a valid goal pose key is found\n",
        "    else:\n",
        "        print(\"No goal pose found. Setting to zero.\")\n",
        "        length = len(cleaned_obs[\"tcp_pose\"])\n",
        "        cleaned_obs[\"goal_pose\"] = np.zeros((length, 7), dtype=np.float32)  # Ensure 2D shape\n",
        "\n",
        "    #is_grasped_reshaped = np.reshape(obs[\"extra\"][\"is_grasped\"], (len(obs[\"extra\"][\"is_grasped\"]), 1))\n",
        "\n",
        "    # Filter and add other observations with 7 columns\n",
        "    for key, value in obs[\"extra\"].items():\n",
        "        if value.shape[-1] == 7 and value.ndim == 2:\n",
        "            cleaned_obs[key] = value\n",
        "\n",
        "    return cleaned_obs\n",
        "\n",
        "\n",
        "def normalize_batch(batch, min_vals, max_vals, exclude_features):\n",
        "    batch = batch[\"obs\"]\n",
        "    batch_reshaped = batch.view(-1, batch.shape[-1])\n",
        "    mask = torch.ones(batch_reshaped.shape[1], dtype=torch.bool)\n",
        "    mask[exclude_features] = False\n",
        "\n",
        "    normalized_batch = batch_reshaped.clone()\n",
        "    normalized_batch[:, mask] = (batch_reshaped[:, mask] - min_vals) / (max_vals - min_vals + 0.1)\n",
        "    return normalized_batch.view(batch.shape)\n",
        "\n",
        "def denormalize_batch(batch, min_vals, max_vals, exclude_features):\n",
        "    batch = batch[\"obs\"]\n",
        "    batch_reshaped = batch.view(-1, batch.shape[-1])\n",
        "    mask = torch.ones(batch_reshaped.shape[1], dtype=torch.bool)\n",
        "    mask[exclude_features] = False\n",
        "\n",
        "    denormalized_batch = batch_reshaped.clone()\n",
        "    denormalized_batch[:, mask] = batch_reshaped[:, mask] * (max_vals - min_vals + 0.1) + min_vals\n",
        "    return denormalized_batch.view(batch.shape)\n",
        "\n",
        "\n",
        "class StateDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A general torch Dataset you can drop in and use immediately with just about any trajectory .h5 data generated from ManiSkill.\n",
        "    This class simply is a simple starter code to load trajectory data easily, but does not do any data transformation or anything\n",
        "    advanced. We recommend you to copy this code directly and modify it for more advanced use cases\n",
        "\n",
        "    Args:\n",
        "        dataset_file (str): path to the .h5 file containing the data you want to load\n",
        "        load_count (int): the number of trajectories from the dataset to load into memory. If -1, will load all into memory\n",
        "        success_only (bool): whether to skip trajectories that are not successful in the end. Default is false\n",
        "        device: The location to save data to. If None will store as numpy (the default), otherwise will move data to that device\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, dataset_file: str, pred_horizon: int, obs_horizon: int, action_horizon:int, task_id: np.float32, load_count=-1, device=None\n",
        "    ) -> None:\n",
        "        self.dataset_file = dataset_file\n",
        "        self.pred_horizon = pred_horizon\n",
        "        self.obs_horizon = obs_horizon\n",
        "        self.action_horizon = action_horizon\n",
        "        self.task_id = task_id\n",
        "        self.device = device\n",
        "        self.data = h5py.File(dataset_file, \"r\")\n",
        "        json_path = dataset_file.replace(\".h5\", \".json\")\n",
        "        self.json_data = load_json(json_path)\n",
        "        self.episodes = self.json_data[\"episodes\"]\n",
        "        self.env_info = self.json_data[\"env_info\"]\n",
        "        self.env_id = self.env_info[\"env_id\"]\n",
        "        self.env_kwargs = self.env_info[\"env_kwargs\"]\n",
        "\n",
        "        self.obs = None\n",
        "        self.actions = []\n",
        "        self.terminated = []\n",
        "        self.truncated = []\n",
        "        self.success, self.fail, self.rewards = None, None, None\n",
        "        if load_count == -1:\n",
        "            load_count = len(self.episodes)\n",
        "        for eps_id in tqdm(range(load_count), desc=\"Loading Episodes\", colour=\"green\"):\n",
        "            eps = self.episodes[eps_id]\n",
        "            assert (\n",
        "                \"success\" in eps\n",
        "            ), \"episodes in this dataset do not have the success attribute, cannot load dataset with success_only=True\"\n",
        "            if not eps[\"success\"]:\n",
        "                continue\n",
        "            trajectory = self.data[f\"traj_{eps['episode_id']}\"]\n",
        "            trajectory = load_h5_data(trajectory)\n",
        "            eps_len = len(trajectory[\"actions\"])\n",
        "\n",
        "            # exclude the final observation as most learning workflows do not use it\n",
        "            obs = common.index_dict_array(trajectory[\"obs\"], slice(eps_len))\n",
        "            if eps_id == 0:\n",
        "                self.obs = obs\n",
        "            else:\n",
        "                self.obs = common.append_dict_array(self.obs, obs)\n",
        "\n",
        "            self.actions.append(trajectory[\"actions\"])\n",
        "            self.terminated.append(trajectory[\"terminated\"])\n",
        "            self.truncated.append(trajectory[\"truncated\"])\n",
        "\n",
        "            # handle data that might optionally be in the trajectory\n",
        "            if \"rewards\" in trajectory:\n",
        "                if self.rewards is None:\n",
        "                    self.rewards = [trajectory[\"rewards\"]]\n",
        "                else:\n",
        "                    self.rewards.append(trajectory[\"rewards\"])\n",
        "            if \"success\" in trajectory:\n",
        "                if self.success is None:\n",
        "                    self.success = [trajectory[\"success\"]]\n",
        "                else:\n",
        "                    self.success.append(trajectory[\"success\"])\n",
        "            if \"fail\" in trajectory:\n",
        "                if self.fail is None:\n",
        "                    self.fail = [trajectory[\"fail\"]]\n",
        "                else:\n",
        "                    self.fail.append(trajectory[\"fail\"])\n",
        "\n",
        "        self.actions = np.vstack(self.actions)\n",
        "        self.terminated = np.concatenate(self.terminated)\n",
        "        self.truncated = np.concatenate(self.truncated)\n",
        "\n",
        "        self.truncated = np.zeros(self.actions.shape[0], dtype=bool)\n",
        "        self.truncated[-1] = True\n",
        "\n",
        "        if self.rewards is not None:\n",
        "            self.rewards = np.concatenate(self.rewards)\n",
        "        if self.success is not None:\n",
        "            self.success = np.concatenate(self.success)\n",
        "        if self.fail is not None:\n",
        "            self.fail = np.concatenate(self.fail)\n",
        "\n",
        "        def remove_np_uint16(x: Union[np.ndarray, dict]):\n",
        "            if isinstance(x, dict):\n",
        "                for k in x.keys():\n",
        "                    x[k] = remove_np_uint16(x[k])\n",
        "                return x\n",
        "            else:\n",
        "                if x.dtype == np.uint16:\n",
        "                    return x.astype(np.int32)\n",
        "                return x\n",
        "\n",
        "        # uint16 dtype is used to conserve disk space and memory\n",
        "        # you can optimize this dataset code to keep it as uint16 and process that\n",
        "        # dtype of data yourself. for simplicity we simply cast to a int32 so\n",
        "        # it can automatically be converted to torch tensors without complaint\n",
        "        self.obs = remove_np_uint16(self.obs)\n",
        "\n",
        "        if device is not None:\n",
        "            self.actions = common.to_tensor(self.actions, device=device)\n",
        "            self.obs = common.to_tensor(self.obs, device=device)\n",
        "            self.terminated = common.to_tensor(self.terminated, device=device)\n",
        "            self.truncated = common.to_tensor(self.truncated, device=device)\n",
        "            if self.rewards is not None:\n",
        "                self.rewards = common.to_tensor(self.rewards, device=device)\n",
        "            if self.success is not None:\n",
        "                self.success = common.to_tensor(self.terminated, device=device)\n",
        "            if self.fail is not None:\n",
        "                self.fail = common.to_tensor(self.truncated, device=device)\n",
        "\n",
        "\n",
        "\n",
        "        # Added code for diffusion policy\n",
        "        obs_dict = get_observations(self.obs)\n",
        "        self.train_data = dict(\n",
        "                        obs=convert_observation(obs_dict, self.task_id),\n",
        "                        actions=self.actions,\n",
        "                        )\n",
        "\n",
        "         # Initialize index lists and stat dicts\n",
        "        self.indices = create_sample_indices(\n",
        "            episode_ends=self.truncated,\n",
        "            sequence_length=self.pred_horizon,\n",
        "            pad_before=self.obs_horizon - 1,\n",
        "            pad_after=self.action_horizon - 1\n",
        "        )\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # all possible sequenzes of the dataset\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Change data to fit diffusion policy\n",
        "        buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx = self.indices[idx]\n",
        "\n",
        "\n",
        "        sampled = sample_sequence(\n",
        "            train_data=self.train_data,\n",
        "            sequence_length=self.pred_horizon,\n",
        "            buffer_start_idx=buffer_start_idx,\n",
        "            buffer_end_idx=buffer_end_idx,\n",
        "            sample_start_idx=sample_start_idx,\n",
        "            sample_end_idx=sample_end_idx\n",
        "        )\n",
        "\n",
        "        # discard unused observations in the sequence\n",
        "        for k in sampled.keys():\n",
        "            if k != \"actions\":\n",
        "                # discard unused observations in the sequence\n",
        "                sampled[k] = sampled[k][:self.obs_horizon,:]\n",
        "        sampled[k] = common.to_tensor(sampled[k], device=self.device)\n",
        "\n",
        "        return sampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQa74CESgmYl"
      },
      "source": [
        "#### Network code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UQwS6ecDgmYl"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class Downsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Upsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Conv1dBlock(nn.Module):\n",
        "    '''\n",
        "        Conv1d --> GroupNorm --> Mish\n",
        "    '''\n",
        "\n",
        "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
        "            nn.GroupNorm(n_groups, out_channels),\n",
        "            nn.Mish(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class ConditionalResidualBlock1D(nn.Module):\n",
        "    def __init__(self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            cond_dim,\n",
        "            kernel_size=3,\n",
        "            n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "        ])\n",
        "\n",
        "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
        "        # predicts per-channel scale and bias\n",
        "        cond_channels = out_channels * 2\n",
        "        self.out_channels = out_channels\n",
        "        self.cond_encoder = nn.Sequential(\n",
        "            nn.Mish(),\n",
        "            nn.Linear(cond_dim, cond_channels),\n",
        "            nn.Unflatten(-1, (-1, 1))\n",
        "        )\n",
        "\n",
        "        # make sure dimensions compatible\n",
        "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
        "            if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        '''\n",
        "            x : [ batch_size x in_channels x horizon ]\n",
        "            cond : [ batch_size x cond_dim]\n",
        "\n",
        "            returns:\n",
        "            out : [ batch_size x out_channels x horizon ]\n",
        "        '''\n",
        "        out = self.blocks[0](x)\n",
        "        embed = self.cond_encoder(cond)\n",
        "\n",
        "        embed = embed.reshape(\n",
        "            embed.shape[0], 2, self.out_channels, 1)\n",
        "        scale = embed[:,0,...]\n",
        "        bias = embed[:,1,...]\n",
        "        out = scale * out + bias\n",
        "\n",
        "        out = self.blocks[1](out)\n",
        "        out = out + self.residual_conv(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConditionalUnet1D(nn.Module):\n",
        "    def __init__(self,\n",
        "        input_dim,\n",
        "        global_cond_dim,\n",
        "        diffusion_step_embed_dim=256,\n",
        "        down_dims=[256,512,1024],\n",
        "        kernel_size=5,\n",
        "        n_groups=8\n",
        "        ):\n",
        "        \"\"\"\n",
        "        input_dim: Dim of actions.\n",
        "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
        "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
        "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
        "        down_dims: Channel size for each UNet level.\n",
        "          The length of this array determines numebr of levels.\n",
        "        kernel_size: Conv kernel size\n",
        "        n_groups: Number of groups for GroupNorm\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        all_dims = [input_dim] + list(down_dims)\n",
        "        start_dim = down_dims[0]\n",
        "\n",
        "        dsed = diffusion_step_embed_dim\n",
        "        diffusion_step_encoder = nn.Sequential(\n",
        "            SinusoidalPosEmb(dsed),\n",
        "            nn.Linear(dsed, dsed * 4),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(dsed * 4, dsed),\n",
        "        )\n",
        "        cond_dim = dsed + global_cond_dim\n",
        "\n",
        "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
        "        mid_dim = all_dims[-1]\n",
        "        self.mid_modules = nn.ModuleList([\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        down_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            down_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_out, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out, dim_out, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        up_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            up_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        final_conv = nn.Sequential(\n",
        "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
        "            nn.Conv1d(start_dim, input_dim, 1),\n",
        "        )\n",
        "\n",
        "        self.diffusion_step_encoder = diffusion_step_encoder\n",
        "        self.up_modules = up_modules\n",
        "        self.down_modules = down_modules\n",
        "        self.final_conv = final_conv\n",
        "\n",
        "        print(\"number of parameters: {:e}\".format(\n",
        "            sum(p.numel() for p in self.parameters()))\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "            sample: torch.Tensor,\n",
        "            timestep: Union[torch.Tensor, float, int],\n",
        "            global_cond=None):\n",
        "        \"\"\"\n",
        "        x: (B,T,input_dim)\n",
        "        timestep: (B,) or int, diffusion step\n",
        "        global_cond: (B,global_cond_dim)\n",
        "        output: (B,T,input_dim)\n",
        "        \"\"\"\n",
        "        # (B,T,C)\n",
        "        sample = sample.moveaxis(-1,-2)\n",
        "        # (B,C,T)\n",
        "\n",
        "        # 1. time\n",
        "        timesteps = timestep\n",
        "        if not torch.is_tensor(timesteps):\n",
        "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
        "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
        "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
        "            timesteps = timesteps[None].to(sample.device)\n",
        "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
        "        timesteps = timesteps.expand(sample.shape[0])\n",
        "\n",
        "        global_feature = self.diffusion_step_encoder(timesteps)\n",
        "\n",
        "        if global_cond is not None:\n",
        "            global_feature = torch.cat([\n",
        "                global_feature, global_cond\n",
        "            ], axis=-1)\n",
        "\n",
        "        x = sample\n",
        "        h = []\n",
        "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        for mid_module in self.mid_modules:\n",
        "            x = mid_module(x, global_feature)\n",
        "\n",
        "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        # (B,C,T)\n",
        "        x = x.moveaxis(-1,-2)\n",
        "        # (B,T,C)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdixG-GzgmYm"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx5LOyrPgmYm",
        "outputId": "7e4a1ade-d5ad-4796-99af-49a36e27ff28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading Episodes: 100%|\u001b[32m██████████\u001b[0m| 750/750 [00:04<00:00, 164.37it/s]\n",
            "Loading Episodes: 100%|\u001b[32m██████████\u001b[0m| 250/250 [00:01<00:00, 215.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['obs', 'actions'])\n",
            "observations: torch.Size([128, 2, 40]) torch.float32\n",
            "actions: torch.Size([128, 16, 8]) torch.float32\n",
            "obs_dim: 40\n",
            "action_dim: 8\n",
            "number of parameters: 6.636750e+07\n"
          ]
        }
      ],
      "source": [
        "# download demonstration data from Google Drive\n",
        "env_id = 'PickCube-v1'\n",
        "#env_id = 'StackCube-v1'\n",
        "#env_id = 'PegInsertionSide-v1'\n",
        "#env_id = 'PlugCharger-v1'\n",
        "#env_id = 'PushCube-v1'\n",
        "obs_mode = 'state_dict'\n",
        "control_mode = 'pd_joint_delta_pos'\n",
        "#control_mode = 'pd_ee_delta_pos'\n",
        "\n",
        "pred_horizon = 16\n",
        "obs_horizon = 2\n",
        "action_horizon = 8\n",
        "\n",
        "#==============================================================================\n",
        "\n",
        "task_id = {\n",
        "    'PickCube-v1': 0.0,\n",
        "    'StackCube-v1': 0.1,\n",
        "    'PegInsertionSide-v1': 0.2,\n",
        "    'PlugCharger-v1': 0.3,\n",
        "    'PushCube-v1': 0.4\n",
        "}\n",
        "\n",
        "exclude_features = [25, 26, 27, 28, 29, 30, 31, 39] # goal pose x, y, z, qw, qx, qy, qz and task_id\n",
        "min_vals = None\n",
        "max_vals = None\n",
        "\n",
        "train_dataset_path = '/content/03_pd_joint_pos_training_set.h5'\n",
        "val_dataset_path = '/content/03_pd_joint_pos_validation_set.h5'\n",
        "\n",
        "model_path = f'drive/MyDrive/Data/Checkpoints/{env_id}_{control_mode}_model.pt'\n",
        "\n",
        "\n",
        "# create dataset from file\n",
        "train_dataset = StateDataset(\n",
        "    dataset_file=train_dataset_path,\n",
        "    pred_horizon=pred_horizon,\n",
        "    obs_horizon=obs_horizon,\n",
        "    action_horizon=action_horizon,\n",
        "    task_id=task_id[env_id],\n",
        "    device=None\n",
        ")\n",
        "\n",
        "val_dataset = StateDataset(\n",
        "    dataset_file=val_dataset_path,\n",
        "    pred_horizon=pred_horizon,\n",
        "    obs_horizon=obs_horizon,\n",
        "    action_horizon=action_horizon,\n",
        "    task_id=task_id[env_id],\n",
        "    device=None\n",
        ")\n",
        "\n",
        "# create dataloader\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=128,\n",
        "    num_workers=1,\n",
        "    # don't kill worker process afte each epoch\n",
        "    persistent_workers=True,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=128,\n",
        "    num_workers=1,\n",
        "    # don't kill worker process afte each epoch\n",
        "    persistent_workers=True,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    batch = batch['obs']\n",
        "    batch_reshaped = batch.view(-1, batch.shape[-1])\n",
        "    mask = torch.ones(batch_reshaped.shape[1], dtype=torch.bool)\n",
        "    mask[exclude_features] = False\n",
        "\n",
        "    batch_min = batch_reshaped[:, mask].min(dim=0)[0]\n",
        "    batch_max = batch_reshaped[:, mask].max(dim=0)[0]\n",
        "\n",
        "    if min_vals is None and max_vals is None:\n",
        "        min_vals = batch_min\n",
        "        max_vals = batch_max\n",
        "    else:\n",
        "        min_vals = torch.min(min_vals, batch_min)\n",
        "        max_vals = torch.max(max_vals, batch_max)\n",
        "\n",
        "\n",
        "# visualize data in batch\n",
        "batch = next(iter(train_dataloader))\n",
        "print(batch.keys())\n",
        "print(\"observations:\", batch['obs'].shape, batch['obs'].dtype)\n",
        "print(\"actions:\", batch['actions'].shape, batch['actions'].dtype)\n",
        "\n",
        "\n",
        "# observation and action dimensions corrsponding to the dataset\n",
        "obs_dim = batch['obs'].shape[-1]\n",
        "action_dim = batch['actions'].shape[-1]\n",
        "print(\"obs_dim:\", obs_dim)\n",
        "print(\"action_dim:\", action_dim)\n",
        "\n",
        "# create network object\n",
        "noise_pred_net = ConditionalUnet1D(\n",
        "    input_dim=action_dim,\n",
        "    global_cond_dim=obs_dim*obs_horizon\n",
        ")\n",
        "\n",
        "# example inputs\n",
        "noised_action = torch.randn((1, pred_horizon, action_dim))\n",
        "obs = torch.zeros((1, obs_horizon, obs_dim))\n",
        "diffusion_iter = torch.zeros((1,))\n",
        "\n",
        "# the noise prediction network\n",
        "# takes noisy action, diffusion iteration and observation as input\n",
        "# predicts the noise added to action\n",
        "noise = noise_pred_net(\n",
        "    sample=noised_action,\n",
        "    timestep=diffusion_iter,\n",
        "    global_cond=obs.flatten(start_dim=1))\n",
        "\n",
        "# illustration of removing noise\n",
        "# the actual noise removal is performed by NoiseScheduler\n",
        "# and is dependent on the diffusion noise schedule\n",
        "denoised_action = noised_action - noise\n",
        "\n",
        "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
        "num_diffusion_iters = 100\n",
        "noise_scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=num_diffusion_iters,\n",
        "    # the choise of beta schedule has big impact on performance\n",
        "    # we found squared cosine works the best\n",
        "    beta_schedule='squaredcos_cap_v2',\n",
        "    # clip output to [-1,1] to improve stability\n",
        "    clip_sample=True,\n",
        "    # our network predicts noise (instead of denoised action)\n",
        "    prediction_type='epsilon'\n",
        ")\n",
        "\n",
        "# device transfer\n",
        "device = torch.device(\"cuda\")\n",
        "_ = noise_pred_net.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBEFVZ_qgmYm"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "e6hNneMogmYm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "906e9575-a5e7-4668-9146-3c713a74b146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
            "Train Batch:   0%|          | 0/1071 [00:00<?, ?it/s]\u001b[A\n",
            "Train Batch:   0%|          | 0/1071 [00:00<?, ?it/s, loss=1.18]\u001b[A\n",
            "Train Batch:   0%|          | 1/1071 [00:00<05:57,  2.99it/s, loss=1.18]\u001b[A\n",
            "Train Batch:   0%|          | 1/1071 [00:00<05:57,  2.99it/s, loss=1.17]\u001b[A\n",
            "Train Batch:   0%|          | 2/1071 [00:00<04:50,  3.69it/s, loss=1.17]\u001b[A\n",
            "Train Batch:   0%|          | 2/1071 [00:00<04:50,  3.69it/s, loss=1.19]\u001b[A\n",
            "Train Batch:   0%|          | 3/1071 [00:00<04:30,  3.95it/s, loss=1.19]\u001b[A\n",
            "Train Batch:   0%|          | 3/1071 [00:01<04:30,  3.95it/s, loss=1.16]\u001b[A\n",
            "Train Batch:   0%|          | 4/1071 [00:01<04:20,  4.09it/s, loss=1.16]\u001b[A\n",
            "Train Batch:   0%|          | 4/1071 [00:01<04:20,  4.09it/s, loss=1.16]\u001b[A\n",
            "Train Batch:   0%|          | 5/1071 [00:01<04:14,  4.18it/s, loss=1.16]\u001b[A\n",
            "Train Batch:   0%|          | 5/1071 [00:01<04:14,  4.18it/s, loss=1.15]\u001b[A\n",
            "Train Batch:   1%|          | 6/1071 [00:01<04:12,  4.22it/s, loss=1.15]\u001b[A\n",
            "Train Batch:   1%|          | 6/1071 [00:01<04:12,  4.22it/s, loss=1.14]\u001b[A\n",
            "Train Batch:   1%|          | 7/1071 [00:01<04:09,  4.26it/s, loss=1.14]\u001b[A\n",
            "Train Batch:   1%|          | 7/1071 [00:01<04:09,  4.26it/s, loss=1.12]\u001b[A\n",
            "Train Batch:   1%|          | 8/1071 [00:01<04:04,  4.35it/s, loss=1.12]\u001b[A\n",
            "Train Batch:   1%|          | 8/1071 [00:02<04:04,  4.35it/s, loss=1.12]\u001b[A\n",
            "Train Batch:   1%|          | 9/1071 [00:02<04:02,  4.38it/s, loss=1.12]\u001b[A\n",
            "Train Batch:   1%|          | 9/1071 [00:02<04:02,  4.38it/s, loss=1.12]\u001b[A\n",
            "Train Batch:   1%|          | 10/1071 [00:02<04:03,  4.36it/s, loss=1.12]\u001b[A\n",
            "Train Batch:   1%|          | 10/1071 [00:02<04:03,  4.36it/s, loss=1.09]\u001b[A\n",
            "Train Batch:   1%|          | 11/1071 [00:02<04:00,  4.41it/s, loss=1.09]\u001b[A\n",
            "Train Batch:   1%|          | 11/1071 [00:02<04:00,  4.41it/s, loss=1.09]\u001b[A\n",
            "Train Batch:   1%|          | 12/1071 [00:02<03:58,  4.44it/s, loss=1.09]\u001b[A\n",
            "Train Batch:   1%|          | 12/1071 [00:03<03:58,  4.44it/s, loss=1.06]\u001b[A\n",
            "Train Batch:   1%|          | 13/1071 [00:03<03:56,  4.47it/s, loss=1.06]\u001b[A\n",
            "Train Batch:   1%|          | 13/1071 [00:03<03:56,  4.47it/s, loss=1.06]\u001b[A\n",
            "Train Batch:   1%|▏         | 14/1071 [00:03<03:56,  4.48it/s, loss=1.06]\u001b[A\n",
            "Train Batch:   1%|▏         | 14/1071 [00:03<03:56,  4.48it/s, loss=1.04]\u001b[A\n",
            "Train Batch:   1%|▏         | 15/1071 [00:03<03:55,  4.49it/s, loss=1.04]\u001b[A\n",
            "Train Batch:   1%|▏         | 15/1071 [00:03<03:55,  4.49it/s, loss=1.06]\n",
            "Epoch:   0%|          | 0/5 [00:03<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-48efcab17b75>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mloss_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mtepoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Validation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mset_postfix\u001b[0;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m                                  for key in postfix.keys())\n\u001b[1;32m   1430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_postfix_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;31m# request flush on the background thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0;31m# wait for flush to actually get through, if we can.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# waiting across threads during import can cause deadlocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    618\u001b[0m                 )\n\u001b[1;32m    619\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     def send_multipart(\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 5\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.nn import functional as F\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "# Exponential Moving Average\n",
        "ema = EMAModel(\n",
        "    parameters=noise_pred_net.parameters(),\n",
        "    power=0.75\n",
        ")\n",
        "\n",
        "# Standard ADAM optimizer\n",
        "optimizer = AdamW(\n",
        "    params=noise_pred_net.parameters(),\n",
        "    lr=1e-4, weight_decay=1e-6\n",
        ")\n",
        "\n",
        "# Cosine LR schedule with linear warmup\n",
        "lr_scheduler = get_scheduler(\n",
        "    name='cosine',\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=500,\n",
        "    num_training_steps=len(train_dataloader) * num_epochs\n",
        ")\n",
        "\n",
        "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
        "    for epoch_idx in tglobal:\n",
        "        train_loss = list()\n",
        "\n",
        "        # Training loop\n",
        "        noise_pred_net.train()\n",
        "        with tqdm(train_dataloader, desc='Train Batch', leave=False) as tepoch:\n",
        "            for batch in tepoch:\n",
        "                # Normalize data\n",
        "                nbatch = normalize_batch(batch, min_vals, max_vals, exclude_features)\n",
        "\n",
        "                # Device transfer\n",
        "                nobs = nbatch.to(device)\n",
        "                naction = batch['actions'].to(device)\n",
        "                B = nobs.shape[0]\n",
        "\n",
        "                # Observation as FiLM conditioning\n",
        "                obs_cond = nobs[:, :obs_horizon, :].flatten(start_dim=1)\n",
        "\n",
        "                # Sample noise\n",
        "                noise = torch.randn(naction.shape, device=device)\n",
        "\n",
        "                # Sample diffusion iteration\n",
        "                timesteps = torch.randint(\n",
        "                    0, noise_scheduler.config.num_train_timesteps,\n",
        "                    (B,), device=device\n",
        "                ).long()\n",
        "\n",
        "                # Add noise to actions\n",
        "                noisy_actions = noise_scheduler.add_noise(\n",
        "                    naction, noise, timesteps)\n",
        "\n",
        "                # Predict noise residual\n",
        "                noise_pred = noise_pred_net(\n",
        "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
        "\n",
        "                # L2 loss\n",
        "                loss = F.mse_loss(noise_pred, noise)\n",
        "\n",
        "                # Optimize\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                lr_scheduler.step()\n",
        "\n",
        "                # Update EMA of model weights\n",
        "                ema.step(noise_pred_net.parameters())\n",
        "\n",
        "                # Logging\n",
        "                loss_cpu = loss.item()\n",
        "                train_loss.append(loss_cpu)\n",
        "                tepoch.set_postfix(loss=loss_cpu)\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss = list()\n",
        "        noise_pred_net.eval()\n",
        "        with torch.no_grad():\n",
        "            with tqdm(val_dataloader, desc='Val Batch', leave=False) as vepoch:\n",
        "                for batch in vepoch:\n",
        "                    # Normalize data\n",
        "                    nbatch = normalize_batch(batch, min_vals, max_vals, exclude_features)\n",
        "\n",
        "                    # Device transfer\n",
        "                    nobs = nbatch.to(device)\n",
        "                    naction = batch['actions'].to(device)\n",
        "                    B = nobs.shape[0]\n",
        "\n",
        "                    # Observation as FiLM conditioning\n",
        "                    obs_cond = nobs[:, :obs_horizon, :].flatten(start_dim=1)\n",
        "\n",
        "                    # Sample noise\n",
        "                    noise = torch.randn(naction.shape, device=device)\n",
        "\n",
        "                    # Sample diffusion iteration\n",
        "                    timesteps = torch.randint(\n",
        "                        0, noise_scheduler.config.num_train_timesteps,\n",
        "                        (B,), device=device\n",
        "                    ).long()\n",
        "\n",
        "                    # Add noise to actions\n",
        "                    noisy_actions = noise_scheduler.add_noise(\n",
        "                        naction, noise, timesteps)\n",
        "\n",
        "                    # Predict noise residual\n",
        "                    noise_pred = noise_pred_net(\n",
        "                        noisy_actions, timesteps, global_cond=obs_cond)\n",
        "\n",
        "                    # L2 loss\n",
        "                    loss = F.mse_loss(noise_pred, noise)\n",
        "\n",
        "                    # Logging\n",
        "                    loss_cpu = loss.item()\n",
        "                    val_loss.append(loss_cpu)\n",
        "                    vepoch.set_postfix(loss=loss_cpu)\n",
        "\n",
        "        tglobal.set_postfix(\n",
        "            train_loss=np.mean(train_loss),\n",
        "            val_loss=np.mean(val_loss)\n",
        "        )\n",
        "\n",
        "# Use weights of the EMA model for inference\n",
        "ema_noise_pred_net = noise_pred_net\n",
        "ema.copy_to(ema_noise_pred_net.parameters())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMBgCb-5gmYm"
      },
      "source": [
        "#### Saving model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0XfEzQJgmYm"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    'model_state_dict': ema_noise_pred_net.state_dict(),\n",
        "    'ema_model_state_dict': ema.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
        "    'epoch': epoch_idx,\n",
        "    'loss': loss, # Save the current epoch\n",
        "}, model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Kh8rwt8gmYn"
      },
      "source": [
        "#### Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DagoCn_LgmYn"
      },
      "outputs": [],
      "source": [
        "state_dict = torch.load(model_path, map_location='cuda')\n",
        "ema_noise_pred_net = noise_pred_net\n",
        "ema_noise_pred_net.load_state_dict(state_dict['model_state_dict'])\n",
        "print('Pretrained weights loaded.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpnhDeX1gmYn"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRaCmwsagmYn"
      },
      "outputs": [],
      "source": [
        "# limit enviornment interaction to 200 steps before termination\n",
        "env = gym.make(env_id, obs_mode=obs_mode, control_mode=control_mode, render_mode='rgb_array')\n",
        "\n",
        "max_steps = 400\n",
        "\n",
        "# reset\n",
        "obs, info = env.reset()\n",
        "obs = get_observations(obs)\n",
        "obs = convert_observation(obs, task_id[env_id])\n",
        "\n",
        "# save observations\n",
        "obs_deque = collections.deque([obs] * obs_horizon, maxlen=obs_horizon)\n",
        "\n",
        "# save visualization\n",
        "imgs = []\n",
        "rewards = []\n",
        "done = False\n",
        "step_idx = 0\n",
        "\n",
        "\n",
        "with tqdm(total=max_steps, desc=\"Eval\") as pbar:\n",
        "    while not done:\n",
        "        B = 1\n",
        "        # stack the last obs_horizon (2) number of observations\n",
        "        obs_seq = np.stack(obs_deque)\n",
        "\n",
        "        nobs = normalize_batch({'obs': torch.tensor(obs_seq, dtype=torch.float32)}, min_vals, max_vals, exclude_features)\n",
        "\n",
        "        # device transfer\n",
        "        #nobs = torch.from_numpy(nobs).to(device, dtype=torch.float32)\n",
        "        nobs= nobs.to(device)\n",
        "\n",
        "        # infer action\n",
        "        with torch.no_grad():\n",
        "            # reshape observation to (B,obs_horizon*obs_dim)\n",
        "            obs_cond = nobs.unsqueeze(0).flatten(start_dim=1)\n",
        "\n",
        "            # initialize action from Guassian noise\n",
        "            noisy_action = torch.randn(\n",
        "                (B, pred_horizon, action_dim), device=device)\n",
        "            naction = noisy_action\n",
        "\n",
        "            # init scheduler\n",
        "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
        "\n",
        "            for k in noise_scheduler.timesteps:\n",
        "                # predict noise\n",
        "                noise_pred = ema_noise_pred_net(\n",
        "                    sample=naction,\n",
        "                    timestep=k,\n",
        "                    global_cond=obs_cond\n",
        "                )\n",
        "\n",
        "                # inverse diffusion step (remove noise)\n",
        "                naction = noise_scheduler.step(\n",
        "                    model_output=noise_pred,\n",
        "                    timestep=k,\n",
        "                    sample=naction\n",
        "                ).prev_sample\n",
        "\n",
        "        # unnormalize action\n",
        "        naction = naction.detach().to('cpu').numpy()\n",
        "        # (B, pred_horizon, action_dim)\n",
        "        action_pred = naction[0] # we dont have to denormalize the action\n",
        "\n",
        "        # only take action_horizon number of actions\n",
        "        start = obs_horizon - 1\n",
        "        end = start + action_horizon\n",
        "        action = action_pred[start:end,:]\n",
        "\n",
        "        # execute action_horizon number of steps\n",
        "        # without replanning\n",
        "        for i in range(len(action)):\n",
        "            # stepping env\n",
        "            obs, reward, done, _, info = env.step(action[i])\n",
        "\n",
        "            # process observation\n",
        "            # From the observation dictionary, we concatenate all the observations\n",
        "            # as done in the training data\n",
        "            obs = get_observations(obs)\n",
        "            obs = convert_observation(obs, task_id[env_id])\n",
        "\n",
        "            # save observations\n",
        "            obs_deque.append(obs)\n",
        "\n",
        "            # and reward/vis\n",
        "            rewards.append(reward)\n",
        "            imgs.append(env.render())\n",
        "\n",
        "            # update progress bar\n",
        "            step_idx += 1\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix(reward=reward)\n",
        "            if step_idx > max_steps:\n",
        "                done = True\n",
        "            if done:\n",
        "                break\n",
        "# print out the maximum target coverage\n",
        "print('Score: ', max(rewards))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLAAN1VHgmYn"
      },
      "source": [
        "### Save gif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xG3f-cVNgmYn"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import display, Image as IPImage\n",
        "import io\n",
        "\n",
        "images = [Image.fromarray(img.squeeze(0).cpu().numpy()) for img in imgs]\n",
        "\n",
        "# Save to a bytes buffer\n",
        "buffer = io.BytesIO()\n",
        "images[0].save(buffer, format='GIF', save_all=True, append_images=images[1:], optimize=False, duration=50, loop=0)\n",
        "buffer.seek(0)\n",
        "\n",
        "# Save to a file\n",
        "with open(f'drive/MyDrive/Data/Results/{env_id}_{control_mode}_animation.gif', 'wb') as f:\n",
        "    f.write(buffer.getvalue())\n",
        "\n",
        "# Display the GIF (optional)\n",
        "display(IPImage(data=buffer.getvalue()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88703405b6c0416384800b1478a2f906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72b64855bf9c4af991b69b02bead5a4e",
              "IPY_MODEL_52881d861c5d414eb62de2201e7d3ead",
              "IPY_MODEL_eb46ef7dc03b435ead00a36ae45893b6"
            ],
            "layout": "IPY_MODEL_bf79027210bb4d96a4a922becb7ed04b"
          }
        },
        "72b64855bf9c4af991b69b02bead5a4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4841e65e86d34f33b1b2b13def6cbd73",
            "placeholder": "​",
            "style": "IPY_MODEL_ea2199014d944cfb844f2f4733b0a117",
            "value": ""
          }
        },
        "52881d861c5d414eb62de2201e7d3ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d12eb65ecaee437884b165364b9ac0a9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b389f174941b44ac868118d1f64a5a42",
            "value": 0
          }
        },
        "eb46ef7dc03b435ead00a36ae45893b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93083b1ddc4c4d6ba485df10d86db3ee",
            "placeholder": "​",
            "style": "IPY_MODEL_80c9ea2a67e24c2ba3a7da4e022e09fb",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "bf79027210bb4d96a4a922becb7ed04b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4841e65e86d34f33b1b2b13def6cbd73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea2199014d944cfb844f2f4733b0a117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d12eb65ecaee437884b165364b9ac0a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b389f174941b44ac868118d1f64a5a42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "93083b1ddc4c4d6ba485df10d86db3ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80c9ea2a67e24c2ba3a7da4e022e09fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}