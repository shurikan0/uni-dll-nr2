{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the package\n",
    "%pip install --upgrade mani_skill\n",
    "# install a version of torch that is compatible with your system\n",
    "%pip install torch torchvision torchaudio numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import mani_skill.envs.utils.randomization as randomization\n",
    "from mani_skill.agents.robots import Fetch, Panda, Xmate3Robotiq\n",
    "from mani_skill.envs.sapien_env import BaseEnv\n",
    "from mani_skill.sensors.camera import CameraConfig\n",
    "from mani_skill.utils import sapien_utils\n",
    "from mani_skill.utils.building import actors\n",
    "from mani_skill.utils.registration import register_env\n",
    "from mani_skill.utils.scene_builder.table import TableSceneBuilder\n",
    "from mani_skill.utils.structs.pose import Pose\n",
    "from mani_skill.utils.structs.types import SimConfig\n",
    "\n",
    "\n",
    "@register_env(\"PickCube-v1\", max_episode_steps=50)\n",
    "class PickCubeEnv(BaseEnv):\n",
    "    SUPPORTED_ROBOTS = [\"panda\", \"xmate3_robotiq\", \"fetch\"]\n",
    "    agent: Union[Panda, Xmate3Robotiq, Fetch]\n",
    "    cube_half_size = 0.02\n",
    "    goal_thresh = 0.025\n",
    "\n",
    "    def __init__(self, *args, robot_uids=\"panda\", robot_init_qpos_noise=0.02, **kwargs):\n",
    "        self.robot_init_qpos_noise = robot_init_qpos_noise\n",
    "        super().__init__(*args, robot_uids=robot_uids, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _default_sensor_configs(self):\n",
    "        pose = sapien_utils.look_at(eye=[0.3, 0, 0.6], target=[-0.1, 0, 0.1])\n",
    "        return [CameraConfig(\"base_camera\", pose, 128, 128, np.pi / 2, 0.01, 100)]\n",
    "\n",
    "    @property\n",
    "    def _default_human_render_camera_configs(self):\n",
    "        pose = sapien_utils.look_at([0.6, 0.7, 0.6], [0.0, 0.0, 0.35])\n",
    "        return CameraConfig(\"render_camera\", pose, 512, 512, 1, 0.01, 100)\n",
    "\n",
    "    def _load_scene(self, options: dict):\n",
    "        self.table_scene = TableSceneBuilder(\n",
    "            self, robot_init_qpos_noise=self.robot_init_qpos_noise\n",
    "        )\n",
    "        self.table_scene.build()\n",
    "        self.cube = actors.build_cube(\n",
    "            self.scene, half_size=self.cube_half_size, color=[1, 0, 0, 1], name=\"cube\"\n",
    "        )\n",
    "        self.goal_site = actors.build_sphere(\n",
    "            self.scene,\n",
    "            radius=self.goal_thresh,\n",
    "            color=[0, 1, 0, 1],\n",
    "            name=\"goal_site\",\n",
    "            body_type=\"kinematic\",\n",
    "            add_collision=False,\n",
    "        )\n",
    "        self._hidden_objects.append(self.goal_site)\n",
    "\n",
    "    def _initialize_episode(self, env_idx: torch.Tensor, options: dict):\n",
    "        with torch.device(self.device):\n",
    "            b = len(env_idx)\n",
    "            self.table_scene.initialize(env_idx)\n",
    "            xyz = torch.zeros((b, 3))\n",
    "            xyz[:, :2] = torch.rand((b, 2)) * 0.2 - 0.1\n",
    "            xyz[:, 2] = self.cube_half_size\n",
    "            qs = randomization.random_quaternions(b, lock_x=True, lock_y=True)\n",
    "            self.cube.set_pose(Pose.create_from_pq(xyz, qs))\n",
    "\n",
    "            goal_xyz = torch.zeros((b, 3))\n",
    "            goal_xyz[:, :2] = torch.rand((b, 2)) * 0.2 - 0.1\n",
    "            goal_xyz[:, 2] = torch.rand((b)) * 0.3 + xyz[:, 2]\n",
    "            self.goal_site.set_pose(Pose.create_from_pq(goal_xyz))\n",
    "\n",
    "    def _get_obs_extra(self, info: Dict):\n",
    "        # in reality some people hack is_grasped into observations by checking if the gripper can close fully or not\n",
    "        obs = dict(\n",
    "            is_grasped=info[\"is_grasped\"],\n",
    "            tcp_pose=self.agent.tcp.pose.raw_pose,\n",
    "            goal_pos=self.goal_site.pose.p,\n",
    "        )\n",
    "        if \"state\" in self.obs_mode:\n",
    "            obs.update(\n",
    "                obj_pose=self.cube.pose.raw_pose,\n",
    "                tcp_to_obj_pos=self.cube.pose.p - self.agent.tcp.pose.p,\n",
    "                obj_to_goal_pos=self.goal_site.pose.p - self.cube.pose.p,\n",
    "            )\n",
    "        return obs\n",
    "\n",
    "    def evaluate(self):\n",
    "        is_obj_placed = (\n",
    "            torch.linalg.norm(self.goal_site.pose.p - self.cube.pose.p, axis=1)\n",
    "            <= self.goal_thresh\n",
    "        )\n",
    "        is_grasped = self.agent.is_grasping(self.cube)\n",
    "        is_robot_static = self.agent.is_static(0.2)\n",
    "        return {\n",
    "            \"success\": is_obj_placed & is_robot_static,\n",
    "            \"is_obj_placed\": is_obj_placed,\n",
    "            \"is_robot_static\": is_robot_static,\n",
    "            \"is_grasped\": is_grasped,\n",
    "        }\n",
    "\n",
    "    def compute_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):\n",
    "        tcp_to_obj_dist = torch.linalg.norm(\n",
    "            self.cube.pose.p - self.agent.tcp.pose.p, axis=1\n",
    "        )\n",
    "        reaching_reward = 1 - torch.tanh(5 * tcp_to_obj_dist)\n",
    "        reward = reaching_reward\n",
    "\n",
    "        is_grasped = info[\"is_grasped\"]\n",
    "        reward += is_grasped\n",
    "\n",
    "        obj_to_goal_dist = torch.linalg.norm(\n",
    "            self.goal_site.pose.p - self.cube.pose.p, axis=1\n",
    "        )\n",
    "        place_reward = 1 - torch.tanh(5 * obj_to_goal_dist)\n",
    "        reward += place_reward * is_grasped\n",
    "\n",
    "        static_reward = 1 - torch.tanh(\n",
    "            5 * torch.linalg.norm(self.agent.robot.get_qvel()[..., :-2], axis=1)\n",
    "        )\n",
    "        reward += static_reward * info[\"is_obj_placed\"]\n",
    "\n",
    "        reward[info[\"success\"]] = 5\n",
    "        return reward\n",
    "\n",
    "    def compute_normalized_dense_reward(\n",
    "        self, obs: Any, action: torch.Tensor, info: Dict\n",
    "    ):\n",
    "        return self.compute_dense_reward(obs=obs, action=action, info=info) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sapien\n",
    "\n",
    "from mani_skill.envs.tasks import PickCubeEnv\n",
    "from mani_skill.examples.motionplanning.panda.motionplanner import \\\n",
    "    PandaArmMotionPlanningSolver\n",
    "from mani_skill.examples.motionplanning.panda.utils import (\n",
    "    compute_grasp_info_by_obb, get_actor_obb)\n",
    "\n",
    "def solve(env: PickCubeEnv, seed=None, debug=False, vis=False):\n",
    "    env.reset(seed=seed)\n",
    "    planner = PandaArmMotionPlanningSolver(\n",
    "        env,\n",
    "        debug=debug,\n",
    "        vis=vis,\n",
    "        base_pose=env.unwrapped.agent.robot.pose,\n",
    "        visualize_target_grasp_pose=vis,\n",
    "        print_env_info=False,\n",
    "    )\n",
    "\n",
    "    FINGER_LENGTH = 0.025\n",
    "    env = env.unwrapped\n",
    "\n",
    "    # retrieves the object oriented bounding box (trimesh box object)\n",
    "    obb = get_actor_obb(env.cube)\n",
    "\n",
    "    approaching = np.array([0, 0, -1])\n",
    "    # get transformation matrix of the tcp pose, is default batched and on torch\n",
    "    target_closing = env.agent.tcp.pose.to_transformation_matrix()[0, :3, 1].cpu().numpy()\n",
    "    # we can build a simple grasp pose using this information for Panda\n",
    "    grasp_info = compute_grasp_info_by_obb(\n",
    "        obb,\n",
    "        approaching=approaching,\n",
    "        target_closing=target_closing,\n",
    "        depth=FINGER_LENGTH,\n",
    "    )\n",
    "    closing, center = grasp_info[\"closing\"], grasp_info[\"center\"]\n",
    "    grasp_pose = env.agent.build_grasp_pose(approaching, closing, env.cube.pose.sp.p)\n",
    "\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # Reach\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    reach_pose = grasp_pose * sapien.Pose([0, 0, -0.05])\n",
    "    planner.move_to_pose_with_screw(reach_pose)\n",
    "\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # Grasp\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    planner.move_to_pose_with_screw(grasp_pose)\n",
    "    planner.close_gripper()\n",
    "\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # Move to goal pose\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    goal_pose = sapien.Pose(env.goal_site.pose.sp.p, grasp_pose.q)\n",
    "    res = planner.move_to_pose_with_screw(goal_pose)\n",
    "\n",
    "    planner.close()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os.path as osp\n",
    "from mani_skill.utils.wrappers.record import RecordEpisode\n",
    "from mani_skill.examples.motionplanning.panda.solutions import solvePushCube, solvePickCube, solveStackCube, solvePegInsertionSide, solvePlugCharger\n",
    "\n",
    "\n",
    "MP_SOLUTIONS = {\n",
    "    \"PickCube-v1\": solve,\n",
    "    \"StackCube-v1\": solveStackCube,\n",
    "    \"PegInsertionSide-v1\": solvePegInsertionSide,\n",
    "    \"PlugCharger-v1\": solvePlugCharger,\n",
    "    \"PushCube-v1\": solvePushCube,\n",
    "}\n",
    "\n",
    "def generate(num_traj, env_id, file_path, save_video):\n",
    "    env = gym.make(\n",
    "        env_id,\n",
    "        obs_mode=\"none\",\n",
    "        control_mode=\"pd_joint_pos\",\n",
    "        render_mode=\"rgb_array\",\n",
    "        reward_mode=\"dense\",\n",
    "        shader_dir=\"default\",\n",
    "        sim_backend=\"auto\"\n",
    "    )\n",
    "    if env_id not in MP_SOLUTIONS:\n",
    "        raise RuntimeError(f\"No already written motion planning solutions for {env_id}. Available options are {list(MP_SOLUTIONS.keys())}\")\n",
    "    env = RecordEpisode(\n",
    "        env,\n",
    "        output_dir=osp.join(file_path, env_id),\n",
    "        trajectory_name=env_id, save_video=False,\n",
    "        source_type=\"motionplanning\",\n",
    "        source_desc=\"official motion planning solution from ManiSkill contributors\",\n",
    "        video_fps=30,\n",
    "        save_on_reset=False\n",
    "    )\n",
    "    solve = MP_SOLUTIONS[env_id]\n",
    "    print(f\"Motion Planning Running on {env_id}\")\n",
    "    pbar = tqdm(range(num_traj))\n",
    "    seed = 0\n",
    "    successes = []\n",
    "    passed = 0\n",
    "    while True:\n",
    "        res = solve(env, seed=seed, debug=False, vis=False)\n",
    "        if res == -1:\n",
    "            success = False\n",
    "        else:\n",
    "            success = res[-1][\"success\"].item()\n",
    "        successes.append(success)\n",
    "        if not success:\n",
    "            seed += 1\n",
    "            env.flush_trajectory(save=False)\n",
    "            if save_video:\n",
    "                env.flush_video(save=False)\n",
    "            continue\n",
    "        else:\n",
    "            env.flush_trajectory()\n",
    "            if save_video:\n",
    "                env.flush_video()\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(dict(success_rate=np.mean(successes)))\n",
    "            seed += 1\n",
    "            passed += 1\n",
    "            if passed == num_traj:\n",
    "                break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_traj = 10\n",
    "env_id = \"PlaceCube-v1\"\n",
    "file_path = \"/content/drive/MyDrive/Data/Training/Generated/\"\n",
    "save_video = False\n",
    "\n",
    "generate(num_traj, env_id, file_path, save_video)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
