{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pip Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the package\n",
    "%pip install --upgrade mani_skill\n",
    "# install a version of torch that is compatible with your system\n",
    "%pip install torch torchvision torchaudio numpy diffusers\n",
    "\n",
    "\n",
    "# etc imports\n",
    "from typing import Tuple, Sequence, Dict, Union, Optional\n",
    "from collections import OrderedDict\n",
    "import collections\n",
    "import math\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display, Image as IPImage\n",
    "import io\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# mani_skill imports\n",
    "from mani_skill.utils import common\n",
    "from mani_skill.utils.io_utils import load_json\n",
    "from mani_skill.utils.common import flatten_state_dict\n",
    "import mani_skill.envs\n",
    "from typing import Any, Dict, Union\n",
    "\n",
    "import sapien\n",
    "from mani_skill.envs.scene import ManiSkillScene\n",
    "from transforms3d.euler import euler2quat\n",
    "from mani_skill.agents.robots import PandaWristCam\n",
    "from mani_skill.envs.sapien_env import BaseEnv\n",
    "from mani_skill.envs.utils import randomization\n",
    "from mani_skill.sensors.camera import CameraConfig\n",
    "from mani_skill.utils import common, sapien_utils\n",
    "from mani_skill.utils.geometry import rotation_conversions\n",
    "from mani_skill.utils.registration import register_env\n",
    "from mani_skill.utils.scene_builder.table import TableSceneBuilder\n",
    "from mani_skill.utils.structs.pose import Pose\n",
    "from mani_skill.utils.structs.types import SimConfig\n",
    "\n",
    "\n",
    "#torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# diffuser imports\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "# gym imports\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# google colab imports\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_box_with_hole(\n",
    "    scene: ManiSkillScene, inner_radius, outer_radius, depth, center=(0, 0)\n",
    "):\n",
    "    builder = scene.create_actor_builder()\n",
    "    thickness = (outer_radius - inner_radius) * 0.5\n",
    "    # x-axis is hole direction\n",
    "    half_center = [x * 0.5 for x in center]\n",
    "    half_sizes = [\n",
    "        [depth, thickness - half_center[0], outer_radius],\n",
    "        [depth, thickness + half_center[0], outer_radius],\n",
    "        [depth, outer_radius, thickness - half_center[1]],\n",
    "        [depth, outer_radius, thickness + half_center[1]],\n",
    "    ]\n",
    "    offset = thickness + inner_radius\n",
    "    poses = [\n",
    "        sapien.Pose([0, offset + half_center[0], 0]),\n",
    "        sapien.Pose([0, -offset + half_center[0], 0]),\n",
    "        sapien.Pose([0, 0, offset + half_center[1]]),\n",
    "        sapien.Pose([0, 0, -offset + half_center[1]]),\n",
    "    ]\n",
    "\n",
    "    mat = sapien.render.RenderMaterial(\n",
    "        base_color=sapien_utils.hex2rgba(\"#FFD289\"), roughness=0.5, specular=0.5\n",
    "    )\n",
    "\n",
    "    for half_size, pose in zip(half_sizes, poses):\n",
    "        builder.add_box_collision(pose, half_size)\n",
    "        builder.add_box_visual(pose, half_size, material=mat)\n",
    "    return builder\n",
    "\n",
    "\n",
    "@register_env(\"PegInsertionSide-v2\", max_episode_steps=100)\n",
    "class PegInsertionSideEnv(BaseEnv):\n",
    "    SUPPORTED_REWARD_MODES = (\"normalized_dense\", \"dense\", \"sparse\", \"none\")\n",
    "    SUPPORTED_ROBOTS = [\"panda_wristcam\"]\n",
    "    agent: Union[PandaWristCam]\n",
    "    _clearance = 0.003\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        robot_uids=\"panda_wristcam\",\n",
    "        num_envs=1,\n",
    "        reconfiguration_freq=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if reconfiguration_freq is None:\n",
    "            if num_envs == 1:\n",
    "                reconfiguration_freq = 1\n",
    "            else:\n",
    "                reconfiguration_freq = 0\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            robot_uids=robot_uids,\n",
    "            num_envs=num_envs,\n",
    "            reconfiguration_freq=reconfiguration_freq,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _default_sim_config(self):\n",
    "        return SimConfig()\n",
    "\n",
    "    @property\n",
    "    def _default_sensor_configs(self):\n",
    "        pose = sapien_utils.look_at([0, -0.3, 0.2], [0, 0, 0.1])\n",
    "        return [CameraConfig(\"base_camera\", pose, 128, 128, np.pi / 2, 0.01, 100)]\n",
    "\n",
    "    @property\n",
    "    def _default_human_render_camera_configs(self):\n",
    "        pose = sapien_utils.look_at([0.5, -0.5, 0.8], [0.05, -0.1, 0.4])\n",
    "        return CameraConfig(\"render_camera\", pose, 512, 512, 1, 0.01, 100)\n",
    "\n",
    "    def _load_scene(self, options: dict):\n",
    "        with torch.device(self.device):\n",
    "            self.table_scene = TableSceneBuilder(self)\n",
    "            self.table_scene.build()\n",
    "\n",
    "            lengths = self._episode_rng.uniform(0.085, 0.125, size=(self.num_envs,))\n",
    "            radii = self._episode_rng.uniform(0.015, 0.025, size=(self.num_envs,))\n",
    "            centers = (\n",
    "                0.5\n",
    "                * (lengths - radii)[:, None]\n",
    "                * self._episode_rng.uniform(-1, 1, size=(self.num_envs, 2))\n",
    "            )\n",
    "\n",
    "            # save some useful values for use later\n",
    "            self.peg_half_sizes = common.to_tensor(np.vstack([lengths, radii, radii])).T\n",
    "            peg_head_offsets = torch.zeros((self.num_envs, 3))\n",
    "            peg_head_offsets[:, 0] = self.peg_half_sizes[:, 0]\n",
    "            self.peg_head_offsets = Pose.create_from_pq(p=peg_head_offsets)\n",
    "\n",
    "            box_hole_offsets = torch.zeros((self.num_envs, 3))\n",
    "            box_hole_offsets[:, 1:] = common.to_tensor(centers)\n",
    "            self.box_hole_offsets = Pose.create_from_pq(p=box_hole_offsets)\n",
    "            hole_enlargement_factor = 1.5\n",
    "            self.box_hole_radii = common.to_tensor(radii * hole_enlargement_factor + self._clearance)\n",
    "\n",
    "            # in each parallel env we build a different box with a hole and peg (the task is meant to be quite difficult)\n",
    "            pegs = []\n",
    "            boxes = []\n",
    "\n",
    "            for i in range(self.num_envs):\n",
    "                scene_idxs = [i]\n",
    "                length = lengths[i]\n",
    "                radius = radii[i]\n",
    "                builder = self.scene.create_actor_builder()\n",
    "                builder.add_box_collision(half_size=[length, radius, radius])\n",
    "                # peg head\n",
    "                mat = sapien.render.RenderMaterial(\n",
    "                    base_color=sapien_utils.hex2rgba(\"#EC7357\"),\n",
    "                    roughness=0.5,\n",
    "                    specular=0.5,\n",
    "                )\n",
    "                builder.add_box_visual(\n",
    "                    sapien.Pose([length / 2, 0, 0]),\n",
    "                    half_size=[length / 2, radius, radius],\n",
    "                    material=mat,\n",
    "                )\n",
    "                # peg tail\n",
    "                mat = sapien.render.RenderMaterial(\n",
    "                    base_color=sapien_utils.hex2rgba(\"#EDF6F9\"),\n",
    "                    roughness=0.5,\n",
    "                    specular=0.5,\n",
    "                )\n",
    "                builder.add_box_visual(\n",
    "                    sapien.Pose([-length / 2, 0, 0]),\n",
    "                    half_size=[length / 2, radius, radius],\n",
    "                    material=mat,\n",
    "                )\n",
    "                builder.set_scene_idxs(scene_idxs)\n",
    "                peg = builder.build(f\"peg_{i}\")\n",
    "\n",
    "                # box with hole\n",
    "\n",
    "                inner_radius, outer_radius, depth = (\n",
    "                    radius * hole_enlargement_factor + self._clearance,\n",
    "                    length,\n",
    "                    length,\n",
    "                )\n",
    "                builder = _build_box_with_hole(\n",
    "                    self.scene, inner_radius, outer_radius, depth, center=centers[i]\n",
    "                )\n",
    "                builder.set_scene_idxs(scene_idxs)\n",
    "                box = builder.build_kinematic(f\"box_with_hole_{i}\")\n",
    "\n",
    "                pegs.append(peg)\n",
    "                boxes.append(box)\n",
    "            self.peg = Actor.merge(pegs, \"peg\")\n",
    "            self.box = Actor.merge(boxes, \"box_with_hole\")\n",
    "\n",
    "    def _initialize_episode(self, env_idx: torch.Tensor, options: dict):\n",
    "        with torch.device(self.device):\n",
    "            b = len(env_idx)\n",
    "            self.table_scene.initialize(env_idx)\n",
    "\n",
    "            # initialize the box and peg\n",
    "            xy = randomization.uniform(\n",
    "                low=torch.tensor([-0.1, -0.3]), high=torch.tensor([0.1, 0]), size=(b, 2)\n",
    "            )\n",
    "            pos = torch.zeros((b, 3))\n",
    "            pos[:, :2] = xy\n",
    "            pos[:, 2] = self.peg_half_sizes[env_idx, 2]\n",
    "            quat = randomization.random_quaternions(\n",
    "                b,\n",
    "                self.device,\n",
    "                lock_x=True,\n",
    "                lock_y=True,\n",
    "                bounds=(np.pi / 2 - np.pi / 3, np.pi / 2 + np.pi / 3),\n",
    "            )\n",
    "            self.peg.set_pose(Pose.create_from_pq(pos, quat))\n",
    "\n",
    "            xy = randomization.uniform(\n",
    "                low=torch.tensor([-0.05, 0.2]),\n",
    "                high=torch.tensor([0.05, 0.4]),\n",
    "                size=(b, 2),\n",
    "            )\n",
    "            pos = torch.zeros((b, 3))\n",
    "            pos[:, :2] = xy\n",
    "            pos[:, 2] = self.peg_half_sizes[env_idx, 0]\n",
    "            quat = randomization.random_quaternions(\n",
    "                b,\n",
    "                self.device,\n",
    "                lock_x=True,\n",
    "                lock_y=True,\n",
    "                bounds=(np.pi / 2 - np.pi / 8, np.pi / 2 + np.pi / 8),\n",
    "            )\n",
    "            self.box.set_pose(Pose.create_from_pq(pos, quat))\n",
    "\n",
    "            # Initialize the robot\n",
    "            qpos = np.array(\n",
    "                [\n",
    "                    0.0,\n",
    "                    np.pi / 8,\n",
    "                    0,\n",
    "                    -np.pi * 5 / 8,\n",
    "                    0,\n",
    "                    np.pi * 3 / 4,\n",
    "                    -np.pi / 4,\n",
    "                    0.04,\n",
    "                    0.04,\n",
    "                ]\n",
    "            )\n",
    "            qpos = self._episode_rng.normal(0, 0.02, (b, len(qpos))) + qpos\n",
    "            qpos[:, -2:] = 0.04\n",
    "            self.agent.robot.set_qpos(qpos)\n",
    "            self.agent.robot.set_pose(sapien.Pose([-0.615, 0, 0]))\n",
    "\n",
    "    # save some commonly used attributes\n",
    "    @property\n",
    "    def peg_head_pos(self):\n",
    "        return self.peg.pose.p + self.peg_head_offsets.p\n",
    "\n",
    "    @property\n",
    "    def peg_head_pose(self):\n",
    "        return self.peg.pose * self.peg_head_offsets\n",
    "\n",
    "    @property\n",
    "    def box_hole_pose(self):\n",
    "        return self.box.pose * self.box_hole_offsets\n",
    "\n",
    "    @property\n",
    "    def goal_pose(self):\n",
    "        # NOTE (stao): this is fixed after each _initialize_episode call. You can cache this value\n",
    "        # and simply store it after _initialize_episode or set_state_dict calls.\n",
    "        return self.box.pose * self.box_hole_offsets * self.peg_head_offsets.inv()\n",
    "\n",
    "    def has_peg_inserted(self):\n",
    "        # Only head position is used in fact\n",
    "        peg_head_pos_at_hole = (self.box_hole_pose.inv() * self.peg_head_pose).p\n",
    "        # x-axis is hole direction\n",
    "        x_flag = -0.015 <= peg_head_pos_at_hole[:, 0]\n",
    "        y_flag = (-self.box_hole_radii <= peg_head_pos_at_hole[:, 1]) & (\n",
    "            peg_head_pos_at_hole[:, 1] <= self.box_hole_radii\n",
    "        )\n",
    "        z_flag = (-self.box_hole_radii <= peg_head_pos_at_hole[:, 2]) & (\n",
    "            peg_head_pos_at_hole[:, 2] <= self.box_hole_radii\n",
    "        )\n",
    "        return (\n",
    "            x_flag & y_flag & z_flag,\n",
    "            peg_head_pos_at_hole,\n",
    "        )\n",
    "\n",
    "    def evaluate(self):\n",
    "        success, peg_head_pos_at_hole = self.has_peg_inserted()\n",
    "        return dict(success=success, peg_head_pos_at_hole=peg_head_pos_at_hole)\n",
    "\n",
    "    def _get_obs_extra(self, info: Dict):\n",
    "        obs = dict(tcp_pose=self.agent.tcp.pose.raw_pose)\n",
    "        if self._obs_mode in [\"state\", \"state_dict\"]:\n",
    "            obs.update(\n",
    "                peg_pose=self.peg.pose.raw_pose,\n",
    "                peg_half_size=self.peg_half_sizes,\n",
    "                box_hole_pose=self.box_hole_pose.raw_pose,\n",
    "                box_hole_radius=self.box_hole_radii,\n",
    "            )\n",
    "        return obs\n",
    "\n",
    "    def compute_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):\n",
    "        # Stage 1: Encourage gripper to be rotated to be lined up with the peg\n",
    "\n",
    "        # Stage 2: Encourage gripper to move close to peg tail and grasp it\n",
    "        gripper_pos = self.agent.tcp.pose.p\n",
    "        tgt_gripper_pose = self.peg.pose\n",
    "        offset = sapien.Pose(\n",
    "            [-0.06, 0, 0]\n",
    "        )  # account for panda gripper width with a bit more leeway\n",
    "        tgt_gripper_pose = tgt_gripper_pose * (offset)\n",
    "        gripper_to_peg_dist = torch.linalg.norm(\n",
    "            gripper_pos - tgt_gripper_pose.p, axis=1\n",
    "        )\n",
    "\n",
    "        reaching_reward = 1 - torch.tanh(4.0 * gripper_to_peg_dist)\n",
    "\n",
    "        # check with max_angle=20 to ensure gripper isn't grasping peg at an awkward pose\n",
    "        is_grasped = self.agent.is_grasping(self.peg, max_angle=20)\n",
    "        reward = reaching_reward + is_grasped\n",
    "\n",
    "        # Stage 3: Orient the grasped peg properly towards the hole\n",
    "\n",
    "        # pre-insertion award, encouraging both the peg center and the peg head to match the yz coordinates of goal_pose\n",
    "        peg_head_wrt_goal = self.goal_pose.inv() * self.peg_head_pose\n",
    "        peg_head_wrt_goal_yz_dist = torch.linalg.norm(\n",
    "            peg_head_wrt_goal.p[:, 1:], axis=1\n",
    "        )\n",
    "        peg_wrt_goal = self.goal_pose.inv() * self.peg.pose\n",
    "        peg_wrt_goal_yz_dist = torch.linalg.norm(peg_wrt_goal.p[:, 1:], axis=1)\n",
    "\n",
    "        pre_insertion_reward = 3 * (\n",
    "            1\n",
    "            - torch.tanh(\n",
    "                0.5 * (peg_head_wrt_goal_yz_dist + peg_wrt_goal_yz_dist)\n",
    "                + 4.5 * torch.maximum(peg_head_wrt_goal_yz_dist, peg_wrt_goal_yz_dist)\n",
    "            )\n",
    "        )\n",
    "        reward += pre_insertion_reward * is_grasped\n",
    "        # stage 3 passes if peg is correctly oriented in order to insert into hole easily\n",
    "        pre_inserted = (peg_head_wrt_goal_yz_dist < 0.01) & (\n",
    "            peg_wrt_goal_yz_dist < 0.01\n",
    "        )\n",
    "\n",
    "        # Stage 4: Insert the peg into the hole once it is grasped and lined up\n",
    "        peg_head_wrt_goal_inside_hole = self.box_hole_pose.inv() * self.peg_head_pose\n",
    "        insertion_reward = 5 * (\n",
    "            1\n",
    "            - torch.tanh(\n",
    "                5.0 * torch.linalg.norm(peg_head_wrt_goal_inside_hole.p, axis=1)\n",
    "            )\n",
    "        )\n",
    "        reward += insertion_reward * (is_grasped & pre_inserted)\n",
    "\n",
    "        reward[info[\"success\"]] = 10\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def compute_normalized_dense_reward(\n",
    "        self, obs: Any, action: torch.Tensor, info: Dict\n",
    "    ):\n",
    "        return self.compute_dense_reward(obs, action, info) / 10\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "@register_env(\"PlugCharger-v2\", max_episode_steps=200)\n",
    "class PlugChargerEnv(BaseEnv):\n",
    "    _base_size = [2e-2, 1.5e-2, 1.2e-2]  # charger base half size\n",
    "    _peg_size = [8e-3, 0.75e-3, 3.2e-3]  # charger peg half size\n",
    "    _peg_gap = 7e-3  # charger peg gap\n",
    "    _clearance = 5e-4  # single side clearance\n",
    "    _receptacle_size = [1e-2, 5e-2, 5e-2]  # receptacle half size\n",
    "    _hole_size_gap_x = 3\n",
    "    _hole_size_gap_y = 1.5\n",
    "\n",
    "    SUPPORTED_ROBOTS = [\"panda_wristcam\"]\n",
    "    agent: Union[PandaWristCam]\n",
    "\n",
    "    def __init__(\n",
    "        self, *args, robot_uids=\"panda_wristcam\", robot_init_qpos_noise=0.02, **kwargs\n",
    "    ):\n",
    "        self.robot_init_qpos_noise = robot_init_qpos_noise\n",
    "        super().__init__(*args, robot_uids=robot_uids, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _default_sim_config(self):\n",
    "        return SimConfig()\n",
    "\n",
    "    @property\n",
    "    def _default_sensor_configs(self):\n",
    "        pose = sapien_utils.look_at(eye=[0.3, 0, 0.6], target=[-0.1, 0, 0.1])\n",
    "        return [\n",
    "            CameraConfig(\"base_camera\", pose=pose, width=128, height=128, fov=np.pi / 2)\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def _default_human_render_camera_configs(self):\n",
    "        pose = sapien_utils.look_at([0.3, 0.4, 0.1], [0, 0, 0])\n",
    "        return [\n",
    "            CameraConfig(\n",
    "                \"render_camera\",\n",
    "                pose=pose,\n",
    "                width=512,\n",
    "                height=512,\n",
    "                fov=1,\n",
    "                mount=self.receptacle,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def _build_charger(self, peg_size, base_size, gap):\n",
    "        builder = self.scene.create_actor_builder()\n",
    "\n",
    "        # peg\n",
    "        mat = sapien.render.RenderMaterial()\n",
    "        mat.set_base_color([1, 1, 1, 1])\n",
    "        mat.metallic = 1.0\n",
    "        mat.roughness = 0.0\n",
    "        mat.specular = 1.0\n",
    "        builder.add_box_collision(sapien.Pose([peg_size[0], gap, 0]), peg_size)\n",
    "        builder.add_box_visual(\n",
    "            sapien.Pose([peg_size[0], gap, 0]), peg_size, material=mat\n",
    "        )\n",
    "        builder.add_box_collision(sapien.Pose([peg_size[0], -gap, 0]), peg_size)\n",
    "        builder.add_box_visual(\n",
    "            sapien.Pose([peg_size[0], -gap, 0]), peg_size, material=mat\n",
    "        )\n",
    "\n",
    "        # base\n",
    "        mat = sapien.render.RenderMaterial()\n",
    "        mat.set_base_color([1, 1, 1, 1])\n",
    "        mat.metallic = 0.0\n",
    "        mat.roughness = 0.1\n",
    "        builder.add_box_collision(sapien.Pose([-base_size[0], 0, 0]), base_size)\n",
    "        builder.add_box_visual(\n",
    "            sapien.Pose([-base_size[0], 0, 0]), base_size, material=mat\n",
    "        )\n",
    "\n",
    "        return builder.build(name=\"charger\")\n",
    "\n",
    "    def _build_receptacle(self, peg_size, receptacle_size, gap):\n",
    "        builder = self.scene.create_actor_builder()\n",
    "\n",
    "        sy = 0.5 * (receptacle_size[1] - peg_size[1] - gap)\n",
    "        sz = 0.5 * (receptacle_size[2] - peg_size[2])\n",
    "        dx = -receptacle_size[0]\n",
    "        dy = peg_size[1] + gap + sy\n",
    "        dz = peg_size[2] + sz\n",
    "\n",
    "        mat = sapien.render.RenderMaterial()\n",
    "        mat.set_base_color([1, 1, 1, 1])\n",
    "        mat.metallic = 0.0\n",
    "        mat.roughness = 0.1\n",
    "\n",
    "        poses = [\n",
    "            sapien.Pose([dx, 0, dz]),\n",
    "            sapien.Pose([dx, 0, -dz]),\n",
    "            sapien.Pose([dx, dy, 0]),\n",
    "            sapien.Pose([dx, -dy, 0]),\n",
    "        ]\n",
    "        half_sizes = [\n",
    "            [receptacle_size[0], receptacle_size[1], sz],\n",
    "            [receptacle_size[0], receptacle_size[1], sz],\n",
    "            [receptacle_size[0], sy, receptacle_size[2]],\n",
    "            [receptacle_size[0], sy, receptacle_size[2]],\n",
    "        ]\n",
    "        for pose, half_size in zip(poses, half_sizes):\n",
    "            builder.add_box_collision(pose, half_size)\n",
    "            builder.add_box_visual(pose, half_size, material=mat)\n",
    "\n",
    "        # Fill the gap\n",
    "        pose = sapien.Pose([-receptacle_size[0], 0, 0])\n",
    "        half_size = [receptacle_size[0], gap - peg_size[1], peg_size[2]]\n",
    "        builder.add_box_collision(pose, half_size)\n",
    "        builder.add_box_visual(pose, half_size, material=mat)\n",
    "\n",
    "        # Add dummy visual for hole\n",
    "        mat = sapien.render.RenderMaterial()\n",
    "        mat.set_base_color(sapien_utils.hex2rgba(\"#DBB539\"))\n",
    "        mat.metallic = 1.0\n",
    "        mat.roughness = 0.0\n",
    "        mat.specular = 1.0\n",
    "        pose = sapien.Pose([-receptacle_size[0], -(gap * 0.5 + peg_size[1]), 0])\n",
    "        half_size = [receptacle_size[0], peg_size[1], peg_size[2]]\n",
    "        builder.add_box_visual(pose, half_size, material=mat)\n",
    "        pose = sapien.Pose([-receptacle_size[0], gap * 0.5 + peg_size[1], 0])\n",
    "        builder.add_box_visual(pose, half_size, material=mat)\n",
    "\n",
    "        return builder.build_kinematic(name=\"receptacle\")\n",
    "\n",
    "    def _load_scene(self, options: dict):\n",
    "        self.scene_builder = TableSceneBuilder(\n",
    "            self, robot_init_qpos_noise=self.robot_init_qpos_noise\n",
    "        )\n",
    "        self.scene_builder.build()\n",
    "        self.charger = self._build_charger(\n",
    "            self._peg_size,\n",
    "            self._base_size,\n",
    "            self._peg_gap,\n",
    "        )\n",
    "        self.receptacle = self._build_receptacle(\n",
    "            [\n",
    "                self._peg_size[0],\n",
    "                self._peg_size[1] * self._hole_size_gap_x + self._clearance,\n",
    "                self._peg_size[2] * self._hole_size_gap_y + self._clearance,\n",
    "            ],\n",
    "            self._receptacle_size,\n",
    "            self._peg_gap,\n",
    "        )\n",
    "\n",
    "    def _initialize_episode(self, env_idx: torch.Tensor, options: dict):\n",
    "        with torch.device(self.device):\n",
    "            b = len(env_idx)\n",
    "            self.scene_builder.initialize(env_idx)\n",
    "\n",
    "            # Initialize agent\n",
    "            qpos = torch.tensor(\n",
    "                [\n",
    "                    0.0,\n",
    "                    np.pi / 8,\n",
    "                    0,\n",
    "                    -np.pi * 5 / 8,\n",
    "                    0,\n",
    "                    np.pi * 3 / 4,\n",
    "                    np.pi / 4,\n",
    "                    0.04,\n",
    "                    0.04,\n",
    "                ]\n",
    "            )\n",
    "            qpos = (\n",
    "                torch.normal(\n",
    "                    0, self.robot_init_qpos_noise, (b, len(qpos)), device=self.device\n",
    "                )\n",
    "                + qpos\n",
    "            )\n",
    "            qpos[:, -2:] = 0.04\n",
    "            self.agent.robot.set_qpos(qpos)\n",
    "            self.agent.robot.set_pose(sapien.Pose([-0.615, 0, 0]))\n",
    "\n",
    "            # Initialize charger\n",
    "            xy = randomization.uniform(\n",
    "                [-0.1, -0.2], [-0.01 - self._peg_size[0] * 2, 0.2], size=(b, 2)\n",
    "            )\n",
    "            pos = torch.zeros((b, 3))\n",
    "            pos[:, :2] = xy\n",
    "            pos[:, 2] = self._base_size[2]\n",
    "            ori = randomization.random_quaternions(\n",
    "                n=b, lock_x=True, lock_y=True, bounds=(-torch.pi / 3, torch.pi / 3)\n",
    "            )\n",
    "            self.charger.set_pose(Pose.create_from_pq(pos, ori))\n",
    "\n",
    "            # Initialize receptacle\n",
    "            xy = randomization.uniform([0.01, -0.1], [0.1, 0.1], size=(b, 2))\n",
    "            pos = torch.zeros((b, 3))\n",
    "            pos[:, :2] = xy\n",
    "            pos[:, 2] = 0.1\n",
    "            ori = randomization.random_quaternions(\n",
    "                n=b,\n",
    "                lock_x=True,\n",
    "                lock_y=True,\n",
    "                bounds=(torch.pi - torch.pi / 8, torch.pi + torch.pi / 8),\n",
    "            )\n",
    "            self.receptacle.set_pose(Pose.create_from_pq(pos, ori))\n",
    "\n",
    "            self.goal_pose = self.receptacle.pose * (\n",
    "                sapien.Pose(q=euler2quat(0, 0, np.pi))\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def charger_base_pose(self):\n",
    "        return self.charger.pose * (sapien.Pose([-self._base_size[0], 0, 0]))\n",
    "\n",
    "    def _compute_distance(self):\n",
    "        obj_pose = self.charger.pose\n",
    "        obj_to_goal_pos = self.goal_pose.p - obj_pose.p\n",
    "        obj_to_goal_dist = torch.linalg.norm(obj_to_goal_pos, axis=1)\n",
    "\n",
    "        obj_to_goal_quat = rotation_conversions.quaternion_multiply(\n",
    "            rotation_conversions.quaternion_invert(self.goal_pose.q), obj_pose.q\n",
    "        )\n",
    "        obj_to_goal_axis = rotation_conversions.quaternion_to_axis_angle(\n",
    "            obj_to_goal_quat\n",
    "        )\n",
    "        obj_to_goal_angle = torch.linalg.norm(obj_to_goal_axis, axis=1)\n",
    "        obj_to_goal_angle = torch.min(\n",
    "            obj_to_goal_angle, torch.pi * 2 - obj_to_goal_angle\n",
    "        )\n",
    "\n",
    "        return obj_to_goal_dist, obj_to_goal_angle\n",
    "\n",
    "    def evaluate(self):\n",
    "        obj_to_goal_dist, obj_to_goal_angle = self._compute_distance()\n",
    "        success = (obj_to_goal_dist <= 5e-3) & (obj_to_goal_angle <= 0.2)\n",
    "        return dict(\n",
    "            obj_to_goal_dist=obj_to_goal_dist,\n",
    "            obj_to_goal_angle=obj_to_goal_angle,\n",
    "            success=success,\n",
    "        )\n",
    "\n",
    "    def _get_obs_extra(self, info: Dict):\n",
    "        obs = dict(tcp_pose=self.agent.tcp.pose.raw_pose)\n",
    "        if self._obs_mode in [\"state\", \"state_dict\"]:\n",
    "            obs.update(\n",
    "                charger_pose=self.charger.pose.raw_pose,\n",
    "                receptacle_pose=self.receptacle.pose.raw_pose,\n",
    "                goal_pose=self.goal_pose.raw_pose,\n",
    "            )\n",
    "        return obs\n",
    "\n",
    "    def compute_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):\n",
    "        return torch.zeros(self.num_envs, device=self.device)\n",
    "\n",
    "    def compute_normalized_dense_reward(\n",
    "        self, obs: Any, action: torch.Tensor, info: Dict\n",
    "    ):\n",
    "        max_reward = 1.0\n",
    "        return self.compute_dense_reward(obs=obs, action=action, info=info) / max_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_data(data):\n",
    "    out = dict()\n",
    "    for k in data.keys():\n",
    "        if isinstance(data[k], h5py.Dataset):\n",
    "            out[k] = data[k][:]\n",
    "        else:\n",
    "            out[k] = load_h5_data(data[k])\n",
    "    return out\n",
    "\n",
    "\n",
    "def create_sample_indices(episode_ends: np.ndarray, sequence_length: int, pad_before: int = 0, pad_after: int = 0):\n",
    "    # Currently uses truncated as episode ends which is the end of the episode and not the end of the trajectory\n",
    "    indices = list()\n",
    "    episode_length = 0\n",
    "    episode_index = 1 # Start 1 for human readability\n",
    "    #print(f\"episode_ends: {episode_ends}\")\n",
    "    #print(f\"len(episode_ends): {len(episode_ends)}\")\n",
    "    end_of_last_episode = False\n",
    "    for i in range(len(episode_ends)):\n",
    "        episode_length += 1\n",
    "        if episode_ends[i] and not end_of_last_episode:\n",
    "            start_idx = 0 if i <= 0 else i - episode_length + 1\n",
    "            min_start = -pad_before\n",
    "            max_start = episode_length - sequence_length + pad_after\n",
    "\n",
    "            # Create indices for each possible sequence in the episode\n",
    "            for idx in range(min_start, max_start + 1):\n",
    "                buffer_start_idx = max(idx, 0) + start_idx\n",
    "                buffer_end_idx = min(idx + sequence_length, episode_length) + start_idx\n",
    "                start_offset = buffer_start_idx - (idx + start_idx)\n",
    "                end_offset = (idx + sequence_length + start_idx) - buffer_end_idx\n",
    "                sample_start_idx = 0 + start_offset\n",
    "                sample_end_idx = sequence_length - end_offset\n",
    "                indices.append([buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx])\n",
    "            #print(f\"Episode {episode_index} has {episode_length} steps\")\n",
    "            episode_length = 0\n",
    "            episode_index += 1\n",
    "            end_of_last_episode = True\n",
    "        elif not episode_ends[i]:\n",
    "            end_of_last_episode = False\n",
    "    #print(f\"Created {len(indices)} samples from {episode_index - 1} episodes\")\n",
    "    #print(f\"All indices: {indices}\")\n",
    "    return np.array(indices)\n",
    "\n",
    "\n",
    "\n",
    "def sample_sequence(train_data, sequence_length, buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx):\n",
    "    result = dict()\n",
    "    for key, input_arr in train_data.items():\n",
    "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
    "        data = sample\n",
    "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
    "            if isinstance(input_arr, torch.Tensor):\n",
    "                data = torch.zeros((sequence_length,) + input_arr.shape[1:], dtype=input_arr.dtype)\n",
    "            else:\n",
    "                data = np.zeros(shape=(sequence_length,) + input_arr.shape[1:], dtype=input_arr.dtype)\n",
    "            if sample_start_idx > 0:\n",
    "                data[:sample_start_idx] = sample[0]\n",
    "            if sample_end_idx < sequence_length:\n",
    "                data[sample_end_idx:] = sample[-1]\n",
    "            data[sample_start_idx:sample_end_idx] = sample\n",
    "        result[key] = data\n",
    "    return result\n",
    "\n",
    "def remove_np_uint16(x: Union[np.ndarray, dict]):\n",
    "            if isinstance(x, dict):\n",
    "                for k in x.keys():\n",
    "                    x[k] = remove_np_uint16(x[k])\n",
    "                return x\n",
    "            else:\n",
    "                if x.dtype == np.uint16:\n",
    "                    return x.astype(np.int32)\n",
    "                return x\n",
    "\n",
    "def convert_observation(obs, task_id):\n",
    "    # adds task_id to the observation\n",
    "    values = list(obs.values())\n",
    "    example = values[0]\n",
    "    if isinstance(example, torch.Tensor):\n",
    "          example = example.numpy()\n",
    "\n",
    "    # add task_id to the observation\n",
    "    task_id_array = np.full((example.shape[0], 1), task_id, dtype=example.dtype) \n",
    "    values.append(task_id_array)\n",
    "    # concatenate all the values\n",
    "    return np.concatenate(values, axis=-1)\n",
    "\n",
    "def get_observations(obs):\n",
    "    #ensoure that the observations are in the correct format\n",
    "    #and ordered correctly across tasks\n",
    "\n",
    "    cleaned_obs = OrderedDict()\n",
    "    cleaned_obs[\"qpos\"] = obs[\"agent\"][\"qpos\"]\n",
    "    cleaned_obs[\"qvel\"] = obs[\"agent\"][\"qvel\"]\n",
    "    cleaned_obs[\"tcp_pose\"] = obs[\"extra\"][\"tcp_pose\"]\n",
    "    obs[\"extra\"].pop(\"tcp_pose\")\n",
    "\n",
    "    #this code is not generic and only works for the specific observation spaces we have\n",
    "    # Handle different goal position formats gracefully\n",
    "    goal_pose_keys = [\"goal_pose\", \"goal_pos\", \"box_hole_pose\", \"cubeB_pose\"]\n",
    "    for key in goal_pose_keys:\n",
    "        if key in obs[\"extra\"]:\n",
    "            pos = obs[\"extra\"][key]\n",
    "\n",
    "            # Ensure 'pos' is 2D with the correct number of columns\n",
    "            if pos.ndim == 1:\n",
    "                pos = pos.reshape(1, -1)  # Reshape to 2D if necessary\n",
    "            elif pos.ndim > 2:\n",
    "                raise ValueError(f\"Unexpected dimensions for '{key}': {pos.shape}\")\n",
    "\n",
    "            # Pad or truncate 'pos' to have 7 columns\n",
    "            pos = np.pad(pos[:, :7], ((0, 0), (0, 7 - pos.shape[1])), mode='constant')\n",
    "            if isinstance(cleaned_obs[\"tcp_pose\"], torch.Tensor):\n",
    "                pos = torch.tensor(pos, dtype=cleaned_obs[\"tcp_pose\"].dtype)\n",
    "                \n",
    "            cleaned_obs[\"goal_pose\"] = pos\n",
    "            obs[\"extra\"].pop(key)\n",
    "            break  # Stop once a valid goal pose key is found\n",
    "    else:\n",
    "        print(\"No goal pose found. Setting to zero.\")\n",
    "        length = len(cleaned_obs[\"tcp_pose\"])\n",
    "        cleaned_obs[\"goal_pose\"] = np.zeros((length, 7), dtype=np.float32)  # Ensure 2D shape\n",
    "        \n",
    "    #is_grasped_reshaped = np.reshape(obs[\"extra\"][\"is_grasped\"], (len(obs[\"extra\"][\"is_grasped\"]), 1))\n",
    "    \n",
    "    # Filter and add other observations with 7 columns\n",
    "    for key, value in obs[\"extra\"].items():\n",
    "        if value.shape[-1] == 7 and value.ndim == 2:\n",
    "            if key != \"receptacle_pose\":\n",
    "                cleaned_obs[key] = value\n",
    "\n",
    "    count = 0\n",
    "    for key in cleaned_obs.keys():\n",
    "        count += cleaned_obs[key].shape[-1]\n",
    "    \n",
    "    assert count == 39, \"Observation size is not 39\"\n",
    "\n",
    "    \n",
    "    return cleaned_obs\n",
    "\n",
    "\n",
    "\n",
    "def get_min_max_values(dataloader, exclude_features):\n",
    "    min_obs = None\n",
    "    max_obs = None\n",
    "    min_actions = None\n",
    "    max_actions = None\n",
    "    mask = None\n",
    "    for batch in dataloader:\n",
    "      for key, value in batch.items():\n",
    "        obs_reshaped = batch[key].view(-1, batch[key].shape[-1])\n",
    "        if key == \"obs\":\n",
    "          if mask is None:\n",
    "            mask = torch.ones(obs_reshaped.shape[1], dtype=torch.bool)\n",
    "            mask[exclude_features] = False\n",
    "          min_obs = obs_reshaped[:, mask].min(dim=0).values\n",
    "          max_obs = obs_reshaped[:, mask].max(dim=0).values\n",
    "        else:\n",
    "          min_actions = obs_reshaped.min(dim=0).values\n",
    "          max_actions = obs_reshaped.max(dim=0).values\n",
    "    return {\"obs\": {\"min\": min_obs, \"max\": max_obs, \"mask\": mask}, \"actions\": {\"min\": min_actions, \"max\": max_actions}}\n",
    "\n",
    "def normalize_batch(batch, stats):\n",
    "    for key, value in batch.items():\n",
    "      batch_reshaped = batch[key].view(-1, batch[key].shape[-1])\n",
    "\n",
    "      normalized_batch = batch_reshaped.clone()\n",
    "      if key == \"obs\":\n",
    "        normalized_batch[:, stats[key][\"mask\"]] = (batch_reshaped[:, stats[key][\"mask\"]] - stats[key][\"min\"]) / (stats[key][\"max\"] - stats[key][\"min\"] + 0.1)\n",
    "      else:\n",
    "        normalized_batch = (batch_reshaped - stats[key][\"min\"]) / (stats[key][\"max\"] - stats[key][\"min\"] + 0.1)\n",
    "      batch[key] = normalized_batch.view(batch[key].shape)\n",
    "    return batch\n",
    "\n",
    "def denormalize_batch(batch, stats):\n",
    "    for key, value in batch.items():\n",
    "      batch_reshaped = batch[key].view(-1, batch[key].shape[-1])\n",
    "\n",
    "      denormalized_batch = batch_reshaped.clone()\n",
    "      if key == \"obs\":\n",
    "        denormalized_batch[:, stats[key][\"mask\"]] = batch_reshaped[:, stats[key][\"mask\"]] * (stats[key][\"max\"] - stats[key][\"min\"] + 0.1) + stats[key][\"min\"]\n",
    "      else:\n",
    "        denormalized_batch = batch_reshaped * (stats[key][\"max\"] - stats[key][\"min\"] + 0.1) + stats[key][\"min\"]\n",
    "      batch[key] = denormalized_batch.view(batch[key].shape)\n",
    "    return batch\n",
    "\n",
    "\n",
    "class StateDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A general torch Dataset you can drop in and use immediately with just about any trajectory .h5 data generated from ManiSkill.\n",
    "    This class simply is a simple starter code to load trajectory data easily, but does not do any data transformation or anything\n",
    "    advanced. We recommend you to copy this code directly and modify it for more advanced use cases\n",
    "\n",
    "    Args:\n",
    "        dataset_file (str): path to the .h5 file containing the data you want to load\n",
    "        load_count (int): the number of trajectories from the dataset to load into memory. If -1, will load all into memory\n",
    "        success_only (bool): whether to skip trajectories that are not successful in the end. Default is false\n",
    "        device: The location to save data to. If None will store as numpy (the default), otherwise will move data to that device\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataset_file: str, pred_horizon: int, obs_horizon: int, action_horizon:int, task_id: np.float32, load_count=-1, device=None\n",
    "    ) -> None:\n",
    "        self.dataset_file = dataset_file\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.obs_horizon = obs_horizon\n",
    "        self.action_horizon = action_horizon\n",
    "        self.task_id = task_id\n",
    "        self.device = device\n",
    "        self.data = h5py.File(dataset_file, \"r\")\n",
    "        json_path = dataset_file.replace(\".h5\", \".json\")\n",
    "        self.json_data = load_json(json_path)\n",
    "        self.episodes = self.json_data[\"episodes\"]\n",
    "        self.env_info = self.json_data[\"env_info\"]\n",
    "        self.env_id = self.env_info[\"env_id\"]\n",
    "        self.env_kwargs = self.env_info[\"env_kwargs\"]\n",
    "\n",
    "        self.obs = None\n",
    "        self.actions = []\n",
    "        self.terminated = []\n",
    "        self.truncated = []\n",
    "        self.end_episode = []\n",
    "        self.success, self.fail, self.rewards = None, None, None\n",
    "        if load_count == -1:\n",
    "            load_count = len(self.episodes)\n",
    "        for eps_id in tqdm(range(load_count), desc=\"Loading Episodes\", colour=\"green\"):\n",
    "            eps = self.episodes[eps_id]\n",
    "            assert (\n",
    "                \"success\" in eps\n",
    "            ), \"episodes in this dataset do not have the success attribute, cannot load dataset with success_only=True\"\n",
    "            if not eps[\"success\"]:\n",
    "                continue\n",
    "            trajectory = self.data[f\"traj_{eps['episode_id']}\"]\n",
    "            trajectory = load_h5_data(trajectory)\n",
    "            eps_len = len(trajectory[\"actions\"])\n",
    "            #print(f\"Episode {eps_id} has {eps_len} steps\")\n",
    "\n",
    "            # exclude the final observation as most learning workflows do not use it\n",
    "            obs = common.index_dict_array(trajectory[\"obs\"], slice(eps_len))\n",
    "            if eps_id == 0:\n",
    "                self.obs = obs\n",
    "            else:\n",
    "                self.obs = common.append_dict_array(self.obs, obs)\n",
    "\n",
    "            self.actions.append(trajectory[\"actions\"])\n",
    "            self.terminated.append(trajectory[\"terminated\"])\n",
    "            self.truncated.append(trajectory[\"truncated\"])\n",
    "\n",
    "\n",
    "            end_episode = [False] * eps_len\n",
    "            end_episode[-1] = True\n",
    "            #is_terminated = False\n",
    "            #for i in range(len(end_episode)):\n",
    "            #    if trajectory[\"terminated\"][i] == True or is_terminated:\n",
    "            #        end_episode[i] = True\n",
    "            #        is_terminated = True\n",
    "            #    else:\n",
    "            #        end_episode[i] = False\n",
    "\n",
    "            #print(f\"Episode {eps_id} has {end_episode.count(True)} end of episodes\")\n",
    "            self.end_episode.append(end_episode)\n",
    "            #self.truncated[self.terminated:] = True\n",
    "\n",
    "            # handle data that might optionally be in the trajectory\n",
    "            if \"rewards\" in trajectory:\n",
    "                if self.rewards is None:\n",
    "                    self.rewards = [trajectory[\"rewards\"]]\n",
    "                else:\n",
    "                    self.rewards.append(trajectory[\"rewards\"])\n",
    "            if \"success\" in trajectory:\n",
    "                if self.success is None:\n",
    "                    self.success = [trajectory[\"success\"]]\n",
    "                else:\n",
    "                    self.success.append(trajectory[\"success\"])\n",
    "            if \"fail\" in trajectory:\n",
    "                if self.fail is None:\n",
    "                    self.fail = [trajectory[\"fail\"]]\n",
    "                else:\n",
    "                    self.fail.append(trajectory[\"fail\"])\n",
    "\n",
    "        self.actions = np.vstack(self.actions)\n",
    "        self.terminated = np.concatenate(self.terminated)\n",
    "        self.truncated = np.concatenate(self.truncated)\n",
    "        self.end_episode = np.concatenate(self.end_episode)\n",
    "        \n",
    "\n",
    "\n",
    "        if self.rewards is not None:\n",
    "            self.rewards = np.concatenate(self.rewards)\n",
    "        if self.success is not None:\n",
    "            self.success = np.concatenate(self.success)\n",
    "        if self.fail is not None:\n",
    "            self.fail = np.concatenate(self.fail)\n",
    "\n",
    "        def remove_np_uint16(x: Union[np.ndarray, dict]):\n",
    "            if isinstance(x, dict):\n",
    "                for k in x.keys():\n",
    "                    x[k] = remove_np_uint16(x[k])\n",
    "                return x\n",
    "            else:\n",
    "                if x.dtype == np.uint16:\n",
    "                    return x.astype(np.int32)\n",
    "                return x\n",
    "\n",
    "        # uint16 dtype is used to conserve disk space and memory\n",
    "        # you can optimize this dataset code to keep it as uint16 and process that\n",
    "        # dtype of data yourself. for simplicity we simply cast to a int32 so\n",
    "        # it can automatically be converted to torch tensors without complaint\n",
    "        self.obs = remove_np_uint16(self.obs)\n",
    "        \n",
    "        if device is not None:\n",
    "            self.actions = common.to_tensor(self.actions, device=device)\n",
    "            self.obs = common.to_tensor(self.obs, device=device)\n",
    "            self.terminated = common.to_tensor(self.terminated, device=device)\n",
    "            self.truncated = common.to_tensor(self.truncated, device=device)\n",
    "            if self.rewards is not None:\n",
    "                self.rewards = common.to_tensor(self.rewards, device=device)\n",
    "            if self.success is not None:\n",
    "                self.success = common.to_tensor(self.terminated, device=device)\n",
    "            if self.fail is not None:\n",
    "                self.fail = common.to_tensor(self.truncated, device=device)\n",
    "        \n",
    "\n",
    "\n",
    "        # Added code for diffusion policy\n",
    "        obs_dict = get_observations(self.obs)\n",
    "        self.train_data = dict(\n",
    "                        obs=convert_observation(obs_dict, self.task_id),\n",
    "                        actions=self.actions,\n",
    "                        )\n",
    "\n",
    "         # Initialize index lists and stat dicts\n",
    "        self.indices = create_sample_indices(\n",
    "            episode_ends=self.end_episode, \n",
    "            sequence_length=self.pred_horizon,\n",
    "            pad_before=self.obs_horizon - 1,\n",
    "            pad_after=self.action_horizon - 1\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # all possible sequenzes of the dataset\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Change data to fit diffusion policy\n",
    "        buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx = self.indices[idx]\n",
    "\n",
    "    \n",
    "        sampled = sample_sequence(\n",
    "            train_data=self.train_data, \n",
    "            sequence_length=self.pred_horizon,\n",
    "            buffer_start_idx=buffer_start_idx,\n",
    "            buffer_end_idx=buffer_end_idx,\n",
    "            sample_start_idx=sample_start_idx,\n",
    "            sample_end_idx=sample_end_idx\n",
    "        )\n",
    "    \n",
    "        # discard unused observations in the sequence\n",
    "        for k in sampled.keys():\n",
    "            if k != \"actions\":\n",
    "                # discard unused observations in the sequence\n",
    "                sampled[k] = sampled[k][:self.obs_horizon,:]\n",
    "        sampled[k] = common.to_tensor(sampled[k], device=self.device)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class Downsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    '''\n",
    "        Conv1d --> GroupNorm --> Mish\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.GroupNorm(n_groups, out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ConditionalResidualBlock1D(nn.Module):\n",
    "    def __init__(self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            cond_dim,\n",
    "            kernel_size=3,\n",
    "            n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "        ])\n",
    "\n",
    "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
    "        # predicts per-channel scale and bias\n",
    "        cond_channels = out_channels * 2\n",
    "        self.out_channels = out_channels\n",
    "        self.cond_encoder = nn.Sequential(\n",
    "            nn.Mish(),\n",
    "            nn.Linear(cond_dim, cond_channels),\n",
    "            nn.Unflatten(-1, (-1, 1))\n",
    "        )\n",
    "\n",
    "        # make sure dimensions compatible\n",
    "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        '''\n",
    "            x : [ batch_size x in_channels x horizon ]\n",
    "            cond : [ batch_size x cond_dim]\n",
    "\n",
    "            returns:\n",
    "            out : [ batch_size x out_channels x horizon ]\n",
    "        '''\n",
    "        out = self.blocks[0](x)\n",
    "        embed = self.cond_encoder(cond)\n",
    "\n",
    "        embed = embed.reshape(\n",
    "            embed.shape[0], 2, self.out_channels, 1)\n",
    "        scale = embed[:,0,...]\n",
    "        bias = embed[:,1,...]\n",
    "        out = scale * out + bias\n",
    "\n",
    "        out = self.blocks[1](out)\n",
    "        out = out + self.residual_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConditionalUnet1D(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim,\n",
    "        global_cond_dim,\n",
    "        diffusion_step_embed_dim=256,\n",
    "        down_dims=[256,512,1024],\n",
    "        kernel_size=5,\n",
    "        n_groups=8\n",
    "        ):\n",
    "        \"\"\"\n",
    "        input_dim: Dim of actions.\n",
    "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
    "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
    "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
    "        down_dims: Channel size for each UNet level.\n",
    "          The length of this array determines numebr of levels.\n",
    "        kernel_size: Conv kernel size\n",
    "        n_groups: Number of groups for GroupNorm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        all_dims = [input_dim] + list(down_dims)\n",
    "        start_dim = down_dims[0]\n",
    "\n",
    "        dsed = diffusion_step_embed_dim\n",
    "        diffusion_step_encoder = nn.Sequential(\n",
    "            SinusoidalPosEmb(dsed),\n",
    "            nn.Linear(dsed, dsed * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(dsed * 4, dsed),\n",
    "        )\n",
    "        cond_dim = dsed + global_cond_dim\n",
    "\n",
    "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
    "        mid_dim = all_dims[-1]\n",
    "        self.mid_modules = nn.ModuleList([\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        down_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            down_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        up_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            up_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        final_conv = nn.Sequential(\n",
    "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
    "            nn.Conv1d(start_dim, input_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.diffusion_step_encoder = diffusion_step_encoder\n",
    "        self.up_modules = up_modules\n",
    "        self.down_modules = down_modules\n",
    "        self.final_conv = final_conv\n",
    "\n",
    "        print(\"number of parameters: {:e}\".format(\n",
    "            sum(p.numel() for p in self.parameters()))\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "            sample: torch.Tensor,\n",
    "            timestep: Union[torch.Tensor, float, int],\n",
    "            global_cond=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,input_dim)\n",
    "        timestep: (B,) or int, diffusion step\n",
    "        global_cond: (B,global_cond_dim)\n",
    "        output: (B,T,input_dim)\n",
    "        \"\"\"\n",
    "        # (B,T,C)\n",
    "        sample = sample.moveaxis(-1,-2)\n",
    "        # (B,C,T)\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        global_feature = self.diffusion_step_encoder(timesteps)\n",
    "\n",
    "        if global_cond is not None:\n",
    "            global_feature = torch.cat([\n",
    "                global_feature, global_cond\n",
    "            ], axis=-1)\n",
    "\n",
    "        x = sample\n",
    "        h = []\n",
    "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        for mid_module in self.mid_modules:\n",
    "            x = mid_module(x, global_feature)\n",
    "\n",
    "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # (B,C,T)\n",
    "        x = x.moveaxis(-1,-2)\n",
    "        # (B,T,C)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#=====================================CHANGE=========================================\n",
    "env_id = 'PickCube-v1'\n",
    "env_id_transfer = 'StackCube-v1'\n",
    "#env_id = 'StackCube-v1'\n",
    "#env_id = 'PegInsertionSide-v2'\n",
    "#env_id = 'PlugCharger-v2'\n",
    "#env_id = 'PushCube-v1'\n",
    "obs_mode = 'state_dict'\n",
    "control_mode = 'pd_joint_delta_pos'\n",
    "\n",
    "pred_horizon = 16\n",
    "obs_horizon = 2\n",
    "action_horizon = 8\n",
    "\n",
    "num_epochs = 50 # number of epochs to train (default: 50)\n",
    "\n",
    "#======================================CHANGE========================================\n",
    "\n",
    "task_id = {\n",
    "    'PickCube-v1': 0.0,\n",
    "    'StackCube-v1': 0.1,\n",
    "    'PegInsertionSide-v1': 0.2,\n",
    "    'PlugCharger-v1': 0.3,\n",
    "    'PushCube-v1': 0.4\n",
    "}\n",
    "\n",
    "#exclude_features = [25, 26, 27, 28, 29, 30, 31, 39] # goal pose x, y, z, qw, qx, qy, qz and task_id\n",
    "exclude_features = [39] # task_id should be excluded and not used for normalization\n",
    "\n",
    "# part of the path to the dataset\n",
    "base_path = '/content/drive/MyDrive/Data'\n",
    "generated_path = f'{base_path}/Generated/{env_id}/motionplanning'\n",
    "generated_path_transfer = f'{base_path}/Generated/{env_id_transfer}/motionplanning'\n",
    "checkpoints_load_path = f'{base_path}/Checkpoints/{env_id}'\n",
    "checkpoints_path = f'{base_path}/Checkpoints/{env_id}_to_{env_id_transfer}'\n",
    "results_path = f'{base_path}/Results/{env_id}_to_{env_id_transfer}'\n",
    "information = f'_p{pred_horizon}_o{obs_horizon}_a{action_horizon}_e{100}' # probably 100 for epochs\n",
    "information_transfer = f'_p{pred_horizon}_o{obs_horizon}_a{action_horizon}_e{num_epochs}'\n",
    "\n",
    "# load data\n",
    "train_dataset_path_transfer = f'{generated_path_transfer}/training.{obs_mode}.{control_mode}.h5'\n",
    "val_dataset_path = f'{generated_path}/validation.{obs_mode}.{control_mode}.h5'\n",
    "val_dataset_path_transfer = f'{generated_path_transfer}/validation.{obs_mode}.{control_mode}.h5'\n",
    "model_path = f'{checkpoints_load_path}/model{information}.pt'\n",
    "\n",
    "# save results\n",
    "model_path_transfer = f'{checkpoints_path}/model{information_transfer}.pt'\n",
    "loss_path = f'{results_path}/loss{information_transfer}.npz'\n",
    "plot_path = f'{results_path}/plot{information_transfer}.png'\n",
    "animation_path = f'{results_path}/animation{information_transfer}.gif'\n",
    "\n",
    "\n",
    "# create dataset from file\n",
    "train_dataset = StateDataset(\n",
    "    dataset_file=train_dataset_path_transfer,\n",
    "    pred_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    "    action_horizon=action_horizon,\n",
    "    task_id=task_id[env_id_transfer],\n",
    "    load_count=300, # should be around 1000 for training\n",
    "    device=None\n",
    ")\n",
    "\n",
    "val_dataset = StateDataset(\n",
    "    dataset_file=val_dataset_path,\n",
    "    pred_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    "    action_horizon=action_horizon,\n",
    "    task_id=task_id[env_id],\n",
    "    load_count=100, # should be around 100 for validation\n",
    "    device=None\n",
    ")\n",
    "\n",
    "val_dataset_transfer = StateDataset(\n",
    "    dataset_file=val_dataset_path_transfer,\n",
    "    pred_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    "    action_horizon=action_horizon,\n",
    "    task_id=task_id[env_id_transfer],\n",
    "    load_count=100, # should be around 100 for validation\n",
    "    device=None\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    num_workers=1,\n",
    "    # don't kill worker process afte each epoch\n",
    "    persistent_workers=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,\n",
    "    num_workers=1,\n",
    "    # don't kill worker process afte each epoch\n",
    "    persistent_workers=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader_transfer = DataLoader(\n",
    "    val_dataset_transfer,\n",
    "    batch_size=128,\n",
    "    num_workers=1,\n",
    "    # don't kill worker process afte each epoch\n",
    "    persistent_workers=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "stats = get_min_max_values(train_dataloader, exclude_features)\n",
    "\n",
    "# visualize data in batch\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "print(\"Data obs:\", batch['obs'].shape, batch['obs'].dtype)\n",
    "print(\"Data actions:\", batch['actions'].shape, batch['actions'].dtype)\n",
    "\n",
    "\n",
    "# observation and action dimensions corrsponding to the dataset\n",
    "obs_dim = batch['obs'].shape[-1]\n",
    "action_dim = batch['actions'].shape[-1]\n",
    "print(\"obs_dim:\", obs_dim)\n",
    "print(\"action_dim:\", action_dim)\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim*obs_horizon\n",
    ")\n",
    "\n",
    "# example inputs\n",
    "noised_action = torch.randn((1, pred_horizon, action_dim))\n",
    "obs = torch.zeros((1, obs_horizon, obs_dim))\n",
    "diffusion_iter = torch.zeros((1,))\n",
    "\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "num_diffusion_iters = 100\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    # the choise of beta schedule has big impact on performance\n",
    "    # we found squared cosine works the best\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    # clip output to [-1,1] to improve stability\n",
    "    clip_sample=True,\n",
    "    # our network predicts noise (instead of denoised action)\n",
    "    prediction_type='epsilon'\n",
    ")\n",
    "\n",
    "# device transfer\n",
    "device = torch.device(\"cuda\")\n",
    "_ = noise_pred_net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential Moving Average\n",
    "# accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "ema = EMAModel(\n",
    "    parameters=noise_pred_net.parameters(),\n",
    "    power=0.75)\n",
    "\n",
    "# Standard ADAM optimizer\n",
    "# Note that EMA parametesr are not optimized\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=noise_pred_net.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(train_dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "state_dict = torch.load(model_path, map_location='cuda')\n",
    "noise_pred_net.load_state_dict(state_dict['model_state_dict'])\n",
    "print('Pretrained weights loaded.')\n",
    "\n",
    "train_losses = list()\n",
    "val_losses = list()\n",
    "val_losses_transfer = list()\n",
    "\n",
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    # epoch loop\n",
    "    for epoch_idx in tglobal:\n",
    "        train_loss = list()\n",
    "        \n",
    "        # training loop\n",
    "        noise_pred_net.train()\n",
    "        with tqdm(train_dataloader, desc='Train Batch', leave=False) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                # data normalized in dataset\n",
    "                nbatch = normalize_batch({\"obs\": batch[\"obs\"]}, stats)\n",
    "                nbatch = nbatch[\"obs\"]\n",
    "\n",
    "                # device transfer\n",
    "                nobs = nbatch.to(device)\n",
    "                naction = batch['actions'].to(device)\n",
    "                B = nobs.shape[0]\n",
    "\n",
    "                # observation as FiLM conditioning\n",
    "                # (B, obs_horizon, obs_dim)\n",
    "                obs_cond = nobs[:,:obs_horizon,:]\n",
    "                # (B, obs_horizon * obs_dim)\n",
    "                obs_cond = obs_cond.flatten(start_dim=1)\n",
    "\n",
    "                # sample noise to add to actions\n",
    "                noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "                # sample a diffusion iteration for each data point\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps,\n",
    "                    (B,), device=device\n",
    "                ).long()\n",
    "\n",
    "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_actions = noise_scheduler.add_noise(\n",
    "                    naction, noise, timesteps)\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = noise_pred_net(\n",
    "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "                # L2 loss\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "                # optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # step lr scheduler every batch\n",
    "                # this is different from standard pytorch behavior\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # update Exponential Moving Average of the model weights\n",
    "                ema.step(noise_pred_net.parameters())\n",
    "\n",
    "                # logging\n",
    "                loss_cpu = loss.item()\n",
    "                train_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "        \n",
    "        \n",
    "        # validation loop\n",
    "        val_loss = list()\n",
    "        noise_pred_net.eval()\n",
    "        with torch.no_grad():\n",
    "            with tqdm(val_dataloader, desc='Val Batch', leave=False) as vepoch:\n",
    "                for batch in vepoch:\n",
    "                    # Normalize data\n",
    "                    nbatch = normalize_batch({\"obs\": batch[\"obs\"]}, stats)\n",
    "                    nbatch = nbatch[\"obs\"]\n",
    "                    # Device transfer\n",
    "                    nobs = nbatch.to(device)\n",
    "                    naction = batch['actions'].to(device)\n",
    "                    B = nobs.shape[0]\n",
    "\n",
    "                    # Observation as FiLM conditioning\n",
    "                    obs_cond = nobs[:, :obs_horizon, :].flatten(start_dim=1)\n",
    "\n",
    "                    # Sample noise\n",
    "                    noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "                    # Sample diffusion iteration\n",
    "                    timesteps = torch.randint(\n",
    "                        0, noise_scheduler.config.num_train_timesteps,\n",
    "                        (B,), device=device\n",
    "                    ).long()\n",
    "\n",
    "                    # Add noise to actions\n",
    "                    noisy_actions = noise_scheduler.add_noise(\n",
    "                        naction, noise, timesteps)\n",
    "\n",
    "                    # Predict noise residual\n",
    "                    noise_pred = noise_pred_net(\n",
    "                        noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "                    # L2 loss\n",
    "                    loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "                    # Logging\n",
    "                    loss_cpu = loss.item()\n",
    "                    val_loss.append(loss_cpu)\n",
    "                    vepoch.set_postfix(loss=loss_cpu)\n",
    "\n",
    "        # validation loop 2\n",
    "        val_loss_transfer = list()\n",
    "        noise_pred_net.eval()\n",
    "        with torch.no_grad():\n",
    "            with tqdm(val_dataloader_transfer, desc='Val Batch', leave=False) as vepoch:\n",
    "                for batch in vepoch:\n",
    "                    # Normalize data\n",
    "                    nbatch = normalize_batch({\"obs\": batch[\"obs\"]}, stats)\n",
    "                    nbatch = nbatch[\"obs\"]\n",
    "                    # Device transfer\n",
    "                    nobs = nbatch.to(device)\n",
    "                    naction = batch['actions'].to(device)\n",
    "                    B = nobs.shape[0]\n",
    "\n",
    "                    # Observation as FiLM conditioning\n",
    "                    obs_cond = nobs[:, :obs_horizon, :].flatten(start_dim=1)\n",
    "\n",
    "                    # Sample noise\n",
    "                    noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "                    # Sample diffusion iteration\n",
    "                    timesteps = torch.randint(\n",
    "                        0, noise_scheduler.config.num_train_timesteps,\n",
    "                        (B,), device=device\n",
    "                    ).long()\n",
    "\n",
    "                    # Add noise to actions\n",
    "                    noisy_actions = noise_scheduler.add_noise(\n",
    "                        naction, noise, timesteps)\n",
    "\n",
    "                    # Predict noise residual\n",
    "                    noise_pred = noise_pred_net(\n",
    "                        noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "                    # L2 loss\n",
    "                    loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "                    # Logging\n",
    "                    loss_cpu = loss.item()\n",
    "                    val_loss_transfer.append(loss_cpu)\n",
    "                    vepoch.set_postfix(loss=loss_cpu)\n",
    "\n",
    "        # Logging\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_losses_transfer.append(val_loss_transfer)\n",
    "        tglobal.set_postfix(\n",
    "            train_loss=np.mean(train_loss),\n",
    "            val_loss=np.mean(val_loss),\n",
    "            val_loss_transfer=np.mean(val_loss_transfer)\n",
    "        )\n",
    "\n",
    "# Weights of the EMA model\n",
    "# is used for inference\n",
    "ema_noise_pred_net = noise_pred_net\n",
    "ema.copy_to(ema_noise_pred_net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': ema_noise_pred_net.state_dict(),\n",
    "    'ema_model_state_dict': ema.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "    'epoch': epoch_idx,\n",
    "    \"stats\": stats,\n",
    "    'loss': loss, # Save the current epoch\n",
    "}, model_path_transfer)\n",
    "\n",
    "# Save the training losses\n",
    "np.savez(loss_path, train_losses=train_losses, val_losses=val_losses, val_losses_transfer=val_losses_transfer)  # Multiple arrays in one file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from NPZ file\n",
    "with np.load(loss_path) as data:\n",
    "    train_losses = data['train_losses']\n",
    "    val_losses = data['val_losses']\n",
    "    val_losses_transfer = data['val_losses_transfer']\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = train_losses.shape[0]\n",
    "\n",
    "# Calculate x-axis positions for each step, based on epoch boundaries\n",
    "x_positions_train = []\n",
    "x_positions_val = []\n",
    "x_positions_val_transfer = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = epoch \n",
    "    epoch_end = epoch + 1\n",
    "\n",
    "    # Training steps within the epoch\n",
    "    num_steps_train = len(train_losses[epoch])\n",
    "    step_positions_train = np.linspace(epoch_start, epoch_end, num_steps_train, endpoint=False)\n",
    "    x_positions_train.extend(step_positions_train)\n",
    "\n",
    "    # Validation steps within the epoch (if available)\n",
    "    if epoch < len(val_losses):  \n",
    "        num_steps_val = len(val_losses[epoch])\n",
    "        step_positions_val = np.linspace(epoch_start, epoch_end, num_steps_val, endpoint=False)\n",
    "        x_positions_val.extend(step_positions_val)\n",
    "    \n",
    "    # Validation steps within the epoch (if available)\n",
    "    if epoch < len(val_losses_transfer):  \n",
    "        num_steps_val = len(val_losses_transfer[epoch])\n",
    "        step_positions_val = np.linspace(epoch_start, epoch_end, num_steps_val, endpoint=False)\n",
    "        x_positions_val_transfer.extend(step_positions_val)\n",
    "\n",
    "x_positions_train = np.array(x_positions_train)\n",
    "x_positions_val = np.array(x_positions_val)\n",
    "x_positions_val_transfer = np.array(x_positions_val_transfer)\n",
    "\n",
    "# Smoothing for better visualization (adjust window size as needed)\n",
    "def smooth_curve(data, x_positions, window_size=10):\n",
    "    smoothed_data = np.convolve(data, np.ones(window_size) / window_size, mode='valid')\n",
    "    valid_x_positions = x_positions[window_size // 2:-window_size // 2 + 1]\n",
    "    if len(smoothed_data) > len(valid_x_positions):  # handle case when data is shorter than window\n",
    "        smoothed_data = smoothed_data[:len(valid_x_positions)]\n",
    "    return valid_x_positions, smoothed_data  \n",
    "\n",
    "x_positions_train, smoothed_training_losses = smooth_curve(train_losses.flatten(), x_positions_train)\n",
    "x_positions_val, smoothed_validation_losses = smooth_curve(val_losses.flatten(), x_positions_val)\n",
    "x_positions_val_transfer, smoothed_validation_losses_transfer = smooth_curve(val_losses_transfer.flatten(), x_positions_val_transfer)\n",
    "\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(12, 6))  # Adjust figure size\n",
    "plt.plot(x_positions_train, smoothed_training_losses, label='Training Loss', color='black', linestyle='--')\n",
    "plt.plot(x_positions_val, smoothed_validation_losses, label='Task Val Loss', color='red', linestyle='-')\n",
    "plt.plot(x_positions_val_transfer, smoothed_validation_losses_transfer, label='Transfer Task Val Loss ', color='blue', linestyle='-')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.yscale('log')\n",
    "plt.title(f'Training and Validation Loss per Epoch for {env_id_transfer} on {env_id} model', fontsize=14)\n",
    "plt.xticks(range(num_epochs+1))  # Set x ticks at integer epoch values\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--')  # Grid only on y-axis for better readability\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path)  # Save the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_id_transfer, obs_mode=obs_mode, control_mode=control_mode, render_mode='rgb_array')\n",
    "\n",
    "max_steps = 400\n",
    "\n",
    "helper_techniques = False\n",
    "\n",
    "num_episodes = 50\n",
    "mean_success = 0 \n",
    "mean_reward = 0\n",
    "rewards = []\n",
    "csv_file = f\"{results_path}/results{information}.csv\"\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Episode', 'Max Reward', 'Success'])\n",
    "    print(f\"Opened file {csv_file} for writing.\")\n",
    "\n",
    "    with tqdm(range(num_episodes), desc='Epoch') as episodes:\n",
    "\n",
    "        for episode in episodes:\n",
    "            \n",
    "            # reset \n",
    "            obs, info = env.reset()\n",
    "            obs = get_observations(obs)\n",
    "            obs = convert_observation(obs, task_id[env_id_transfer])\n",
    "\n",
    "            # save observations\n",
    "            obs_deque = collections.deque([obs] * obs_horizon, maxlen=obs_horizon)\n",
    "            actions_deque = collections.deque([[0,0,0,0,0,0,1]] * action_horizon, maxlen=action_horizon)\n",
    "\n",
    "            obs_seq = np.stack(obs_deque)  \n",
    "            print(\"obs unnormalized\", obs_seq)\n",
    "            nobs = normalize_batch({'obs': torch.tensor(obs_seq, dtype=torch.float32)}, stats)\n",
    "            nobs = nobs['obs']\n",
    "            print(\"obs normalized\",nobs)\n",
    "            print(\"stats\",stats[\"obs\"][\"max\"])\n",
    "            # save visualization\n",
    "            imgs = []\n",
    "            rewards = []\n",
    "            done = False\n",
    "            step_idx = 0\n",
    "            unsuccessful = False\n",
    "\n",
    "\n",
    "            with tqdm(total=max_steps, desc=\"Eval\", leave=False) as pbar:\n",
    "                while not done:\n",
    "                    B = 1\n",
    "                    # stack the last obs_horizon (2) number of observations\n",
    "                    obs_seq = np.stack(obs_deque)\n",
    "                    if env_id == \"PickCube-v1\" and helper_techniques:\n",
    "                      obs_seq[:, :,27] += 0.01\n",
    "                    nobs = normalize_batch({'obs': torch.tensor(obs_seq, dtype=torch.float32)}, stats)\n",
    "                    nobs = nobs['obs']\n",
    "                    \n",
    "                    # device transfer\n",
    "                    #nobs = torch.from_numpy(nobs).to(device, dtype=torch.float32)\n",
    "                    nobs= nobs.to(device)\n",
    "\n",
    "                    # infer action\n",
    "                    with torch.no_grad():\n",
    "                        # reshape observation to (B,obs_horizon*obs_dim)\n",
    "                        obs_cond = nobs.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "                        # initialize action from Guassian noise\n",
    "                        noisy_action = torch.randn(\n",
    "                            (B, pred_horizon, action_dim), device=device)\n",
    "                        naction = noisy_action\n",
    "\n",
    "                        # init scheduler\n",
    "                        noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "\n",
    "                        for k in noise_scheduler.timesteps:\n",
    "                            # predict noise\n",
    "                            noise_pred = ema_noise_pred_net(\n",
    "                                sample=naction,\n",
    "                                timestep=k,\n",
    "                                global_cond=obs_cond\n",
    "                            )\n",
    "\n",
    "                            # inverse diffusion step (remove noise)\n",
    "                            naction = noise_scheduler.step(\n",
    "                                model_output=noise_pred,\n",
    "                                timestep=k,\n",
    "                                sample=naction\n",
    "                            ).prev_sample\n",
    "\n",
    "                    # unnormalize action\n",
    "                    naction = naction.detach().to('cpu').numpy()\n",
    "                    # (B, pred_horizon, action_dim)\n",
    "                    action_pred = naction[0] # we dont have to denormalize the action\n",
    "\n",
    "                    # only take action_horizon number of actions\n",
    "                    start = obs_horizon - 1\n",
    "                    end = start + action_horizon\n",
    "                    action = action_pred[start:end,:]\n",
    "\n",
    "                    for i in range(len(action)):\n",
    "                        action[i] = np.clip(action[i], -1, 1)\n",
    "                        actions_deque.append(action[i])\n",
    "\n",
    "                    # execute action_horizon number of steps\n",
    "                    # without replanning\n",
    "                    for i in range(len(action)):\n",
    "               \n",
    "                        # only allow gripper action to be same for action_horizon number of steps\n",
    "                        modified_action = action[i]\n",
    "                        \n",
    "                        #if env_id == \"PickCube-v1\":\n",
    "\n",
    "                          #if len(rewards) > 0 and rewards[-1] > 0.4: #Threshold for almost there\n",
    "                          #  modified_action[-1] = -1\n",
    "\n",
    "                        if env_id == \"StackCube-v1\" and helper_techniques:\n",
    "                          same_gripper_action = 0\n",
    "                          last_x_action = 5\n",
    "                          start_idx = len(actions_deque) - last_x_action\n",
    "                          last_x_actions = list(actions_deque)[start_idx:]\n",
    "        \n",
    "                          for i in last_x_actions:\n",
    "                              same_gripper_action += i[-1]\n",
    "                      \n",
    "                          if same_gripper_action >= 0:\n",
    "                            modified_action[-1] = 1# change the gripper action to opposite\n",
    "                          else :\n",
    "                            modified_action[-1] = -1\n",
    "                        \n",
    "                        \n",
    "                        obs, reward, done, _, info = env.step(modified_action)\n",
    "\n",
    "                        # process observation\n",
    "                        # From the observation dictionary, we concatenate all the observations\n",
    "                        # as done in the training data\n",
    "                        obs = get_observations(obs)\n",
    "                        obs = convert_observation(obs, task_id[env_id_transfer])\n",
    "\n",
    "                        # save observations\n",
    "                        obs_deque.append(obs)\n",
    "\n",
    "                        # and reward/vis\n",
    "                        rewards.append(reward)\n",
    "                        imgs.append(env.render())\n",
    "\n",
    "                        # update progress bar\n",
    "                        step_idx += 1\n",
    "                        pbar.update(1)\n",
    "                        pbar.set_postfix(reward=reward)\n",
    "                        if step_idx > max_steps:\n",
    "                          \n",
    "                            done = True\n",
    "                            unsuccessful = True\n",
    "                        if done:\n",
    "                            break\n",
    "            \n",
    "            if not unsuccessful:\n",
    "                mean_success += 1\n",
    "            mean_reward += max(rewards)\n",
    "            writer.writerow([episode + 1, max(rewards), int(not unsuccessful)])\n",
    "            episodes.set_postfix(\n",
    "                reward=mean_reward / (episode + 1),\n",
    "                success=mean_success / (episode + 1)\n",
    "            )\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    print(\"Reward: \", mean_reward / num_episodes)\n",
    "    print(\"Success: \", mean_success/num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [Image.fromarray(img.squeeze(0).cpu().numpy()) for img in imgs]\n",
    "\n",
    "# Save to a bytes buffer\n",
    "buffer = io.BytesIO()\n",
    "images[0].save(buffer, format='GIF', save_all=True, append_images=images[1:], optimize=False, duration=50, loop=0)\n",
    "buffer.seek(0)\n",
    "\n",
    "# Save to a file\n",
    "with open(animation_path, 'wb') as f:\n",
    "    f.write(buffer.getvalue())\n",
    "\n",
    "# Display the GIF (optional)\n",
    "display(IPImage(data=buffer.getvalue()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
