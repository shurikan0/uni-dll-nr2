{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the package\n",
    "%pip install --upgrade mani_skill\n",
    "# install a version of torch that is compatible with your system\n",
    "%pip install torch torchvision torchaudio numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import mani_skill.envs.utils.randomization as randomization\n",
    "from mani_skill.agents.robots import Fetch, Panda, Xmate3Robotiq\n",
    "from mani_skill.envs.sapien_env import BaseEnv\n",
    "from mani_skill.sensors.camera import CameraConfig\n",
    "from mani_skill.utils import sapien_utils\n",
    "from mani_skill.utils.building import actors\n",
    "from mani_skill.utils.registration import register_env\n",
    "from mani_skill.utils.scene_builder.table import TableSceneBuilder\n",
    "from mani_skill.utils.structs.pose import Pose\n",
    "from mani_skill.utils.structs.types import SimConfig\n",
    "\n",
    "\n",
    "@register_env(\"PlaceCube-v1\", max_episode_steps=50)\n",
    "class PlaceCubeEnv(BaseEnv):\n",
    "    SUPPORTED_ROBOTS = [\"panda\", \"xmate3_robotiq\", \"fetch\"]\n",
    "    agent: Union[Panda, Xmate3Robotiq, Fetch]\n",
    "    cube_half_size = 0.02\n",
    "    goal_thresh = 0.025\n",
    "\n",
    "    def __init__(self, *args, robot_uids=\"panda\", robot_init_qpos_noise=0.02, **kwargs):\n",
    "        self.robot_init_qpos_noise = robot_init_qpos_noise\n",
    "        super().__init__(*args, robot_uids=robot_uids, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _default_sensor_configs(self):\n",
    "        pose = sapien_utils.look_at(eye=[0.3, 0, 0.6], target=[-0.1, 0, 0.1])\n",
    "        return [CameraConfig(\"base_camera\", pose, 128, 128, np.pi / 2, 0.01, 100)]\n",
    "\n",
    "    @property\n",
    "    def _default_human_render_camera_configs(self):\n",
    "        pose = sapien_utils.look_at([0.6, 0.7, 0.6], [0.0, 0.0, 0.35])\n",
    "        return CameraConfig(\"render_camera\", pose, 512, 512, 1, 0.01, 100)\n",
    "\n",
    "    def _load_scene(self, options: dict):\n",
    "        self.table_scene = TableSceneBuilder(\n",
    "            self, robot_init_qpos_noise=self.robot_init_qpos_noise\n",
    "        )\n",
    "        self.table_scene.build()\n",
    "        self.cube = actors.build_cube(\n",
    "            self.scene, half_size=self.cube_half_size, color=[1, 0, 0, 1], name=\"cube\"\n",
    "        )\n",
    "        self.goal_site = actors.build_sphere(\n",
    "            self.scene,\n",
    "            radius=self.goal_thresh,\n",
    "            color=[0, 1, 0, 1],\n",
    "            name=\"goal_site\",\n",
    "            body_type=\"kinematic\",\n",
    "            add_collision=False,\n",
    "        )\n",
    "        self._hidden_objects.append(self.goal_site)\n",
    "\n",
    "    def _initialize_episode(self, env_idx: torch.Tensor, options: dict):\n",
    "        with torch.device(self.device):\n",
    "            b = len(env_idx)\n",
    "            self.table_scene.initialize(env_idx)\n",
    "\n",
    "            # Initial cube pose: Randomly place the cube within the gripper's grasp\n",
    "            cube_positions = self.agent.get_grasp_pose(self.cube)[0].p\n",
    "            cube_orientations = randomization.random_quaternions(b, lock_x=True, lock_y=True)\n",
    "            self.cube.set_pose(Pose.create_from_pq(cube_positions, cube_orientations))\n",
    "\n",
    "            # Initial robot state: Grasping the cube\n",
    "            self.agent.close_gripper(self.cube)\n",
    "\n",
    "            # Goal site pose: Randomly place on the table\n",
    "            goal_xyz = torch.zeros((b, 3))\n",
    "            goal_xyz[:, :2] = torch.rand((b, 2)) * 0.2 - 0.1\n",
    "            goal_xyz[:, 2] = self.cube_half_size  # Place goal on the table surface\n",
    "            self.goal_site.set_pose(Pose.create_from_pq(goal_xyz))\n",
    "\n",
    "    def _get_obs_extra(self, info: Dict):\n",
    "        # in reality some people hack is_grasped into observations by checking if the gripper can close fully or not\n",
    "        obs = dict(\n",
    "            is_grasped=info[\"is_grasped\"],\n",
    "            tcp_pose=self.agent.tcp.pose.raw_pose,\n",
    "            goal_pos=self.goal_site.pose.p,\n",
    "        )\n",
    "        if \"state\" in self.obs_mode:\n",
    "            obs.update(\n",
    "                obj_pose=self.cube.pose.raw_pose,\n",
    "                tcp_to_obj_pos=self.cube.pose.p - self.agent.tcp.pose.p,\n",
    "                obj_to_goal_pos=self.goal_site.pose.p - self.cube.pose.p,\n",
    "            )\n",
    "        return obs\n",
    "\n",
    "    def evaluate(self):\n",
    "        is_obj_placed = (\n",
    "            torch.linalg.norm(self.goal_site.pose.p - self.cube.pose.p, axis=1)\n",
    "            <= self.goal_thresh\n",
    "        )\n",
    "        is_grasped = self.agent.is_grasping(self.cube)\n",
    "        is_robot_static = self.agent.is_static(0.2)\n",
    "        return {\n",
    "            \"success\": is_obj_placed & is_robot_static,\n",
    "            \"is_obj_placed\": is_obj_placed,\n",
    "            \"is_robot_static\": is_robot_static,\n",
    "            \"is_grasped\": is_grasped,\n",
    "        }\n",
    "\n",
    "    def compute_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):\n",
    "        tcp_to_obj_dist = torch.linalg.norm(\n",
    "            self.cube.pose.p - self.agent.tcp.pose.p, axis=1\n",
    "        )\n",
    "        reaching_reward = 1 - torch.tanh(5 * tcp_to_obj_dist)\n",
    "        reward = reaching_reward\n",
    "\n",
    "        is_grasped = info[\"is_grasped\"]\n",
    "        reward += is_grasped\n",
    "\n",
    "        obj_to_goal_dist = torch.linalg.norm(\n",
    "            self.goal_site.pose.p - self.cube.pose.p, axis=1\n",
    "        )\n",
    "        place_reward = 1 - torch.tanh(5 * obj_to_goal_dist)\n",
    "        reward += place_reward * is_grasped\n",
    "\n",
    "        static_reward = 1 - torch.tanh(\n",
    "            5 * torch.linalg.norm(self.agent.robot.get_qvel()[..., :-2], axis=1)\n",
    "        )\n",
    "        reward += static_reward * info[\"is_obj_placed\"]\n",
    "\n",
    "        reward[info[\"success\"]] = 5\n",
    "        return reward\n",
    "\n",
    "    def compute_normalized_dense_reward(\n",
    "        self, obs: Any, action: torch.Tensor, info: Dict\n",
    "    ):\n",
    "        return self.compute_dense_reward(obs=obs, action=action, info=info) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sapien\n",
    "\n",
    "from .custom_envs.place_cube import PlaceCubeEnv\n",
    "from mani_skill.examples.motionplanning.panda.motionplanner import \\\n",
    "    PandaArmMotionPlanningSolver\n",
    "from mani_skill.examples.motionplanning.panda.utils import (\n",
    "    compute_grasp_info_by_obb, get_actor_obb)\n",
    "\n",
    "def solve(env: PlaceCubeEnv, seed=None, debug=False, vis=False):\n",
    "    env.reset(seed=seed)\n",
    "    planner = PandaArmMotionPlanningSolver(\n",
    "        env,\n",
    "        debug=debug,\n",
    "        vis=vis,\n",
    "        base_pose=env.unwrapped.agent.robot.pose,\n",
    "        visualize_target_grasp_pose=vis,\n",
    "        print_env_info=False,\n",
    "    )\n",
    "\n",
    "    FINGER_LENGTH = 0.025\n",
    "    env = env.unwrapped\n",
    "    \n",
    "    # Object manipulation adjustments\n",
    "    obb = get_actor_obb(env.cube) \n",
    "    goal_pose = env.goal_site.pose \n",
    "    initial_offset = 0.05 # Move up 5cm from the current pose for a better approach\n",
    "\n",
    "    # Get the current grasp pose (since the robot is already holding the cube)\n",
    "    grasp_pose = env.agent.get_grasp_pose(env.cube)[0]  # Assuming a single grasp pose\n",
    "\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # Reach\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # Initial offset adjustment\n",
    "    reach_pose = grasp_pose * sapien.Pose([0, 0, initial_offset])  \n",
    "    planner.move_to_pose_with_screw(reach_pose)\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # Move to goal pose\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    res = planner.move_to_pose_with_screw(goal_pose) \n",
    "\n",
    "    # Release the cube\n",
    "    planner.open_gripper()\n",
    "\n",
    "    planner.close()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os.path as osp\n",
    "from mani_skill.utils.wrappers.record import RecordEpisode\n",
    "from mani_skill.examples.motionplanning.panda.solutions import solvePushCube, solvePickCube, solveStackCube, solvePegInsertionSide, solvePlugCharger\n",
    "\n",
    "\n",
    "MP_SOLUTIONS = {\n",
    "    \"PickCube-v1\": solvePickCube,\n",
    "    \"StackCube-v1\": solveStackCube,\n",
    "    \"PegInsertionSide-v1\": solvePegInsertionSide,\n",
    "    \"PlugCharger-v1\": solvePlugCharger,\n",
    "    \"PushCube-v1\": solvePushCube,\n",
    "    \"PlaceCube-v1\": solve,\n",
    "}\n",
    "#def parse_args(args=None):\n",
    "#    parser = argparse.ArgumentParser()\n",
    "#    parser.add_argument(\"-e\", \"--env-id\", type=str, default=\"PlaceCube-v1\", help=f\"Environment to run motion planning solver on. Available options are {list(MP_SOLUTIONS.keys())}\")\n",
    "#    parser.add_argument(\"-o\", \"--obs-mode\", type=str, default=\"none\", help=\"Observation mode to use. Usually this is kept as 'none' as observations are not necesary to be stored, they can be replayed later via the mani_skill.trajectory.replay_trajectory script.\")\n",
    "#    parser.add_argument(\"-n\", \"--num-traj\", type=int, default=10, help=\"Number of trajectories to generate.\")\n",
    "#    parser.add_argument(\"--only-count-success\", action=\"store_true\", help=\"If true, generates trajectories until num_traj of them are successful and only saves the successful trajectories/videos\")\n",
    "#    parser.add_argument(\"--reward-mode\", type=str)\n",
    "#    parser.add_argument(\"-b\", \"--sim-backend\", type=str, default=\"auto\", help=\"Which simulation backend to use. Can be 'auto', 'cpu', 'gpu'\")\n",
    "#    parser.add_argument(\"--render-mode\", type=str, default=\"rgb_array\", help=\"can be 'sensors' or 'rgb_array' which only affect what is saved to videos\")\n",
    "#    parser.add_argument(\"--vis\", action=\"store_true\", help=\"whether or not to open a GUI to visualize the solution live\")\n",
    "#    parser.add_argument(\"--save-video\", action=\"store_true\", help=\"whether or not to save videos locally\")\n",
    "#    parser.add_argument(\"--traj-name\", type=str, help=\"The name of the trajectory .h5 file that will be created.\")\n",
    "#    parser.add_argument(\"--shader\", default=\"default\", type=str, help=\"Change shader used for rendering. Default is 'default' which is very fast. Can also be 'rt' for ray tracing and generating photo-realistic renders. Can also be 'rt-fast' for a faster but lower quality ray-traced renderer\")\n",
    "#    parser.add_argument(\"--record-dir\", type=str, default=\"demos\", help=\"where to save the recorded trajectories\")\n",
    "#    return parser.parse_args()\n",
    "\n",
    "def generate(num_traj, env_id, file_path, save_video):\n",
    "    env = gym.make(\n",
    "        env_id,\n",
    "        obs_mode=\"none\",\n",
    "        control_mode=\"pd_joint_pos\",\n",
    "        render_mode=\"rgb_array\",\n",
    "        reward_mode=\"dense\",\n",
    "        shader_dir=\"default\",\n",
    "        sim_backend=\"auto\"\n",
    "    )\n",
    "    if env_id not in MP_SOLUTIONS:\n",
    "        raise RuntimeError(f\"No already written motion planning solutions for {env_id}. Available options are {list(MP_SOLUTIONS.keys())}\")\n",
    "    env = RecordEpisode(\n",
    "        env,\n",
    "        output_dir=osp.join(file_path, env_id),\n",
    "        trajectory_name=env_id, save_video=False,\n",
    "        source_type=\"motionplanning\",\n",
    "        source_desc=\"official motion planning solution from ManiSkill contributors\",\n",
    "        video_fps=30,\n",
    "        save_on_reset=False\n",
    "    )\n",
    "    solve = MP_SOLUTIONS[env_id]\n",
    "    print(f\"Motion Planning Running on {env_id}\")\n",
    "    pbar = tqdm(range(num_traj))\n",
    "    seed = 0\n",
    "    successes = []\n",
    "    passed = 0\n",
    "    while True:\n",
    "        res = solve(env, seed=seed, debug=False, vis=False)\n",
    "        if res == -1:\n",
    "            success = False\n",
    "        else:\n",
    "            success = res[-1][\"success\"].item()\n",
    "        successes.append(success)\n",
    "        if not success:\n",
    "            seed += 1\n",
    "            env.flush_trajectory(save=False)\n",
    "            if save_video:\n",
    "                env.flush_video(save=False)\n",
    "            continue\n",
    "        else:\n",
    "            env.flush_trajectory()\n",
    "            if save_video:\n",
    "                env.flush_video()\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(dict(success_rate=np.mean(successes)))\n",
    "            seed += 1\n",
    "            passed += 1\n",
    "            if passed == num_traj:\n",
    "                break\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_traj = 10\n",
    "env_id = \"PlaceCube-v1\"\n",
    "file_path = \"/content/drive/MyDrive/Data/Training/Generated/\"\n",
    "save_video = False\n",
    "\n",
    "generate(num_traj, env_id, file_path, save_video)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
