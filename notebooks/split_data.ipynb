{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etc imports\n",
    "from typing import Tuple, Sequence, Dict, Union, Optional\n",
    "from collections import OrderedDict\n",
    "import collections\n",
    "import math\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display, Image as IPImage\n",
    "import io\n",
    "import os\n",
    "\n",
    "# google colab imports\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "obs_mode = 'state_dict'\n",
    "control_mode = 'pd_joint_delta_pos'\n",
    "\n",
    "#env_ids = ['PickCube-v1', 'StackCube-v1', 'PegInsertionSide-v1', 'PlugCharger-v1', 'PushCube-v1']\n",
    "env_ids = ['StackCube-v1']\n",
    "base_path = '/content/drive/MyDrive/Data'\n",
    "\n",
    "for env_id in env_ids:\n",
    "    # File names\n",
    "    generated_path = f'{base_path}/Generated/{env_id}/motionplanning'\n",
    "    original_h5_file = f'{generated_path}/traj3000.{obs_mode}.{control_mode}.h5'\n",
    "    original_json_file = f'{generated_path}/traj3000.{obs_mode}.{control_mode}.json'\n",
    "    training_h5_file = f'{generated_path}/training.{obs_mode}.{control_mode}.h5'\n",
    "    validation_h5_file = f'{generated_path}/validation.{obs_mode}.{control_mode}.h5'\n",
    "    training_json_file = f'{generated_path}/training.{obs_mode}.{control_mode}.json'\n",
    "    validation_json_file = f'{generated_path}/validation.{obs_mode}.{control_mode}.json'\n",
    "\n",
    "    with open(original_json_file, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    with h5py.File(original_h5_file, 'r') as f:\n",
    "        traj_datasets = [key for key in f.keys() if key.startswith('traj_')]\n",
    "\n",
    "        np.random.shuffle(traj_datasets)\n",
    "\n",
    "        split_point = int(0.9 * len(traj_datasets))\n",
    "\n",
    "        training_datasets = traj_datasets[:split_point]\n",
    "        validation_datasets = traj_datasets[split_point:]\n",
    "\n",
    "        with h5py.File(training_h5_file, 'w') as f_train:\n",
    "            for key in training_datasets:\n",
    "                f.copy(key, f_train)\n",
    "\n",
    "        with h5py.File(validation_h5_file, 'w') as f_val:\n",
    "            for key in validation_datasets:\n",
    "                f.copy(key, f_val)\n",
    "\n",
    "    dataset_indices = {f'traj_{i}': i for i in range(len(json_data['episodes']))}\n",
    "\n",
    "    training_episodes = [json_data['episodes'][dataset_indices[key]] for key in training_datasets]\n",
    "    validation_episodes = [json_data['episodes'][dataset_indices[key]] for key in validation_datasets]\n",
    "\n",
    "    training_json = {\n",
    "        'env_info': json_data['env_info'],\n",
    "        'episodes': training_episodes\n",
    "    }\n",
    "\n",
    "    validation_json = {\n",
    "        'env_info': json_data['env_info'],\n",
    "        'episodes': validation_episodes\n",
    "    }\n",
    "\n",
    "    with open(training_json_file, 'w') as f:\n",
    "        json.dump(training_json, f, indent=4)\n",
    "\n",
    "    with open(validation_json_file, 'w') as f:\n",
    "        json.dump(validation_json, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
