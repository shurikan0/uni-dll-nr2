{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pip Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the package\n",
    "%pip install --upgrade mani_skill\n",
    "# install a version of torch that is compatible with your system\n",
    "%pip install torch torchvision torchaudio numpy diffusers\n",
    "\n",
    "import math\n",
    "from typing import Union\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from mani_skill.utils import common\n",
    "from mani_skill.utils.io_utils import load_json\n",
    "from mani_skill.utils.common import flatten_state_dict\n",
    "import mani_skill.envs\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple, Sequence, Dict, Union, Optional\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# env import\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_data(data):\n",
    "    out = dict()\n",
    "    for k in data.keys():\n",
    "        if isinstance(data[k], h5py.Dataset):\n",
    "            out[k] = data[k][:]\n",
    "        else:\n",
    "            out[k] = load_h5_data(data[k])\n",
    "    return out\n",
    "\n",
    "\n",
    "def create_sample_indices(episode_ends: np.ndarray, sequence_length: int, pad_before: int = 0, pad_after: int = 0):\n",
    "    # Currently uses truncated as episode ends which is the end of the episode and not the end of the trajectory\n",
    "    indices = list()\n",
    "    episode_length = 0\n",
    "    episode_index = 1 # Start 1 for human readability\n",
    "    for i in range(len(episode_ends)):\n",
    "        episode_length += 1\n",
    "        if episode_ends[i]:\n",
    "            start_idx = 0 if i <= 0 else i - episode_length + 1\n",
    "            min_start = -pad_before\n",
    "            max_start = episode_length - sequence_length + pad_after\n",
    "\n",
    "            # Create indices for each possible sequence in the episode\n",
    "            for idx in range(min_start, max_start + 1):\n",
    "                buffer_start_idx = max(idx, 0) + start_idx\n",
    "                buffer_end_idx = min(idx + sequence_length, episode_length) + start_idx\n",
    "                start_offset = buffer_start_idx - (idx + start_idx)\n",
    "                end_offset = (idx + sequence_length + start_idx) - buffer_end_idx\n",
    "                sample_start_idx = 0 + start_offset\n",
    "                sample_end_idx = sequence_length - end_offset\n",
    "                indices.append([buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx])\n",
    "            episode_length = 0\n",
    "            episode_index += 1\n",
    "    return np.array(indices)\n",
    "\n",
    "\n",
    "def sample_sequence(train_data, sequence_length, buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx):\n",
    "    result = dict()\n",
    "    for key, input_arr in train_data.items():\n",
    "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
    "        data = sample\n",
    "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
    "            if isinstance(input_arr, torch.Tensor):\n",
    "                data = torch.zeros((sequence_length,) + input_arr.shape[1:], dtype=input_arr.dtype)\n",
    "            else:\n",
    "                data = np.zeros(shape=(sequence_length,) + input_arr.shape[1:], dtype=input_arr.dtype)\n",
    "            if sample_start_idx > 0:\n",
    "                data[:sample_start_idx] = sample[0]\n",
    "            if sample_end_idx < sequence_length:\n",
    "                data[sample_end_idx:] = sample[-1]\n",
    "            data[sample_start_idx:sample_end_idx] = sample\n",
    "        result[key] = data\n",
    "    return result\n",
    "\n",
    "def remove_np_uint16(x: Union[np.ndarray, dict]):\n",
    "            if isinstance(x, dict):\n",
    "                for k in x.keys():\n",
    "                    x[k] = remove_np_uint16(x[k])\n",
    "                return x\n",
    "            else:\n",
    "                if x.dtype == np.uint16:\n",
    "                    return x.astype(np.int32)\n",
    "                return x\n",
    "\n",
    "def convert_observation(obs, task_id):\n",
    "    # adds task_id to the observation\n",
    "    values = list(obs.values())\n",
    "    example = values[0]\n",
    "    if isinstance(example, torch.Tensor):\n",
    "          example = example.numpy()\n",
    "\n",
    "    # add task_id to the observation\n",
    "    task_id_array = np.full((example.shape[0], 1), task_id, dtype=example.dtype) \n",
    "    values.append(task_id_array)\n",
    "    # concatenate all the values\n",
    "    return np.concatenate(values, axis=-1)\n",
    "\n",
    "def get_observations(obs):\n",
    "    #ensoure that the observations are in the correct format\n",
    "    #and ordered correctly across tasks\n",
    "\n",
    "    cleaned_obs = OrderedDict()\n",
    "    cleaned_obs[\"qpos\"] = obs[\"agent\"][\"qpos\"]\n",
    "    cleaned_obs[\"qvel\"] = obs[\"agent\"][\"qvel\"]\n",
    "    cleaned_obs[\"tcp_pose\"] = obs[\"extra\"][\"tcp_pose\"]\n",
    "    obs[\"extra\"].pop(\"tcp_pose\")\n",
    "\n",
    "    #this code is not generic and only works for the specific observation spaces we have\n",
    "    # Handle different goal position formats gracefully\n",
    "    goal_pose_keys = [\"goal_pose\", \"goal_pos\", \"box_hole_pos\", \"cubeB_pose\"]\n",
    "    for key in goal_pose_keys:\n",
    "        if key in obs[\"extra\"]:\n",
    "            pos = obs[\"extra\"][key]\n",
    "\n",
    "            # Ensure 'pos' is 2D with the correct number of columns\n",
    "            if pos.ndim == 1:\n",
    "                pos = pos.reshape(1, -1)  # Reshape to 2D if necessary\n",
    "            elif pos.ndim > 2:\n",
    "                raise ValueError(f\"Unexpected dimensions for '{key}': {pos.shape}\")\n",
    "\n",
    "            # Pad or truncate 'pos' to have 7 columns\n",
    "            pos = np.pad(pos[:, :7], ((0, 0), (0, 7 - pos.shape[1])), mode='constant')\n",
    "            if isinstance(cleaned_obs[\"tcp_pose\"], torch.Tensor):\n",
    "                pos = torch.tensor(pos, dtype=cleaned_obs[\"tcp_pose\"].dtype)\n",
    "                \n",
    "            cleaned_obs[\"goal_pose\"] = pos\n",
    "            obs[\"extra\"].pop(key)\n",
    "            break  # Stop once a valid goal pose key is found\n",
    "    else:\n",
    "        print(\"No goal pose found. Setting to zero.\")\n",
    "        length = len(obs[\"extra\"][\"tcp_pose\"])\n",
    "        cleaned_obs[\"goal_pose\"] = np.zeros((length, 7), dtype=np.float32)  # Ensure 2D shape\n",
    "        \n",
    "    #is_grasped_reshaped = np.reshape(obs[\"extra\"][\"is_grasped\"], (len(obs[\"extra\"][\"is_grasped\"]), 1))\n",
    "    \n",
    "    # Filter and add other observations with 7 columns\n",
    "    for key, value in obs[\"extra\"].items():\n",
    "        if value.shape[-1] == 7 and value.ndim == 2:\n",
    "            cleaned_obs[key] = value\n",
    "    \n",
    "    return cleaned_obs\n",
    "\n",
    "def get_data_stats(data, obs_mask: bool = False):\n",
    "    data = data.reshape(-1,data.shape[-1])\n",
    "\n",
    "    # Create a mask to exclude the specified values from normalization\n",
    "    mask = np.ones_like(data[0], dtype=bool)\n",
    "    if obs_mask:\n",
    "        # This values are the goal pose values\n",
    "        # + the task id which is the last value\n",
    "        mask[25] = False # x\n",
    "        mask[26] = False # y\n",
    "        mask[27] = False # z\n",
    "        mask[28] = False # qx\n",
    "        mask[29] = False # qy\n",
    "        mask[30] = False # qz\n",
    "        mask[31] = False # qw\n",
    "        mask[39] = False # task id\n",
    "    \n",
    "    # Filter data to exclude specified values\n",
    "    mask = np.repeat(mask[np.newaxis, :], data.shape[0], axis=0)\n",
    "    mask = np.where(mask, 0, 1)\n",
    "    masked_data = np.ma.masked_array(data, mask) # Apply mask to data\n",
    "\n",
    "    stats = {\n",
    "        'min': np.min(masked_data.data, axis=0),\n",
    "        'max': np.max(masked_data.data, axis=0),\n",
    "        'mask': mask,\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "def set_last_dimension(array, mask):\n",
    "    \"\"\"Sets the last dimension of an array to new_data, matching the original shape.\n",
    "\n",
    "    Args:\n",
    "        original_array: The original NumPy array.\n",
    "        new_data: A 1D array containing the new data for the last dimension.\n",
    "\n",
    "    Returns:\n",
    "        A new NumPy array with the same shape as original_array, \n",
    "        but with the last dimension set to new_data.\n",
    "    \"\"\"\n",
    "    new_shape = array.shape[:-1] + (len(mask),)\n",
    "    new_data_expanded = np.expand_dims(mask, axis=0)  \n",
    "    return np.broadcast_to(new_data_expanded, new_shape)\n",
    "\n",
    "\n",
    "def normalize_data(data, stats):\n",
    "    # Calculate the denominator for normalization\n",
    "    # Avoid division by zero\n",
    "    denominator = stats['max'] - stats['min']\n",
    "    denominator[denominator == 0] = 1\n",
    "\n",
    "    # nomalize to [0,1]\n",
    "    ndata = (data - stats['min']) / denominator\n",
    "\n",
    "    # normalize to [-1, 1]\n",
    "    ndata = ndata * 2 - 1\n",
    "\n",
    "    # Set masked values to original values\n",
    "    mask = set_last_dimension(data, stats['mask'][0])\n",
    "    ndata = np.where(mask, data, ndata)\n",
    "\n",
    "    return ndata\n",
    "\n",
    "def unnormalize_data(ndata, stats):\n",
    "    ndata = (ndata + 1) / 2\n",
    "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
    "\n",
    "    # Set masked values to original values\n",
    "    mask = set_last_dimension(data, stats['mask'][0])\n",
    "    data = np.where(mask, data, ndata)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class StateDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A general torch Dataset you can drop in and use immediately with just about any trajectory .h5 data generated from ManiSkill.\n",
    "    This class simply is a simple starter code to load trajectory data easily, but does not do any data transformation or anything\n",
    "    advanced. We recommend you to copy this code directly and modify it for more advanced use cases\n",
    "\n",
    "    Args:\n",
    "        dataset_file (str): path to the .h5 file containing the data you want to load\n",
    "        load_count (int): the number of trajectories from the dataset to load into memory. If -1, will load all into memory\n",
    "        success_only (bool): whether to skip trajectories that are not successful in the end. Default is false\n",
    "        device: The location to save data to. If None will store as numpy (the default), otherwise will move data to that device\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataset_file: str, pred_horizon: int, obs_horizon: int, action_horizon:int, task_id: np.float32, load_count=-1, device=None\n",
    "    ) -> None:\n",
    "        self.dataset_file = dataset_file\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.obs_horizon = obs_horizon\n",
    "        self.action_horizon = action_horizon\n",
    "        self.task_id = task_id\n",
    "        self.device = device\n",
    "        self.data = h5py.File(dataset_file, \"r\")\n",
    "        json_path = dataset_file.replace(\".h5\", \".json\")\n",
    "        self.json_data = load_json(json_path)\n",
    "        self.episodes = self.json_data[\"episodes\"]\n",
    "        self.env_info = self.json_data[\"env_info\"]\n",
    "        self.env_id = self.env_info[\"env_id\"]\n",
    "        self.env_kwargs = self.env_info[\"env_kwargs\"]\n",
    "\n",
    "        self.obs = None\n",
    "        self.actions = []\n",
    "        self.terminated = []\n",
    "        self.truncated = []\n",
    "        self.success, self.fail, self.rewards = None, None, None\n",
    "        if load_count == -1:\n",
    "            load_count = len(self.episodes)\n",
    "        for eps_id in tqdm(range(load_count), desc=\"Loading Episodes\", colour=\"green\"):\n",
    "            eps = self.episodes[eps_id]\n",
    "            assert (\n",
    "                \"success\" in eps\n",
    "            ), \"episodes in this dataset do not have the success attribute, cannot load dataset with success_only=True\"\n",
    "            if not eps[\"success\"]:\n",
    "                continue\n",
    "            trajectory = self.data[f\"traj_{eps['episode_id']}\"]\n",
    "            trajectory = load_h5_data(trajectory)\n",
    "            eps_len = len(trajectory[\"actions\"])\n",
    "\n",
    "            # exclude the final observation as most learning workflows do not use it\n",
    "            obs = common.index_dict_array(trajectory[\"obs\"], slice(eps_len))\n",
    "            if eps_id == 0:\n",
    "                self.obs = obs\n",
    "            else:\n",
    "                self.obs = common.append_dict_array(self.obs, obs)\n",
    "\n",
    "            self.actions.append(trajectory[\"actions\"])\n",
    "            self.terminated.append(trajectory[\"terminated\"])\n",
    "            self.truncated.append(trajectory[\"truncated\"])\n",
    "\n",
    "            # handle data that might optionally be in the trajectory\n",
    "            if \"rewards\" in trajectory:\n",
    "                if self.rewards is None:\n",
    "                    self.rewards = [trajectory[\"rewards\"]]\n",
    "                else:\n",
    "                    self.rewards.append(trajectory[\"rewards\"])\n",
    "            if \"success\" in trajectory:\n",
    "                if self.success is None:\n",
    "                    self.success = [trajectory[\"success\"]]\n",
    "                else:\n",
    "                    self.success.append(trajectory[\"success\"])\n",
    "            if \"fail\" in trajectory:\n",
    "                if self.fail is None:\n",
    "                    self.fail = [trajectory[\"fail\"]]\n",
    "                else:\n",
    "                    self.fail.append(trajectory[\"fail\"])\n",
    "\n",
    "        self.actions = np.vstack(self.actions)\n",
    "        self.terminated = np.concatenate(self.terminated)\n",
    "        self.truncated = np.concatenate(self.truncated)\n",
    "        \n",
    "        self.truncated = np.zeros(self.actions.shape[0], dtype=bool)\n",
    "        self.truncated[-1] = True\n",
    "        \n",
    "        if self.rewards is not None:\n",
    "            self.rewards = np.concatenate(self.rewards)\n",
    "        if self.success is not None:\n",
    "            self.success = np.concatenate(self.success)\n",
    "        if self.fail is not None:\n",
    "            self.fail = np.concatenate(self.fail)\n",
    "\n",
    "        def remove_np_uint16(x: Union[np.ndarray, dict]):\n",
    "            if isinstance(x, dict):\n",
    "                for k in x.keys():\n",
    "                    x[k] = remove_np_uint16(x[k])\n",
    "                return x\n",
    "            else:\n",
    "                if x.dtype == np.uint16:\n",
    "                    return x.astype(np.int32)\n",
    "                return x\n",
    "\n",
    "        # uint16 dtype is used to conserve disk space and memory\n",
    "        # you can optimize this dataset code to keep it as uint16 and process that\n",
    "        # dtype of data yourself. for simplicity we simply cast to a int32 so\n",
    "        # it can automatically be converted to torch tensors without complaint\n",
    "        self.obs = remove_np_uint16(self.obs)\n",
    "        \n",
    "        if device is not None:\n",
    "            self.actions = common.to_tensor(self.actions, device=device)\n",
    "            self.obs = common.to_tensor(self.obs, device=device)\n",
    "            self.terminated = common.to_tensor(self.terminated, device=device)\n",
    "            self.truncated = common.to_tensor(self.truncated, device=device)\n",
    "            if self.rewards is not None:\n",
    "                self.rewards = common.to_tensor(self.rewards, device=device)\n",
    "            if self.success is not None:\n",
    "                self.success = common.to_tensor(self.terminated, device=device)\n",
    "            if self.fail is not None:\n",
    "                self.fail = common.to_tensor(self.truncated, device=device)\n",
    "        \n",
    "\n",
    "\n",
    "        # Added code for diffusion policy\n",
    "        obs_dict = get_observations(self.obs)\n",
    "        train_data = dict(\n",
    "                        obs=convert_observation(obs_dict, self.task_id),\n",
    "                        actions=self.actions,\n",
    "                        )\n",
    "\n",
    "         # Initialize index lists and stat dicts\n",
    "        self.indices = create_sample_indices(\n",
    "            episode_ends=self.truncated, \n",
    "            sequence_length=self.pred_horizon,\n",
    "            pad_before=self.obs_horizon - 1,\n",
    "            pad_after=self.action_horizon - 1\n",
    "        )\n",
    "\n",
    "        stats = dict()\n",
    "        normalized_train_data = dict()\n",
    "        for key, data in train_data.items():\n",
    "            if key == \"actions\":\n",
    "                stats[key] = get_data_stats(data)\n",
    "            else:\n",
    "                stats[key] = get_data_stats(data, True)\n",
    "            \n",
    "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
    "  \n",
    "        self.normalized_train_data = normalized_train_data\n",
    "        self.stats = stats\n",
    "\n",
    "    def __len__(self):\n",
    "        # all possible sequenzes of the dataset\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Change data to fit diffusion policy\n",
    "        buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx = self.indices[idx]\n",
    "\n",
    "    \n",
    "        sampled = sample_sequence(\n",
    "            train_data=self.normalized_train_data, \n",
    "            sequence_length=self.pred_horizon,\n",
    "            buffer_start_idx=buffer_start_idx,\n",
    "            buffer_end_idx=buffer_end_idx,\n",
    "            sample_start_idx=sample_start_idx,\n",
    "            sample_end_idx=sample_end_idx\n",
    "        )\n",
    "    \n",
    "        # discard unused observations in the sequence\n",
    "        for k in sampled.keys():\n",
    "            if k != \"actions\":\n",
    "                # discard unused observations in the sequence\n",
    "                sampled[k] = sampled[k][:self.obs_horizon,:]\n",
    "        sampled[k] = common.to_tensor(sampled[k], device=self.device)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class Downsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    '''\n",
    "        Conv1d --> GroupNorm --> Mish\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.GroupNorm(n_groups, out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ConditionalResidualBlock1D(nn.Module):\n",
    "    def __init__(self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            cond_dim,\n",
    "            kernel_size=3,\n",
    "            n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "        ])\n",
    "\n",
    "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
    "        # predicts per-channel scale and bias\n",
    "        cond_channels = out_channels * 2\n",
    "        self.out_channels = out_channels\n",
    "        self.cond_encoder = nn.Sequential(\n",
    "            nn.Mish(),\n",
    "            nn.Linear(cond_dim, cond_channels),\n",
    "            nn.Unflatten(-1, (-1, 1))\n",
    "        )\n",
    "\n",
    "        # make sure dimensions compatible\n",
    "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        '''\n",
    "            x : [ batch_size x in_channels x horizon ]\n",
    "            cond : [ batch_size x cond_dim]\n",
    "\n",
    "            returns:\n",
    "            out : [ batch_size x out_channels x horizon ]\n",
    "        '''\n",
    "        out = self.blocks[0](x)\n",
    "        embed = self.cond_encoder(cond)\n",
    "\n",
    "        embed = embed.reshape(\n",
    "            embed.shape[0], 2, self.out_channels, 1)\n",
    "        scale = embed[:,0,...]\n",
    "        bias = embed[:,1,...]\n",
    "        out = scale * out + bias\n",
    "\n",
    "        out = self.blocks[1](out)\n",
    "        out = out + self.residual_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConditionalUnet1D(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim,\n",
    "        global_cond_dim,\n",
    "        diffusion_step_embed_dim=256,\n",
    "        down_dims=[256,512,1024],\n",
    "        kernel_size=5,\n",
    "        n_groups=8\n",
    "        ):\n",
    "        \"\"\"\n",
    "        input_dim: Dim of actions.\n",
    "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
    "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
    "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
    "        down_dims: Channel size for each UNet level.\n",
    "          The length of this array determines numebr of levels.\n",
    "        kernel_size: Conv kernel size\n",
    "        n_groups: Number of groups for GroupNorm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        all_dims = [input_dim] + list(down_dims)\n",
    "        start_dim = down_dims[0]\n",
    "\n",
    "        dsed = diffusion_step_embed_dim\n",
    "        diffusion_step_encoder = nn.Sequential(\n",
    "            SinusoidalPosEmb(dsed),\n",
    "            nn.Linear(dsed, dsed * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(dsed * 4, dsed),\n",
    "        )\n",
    "        cond_dim = dsed + global_cond_dim\n",
    "\n",
    "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
    "        mid_dim = all_dims[-1]\n",
    "        self.mid_modules = nn.ModuleList([\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        down_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            down_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        up_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            up_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        final_conv = nn.Sequential(\n",
    "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
    "            nn.Conv1d(start_dim, input_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.diffusion_step_encoder = diffusion_step_encoder\n",
    "        self.up_modules = up_modules\n",
    "        self.down_modules = down_modules\n",
    "        self.final_conv = final_conv\n",
    "\n",
    "        print(\"number of parameters: {:e}\".format(\n",
    "            sum(p.numel() for p in self.parameters()))\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "            sample: torch.Tensor,\n",
    "            timestep: Union[torch.Tensor, float, int],\n",
    "            global_cond=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,input_dim)\n",
    "        timestep: (B,) or int, diffusion step\n",
    "        global_cond: (B,global_cond_dim)\n",
    "        output: (B,T,input_dim)\n",
    "        \"\"\"\n",
    "        # (B,T,C)\n",
    "        sample = sample.moveaxis(-1,-2)\n",
    "        # (B,C,T)\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        global_feature = self.diffusion_step_encoder(timesteps)\n",
    "\n",
    "        if global_cond is not None:\n",
    "            global_feature = torch.cat([\n",
    "                global_feature, global_cond\n",
    "            ], axis=-1)\n",
    "\n",
    "        x = sample\n",
    "        h = []\n",
    "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        for mid_module in self.mid_modules:\n",
    "            x = mid_module(x, global_feature)\n",
    "\n",
    "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # (B,C,T)\n",
    "        x = x.moveaxis(-1,-2)\n",
    "        # (B,T,C)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download demonstration data from Google Drive\n",
    "env_id = 'PickCube-v1'\n",
    "#env_id = 'StackCube-v1'\n",
    "#env_id = 'PegInsertionSide-v1'\n",
    "#env_id = 'PlugCharger-v1'\n",
    "#env_id = 'PushCube-v1'\n",
    "obs_mode = 'state_dict'\n",
    "control_mode = 'pd_joint_delta_pos'\n",
    "#control_mode = 'pd_ee_delta_pos'\n",
    "\n",
    "pred_horizon = 16\n",
    "obs_horizon = 2\n",
    "action_horizon = 8\n",
    "\n",
    "#==============================================================================\n",
    "\n",
    "task_id = {\n",
    "    'PickCube-v1': 0.0,\n",
    "    'StackCube-v1': 0.1,\n",
    "    'PegInsertionSide-v1': 0.2,\n",
    "    'PlugCharger-v1': 0.3,\n",
    "    'PushCube-v1': 0.4\n",
    "}\n",
    "\n",
    "dataset_path = f'/content/drive/MyDrive/Data/Generated/{env_id}/motionplanning/trajectory.{obs_mode}.{control_mode}.h5'\n",
    "model_path = f'drive/MyDrive/Data/Checkpoints/{env_id}_{control_mode}_model.pt'\n",
    "\n",
    "# create dataset from file\n",
    "dataset = StateDataset(\n",
    "    dataset_file=dataset_path,\n",
    "    pred_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    "    action_horizon=action_horizon,\n",
    "    task_id=task_id[env_id],\n",
    "    load_count=1000,\n",
    "    device=None\n",
    ")\n",
    "\n",
    "# create dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    num_workers=1,\n",
    "    # don't kill worker process afte each epoch\n",
    "    persistent_workers=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# visualize data in batch\n",
    "batch = next(iter(dataloader))\n",
    "print(batch.keys())\n",
    "print(\"observations:\", batch['obs'].shape, batch['obs'].dtype)\n",
    "print(\"actions:\", batch['actions'].shape, batch['actions'].dtype)\n",
    "    \n",
    "\n",
    "# observation and action dimensions corrsponding to the dataset\n",
    "obs_dim = batch['obs'].shape[-1]\n",
    "action_dim = batch['actions'].shape[-1]\n",
    "print(\"obs_dim:\", obs_dim)\n",
    "print(\"action_dim:\", action_dim)\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim*obs_horizon\n",
    ")\n",
    "\n",
    "# example inputs\n",
    "noised_action = torch.randn((1, pred_horizon, action_dim))\n",
    "obs = torch.zeros((1, obs_horizon, obs_dim))\n",
    "diffusion_iter = torch.zeros((1,))\n",
    "\n",
    "# the noise prediction network\n",
    "# takes noisy action, diffusion iteration and observation as input\n",
    "# predicts the noise added to action\n",
    "noise = noise_pred_net(\n",
    "    sample=noised_action,\n",
    "    timestep=diffusion_iter,\n",
    "    global_cond=obs.flatten(start_dim=1))\n",
    "\n",
    "# illustration of removing noise\n",
    "# the actual noise removal is performed by NoiseScheduler\n",
    "# and is dependent on the diffusion noise schedule\n",
    "denoised_action = noised_action - noise\n",
    "\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "num_diffusion_iters = 100\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    # the choise of beta schedule has big impact on performance\n",
    "    # we found squared cosine works the best\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    # clip output to [-1,1] to improve stability\n",
    "    clip_sample=True,\n",
    "    # our network predicts noise (instead of denoised action)\n",
    "    prediction_type='epsilon'\n",
    ")\n",
    "\n",
    "# device transfer\n",
    "device = torch.device(\"cuda\")\n",
    "_ = noise_pred_net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "# Exponential Moving Average\n",
    "# accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "ema = EMAModel(\n",
    "    parameters=noise_pred_net.parameters(),\n",
    "    power=0.75)\n",
    "\n",
    "# Standard ADAM optimizer\n",
    "# Note that EMA parametesr are not optimized\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=noise_pred_net.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    # epoch loop\n",
    "    for epoch_idx in tglobal:\n",
    "        epoch_loss = list()\n",
    "        # batch loop\n",
    "        with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
    "            for nbatch in tepoch:\n",
    "                # data normalized in dataset\n",
    "                # device transfer\n",
    "                nobs = nbatch['obs'].to(device)\n",
    "                naction = nbatch['actions'].to(device)\n",
    "                B = nobs.shape[0]\n",
    "\n",
    "                # observation as FiLM conditioning\n",
    "                # (B, obs_horizon, obs_dim)\n",
    "                obs_cond = nobs[:,:obs_horizon,:]\n",
    "                # (B, obs_horizon * obs_dim)\n",
    "                obs_cond = obs_cond.flatten(start_dim=1)\n",
    "\n",
    "                # sample noise to add to actions\n",
    "                noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "                # sample a diffusion iteration for each data point\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps,\n",
    "                    (B,), device=device\n",
    "                ).long()\n",
    "\n",
    "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_actions = noise_scheduler.add_noise(\n",
    "                    naction, noise, timesteps)\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = noise_pred_net(\n",
    "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "                # L2 loss\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "                # optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # step lr scheduler every batch\n",
    "                # this is different from standard pytorch behavior\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # update Exponential Moving Average of the model weights\n",
    "                ema.step(noise_pred_net.parameters())\n",
    "\n",
    "                # logging\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
    "\n",
    "# Weights of the EMA model\n",
    "# is used for inference\n",
    "ema_noise_pred_net = noise_pred_net\n",
    "ema.copy_to(ema_noise_pred_net.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': ema_noise_pred_net.state_dict(),\n",
    "    'ema_model_state_dict': ema.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "    'epoch': epoch_idx,\n",
    "    'loss': loss, # Save the current epoch\n",
    "}, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(model_path, map_location='cuda')\n",
    "ema_noise_pred_net = noise_pred_net\n",
    "ema_noise_pred_net.load_state_dict(state_dict['model_state_dict'])\n",
    "print('Pretrained weights loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit enviornment interaction to 200 steps before termination\n",
    "env = gym.make(env_id, obs_mode=obs_mode, control_mode=control_mode, render_mode='rgb_array')\n",
    "\n",
    "max_steps = 200\n",
    "\n",
    "# reset \n",
    "obs, info = env.reset()\n",
    "obs = get_observations(obs)\n",
    "obs = convert_observation(obs, task_id[env_id])\n",
    "\n",
    "# save observations\n",
    "obs_deque = collections.deque([obs] * obs_horizon, maxlen=obs_horizon)\n",
    "\n",
    "# save visualization\n",
    "imgs = []\n",
    "rewards = []\n",
    "done = False\n",
    "step_idx = 0\n",
    "\n",
    "\n",
    "with tqdm(total=max_steps, desc=\"Eval\") as pbar:\n",
    "    while not done:\n",
    "        B = 1\n",
    "        # stack the last obs_horizon (2) number of observations\n",
    "        obs_seq = np.stack(obs_deque)\n",
    "        # normalize observation\n",
    "       \n",
    "        nobs = normalize_data(obs_seq, stats=dataset.stats['obs'])\n",
    "        # device transfer\n",
    "        nobs = torch.from_numpy(nobs).to(device, dtype=torch.float32)\n",
    "\n",
    "        # infer action\n",
    "        with torch.no_grad():\n",
    "            # reshape observation to (B,obs_horizon*obs_dim)\n",
    "            obs_cond = nobs.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "            # initialize action from Guassian noise\n",
    "            noisy_action = torch.randn(\n",
    "                (B, pred_horizon, action_dim), device=device)\n",
    "            naction = noisy_action\n",
    "\n",
    "            # init scheduler\n",
    "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "\n",
    "            for k in noise_scheduler.timesteps:\n",
    "                # predict noise\n",
    "                noise_pred = ema_noise_pred_net(\n",
    "                    sample=naction,\n",
    "                    timestep=k,\n",
    "                    global_cond=obs_cond\n",
    "                )\n",
    "\n",
    "                # inverse diffusion step (remove noise)\n",
    "                naction = noise_scheduler.step(\n",
    "                    model_output=noise_pred,\n",
    "                    timestep=k,\n",
    "                    sample=naction\n",
    "                ).prev_sample\n",
    "\n",
    "        # unnormalize action\n",
    "        naction = naction.detach().to('cpu').numpy()\n",
    "        # (B, pred_horizon, action_dim)\n",
    "        naction = naction[0]\n",
    "       \n",
    "        action_pred = unnormalize_data(naction, stats=dataset.stats['actions'])\n",
    "\n",
    "        # only take action_horizon number of actions\n",
    "        start = obs_horizon - 1\n",
    "        end = start + action_horizon\n",
    "        action = action_pred[start:end,:]\n",
    "\n",
    "        # execute action_horizon number of steps\n",
    "        # without replanning\n",
    "        for i in range(len(action)):\n",
    "            # stepping env\n",
    "            obs, reward, done, _, info = env.step(action[i])\n",
    "\n",
    "            # process observation\n",
    "            # From the observation dictionary, we concatenate all the observations\n",
    "            # as done in the training data\n",
    "            obs = get_observations(obs)\n",
    "            obs = convert_observation(obs, task_id[env_id])\n",
    "\n",
    "            # save observations\n",
    "            obs_deque.append(obs)\n",
    "\n",
    "            # and reward/vis\n",
    "            rewards.append(reward)\n",
    "            imgs.append(env.render())\n",
    "\n",
    "            # update progress bar\n",
    "            step_idx += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(reward=reward)\n",
    "            if step_idx > max_steps:\n",
    "                done = True\n",
    "            if done:\n",
    "                break\n",
    "# print out the maximum target coverage\n",
    "print('Score: ', max(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display, Image as IPImage\n",
    "import io\n",
    "\n",
    "images = [Image.fromarray(img.squeeze(0).cpu().numpy()) for img in imgs]\n",
    "\n",
    "# Save to a bytes buffer\n",
    "buffer = io.BytesIO()\n",
    "images[0].save(buffer, format='GIF', save_all=True, append_images=images[1:], optimize=False, duration=50, loop=0)\n",
    "buffer.seek(0)\n",
    "\n",
    "# Save to a file\n",
    "with open(f'drive/MyDrive/Data/Results/{env_id}_{control_mode}animation.gif', 'wb') as f:\n",
    "    f.write(buffer.getvalue())\n",
    "\n",
    "# Display the GIF (optional)\n",
    "display(IPImage(data=buffer.getvalue()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
