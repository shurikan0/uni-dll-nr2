{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the package\n",
    "%pip install --upgrade mani_skill\n",
    "# install a version of torch that is compatible with your system\n",
    "%pip install torch torchvision torchaudio numpy diffusers\n",
    "\n",
    "\n",
    "# etc imports\n",
    "from typing import Tuple, Sequence, Dict, Union, Optional\n",
    "from collections import OrderedDict\n",
    "import collections\n",
    "import math\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display, Image as IPImage\n",
    "import io\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# mani_skill imports\n",
    "from mani_skill.utils import common\n",
    "from mani_skill.utils.io_utils import load_json\n",
    "from mani_skill.utils.common import flatten_state_dict\n",
    "import mani_skill.envs\n",
    "\n",
    "#torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# diffuser imports\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "# gym imports\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# google colab imports\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class Downsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    '''\n",
    "        Conv1d --> GroupNorm --> Mish\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.GroupNorm(n_groups, out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ConditionalResidualBlock1D(nn.Module):\n",
    "    def __init__(self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            cond_dim,\n",
    "            kernel_size=3,\n",
    "            n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "        ])\n",
    "\n",
    "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
    "        # predicts per-channel scale and bias\n",
    "        cond_channels = out_channels * 2\n",
    "        self.out_channels = out_channels\n",
    "        self.cond_encoder = nn.Sequential(\n",
    "            nn.Mish(),\n",
    "            nn.Linear(cond_dim, cond_channels),\n",
    "            nn.Unflatten(-1, (-1, 1))\n",
    "        )\n",
    "\n",
    "        # make sure dimensions compatible\n",
    "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        '''\n",
    "            x : [ batch_size x in_channels x horizon ]\n",
    "            cond : [ batch_size x cond_dim]\n",
    "\n",
    "            returns:\n",
    "            out : [ batch_size x out_channels x horizon ]\n",
    "        '''\n",
    "        out = self.blocks[0](x)\n",
    "        embed = self.cond_encoder(cond)\n",
    "\n",
    "        embed = embed.reshape(\n",
    "            embed.shape[0], 2, self.out_channels, 1)\n",
    "        scale = embed[:,0,...]\n",
    "        bias = embed[:,1,...]\n",
    "        out = scale * out + bias\n",
    "\n",
    "        out = self.blocks[1](out)\n",
    "        out = out + self.residual_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConditionalUnet1D(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim,\n",
    "        global_cond_dim,\n",
    "        diffusion_step_embed_dim=256,\n",
    "        down_dims=[256,512,1024],\n",
    "        kernel_size=5,\n",
    "        n_groups=8\n",
    "        ):\n",
    "        \"\"\"\n",
    "        input_dim: Dim of actions.\n",
    "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
    "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
    "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
    "        down_dims: Channel size for each UNet level.\n",
    "          The length of this array determines numebr of levels.\n",
    "        kernel_size: Conv kernel size\n",
    "        n_groups: Number of groups for GroupNorm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        all_dims = [input_dim] + list(down_dims)\n",
    "        start_dim = down_dims[0]\n",
    "\n",
    "        dsed = diffusion_step_embed_dim\n",
    "        diffusion_step_encoder = nn.Sequential(\n",
    "            SinusoidalPosEmb(dsed),\n",
    "            nn.Linear(dsed, dsed * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(dsed * 4, dsed),\n",
    "        )\n",
    "        cond_dim = dsed + global_cond_dim\n",
    "\n",
    "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
    "        mid_dim = all_dims[-1]\n",
    "        self.mid_modules = nn.ModuleList([\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        down_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            down_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        up_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            up_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        final_conv = nn.Sequential(\n",
    "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
    "            nn.Conv1d(start_dim, input_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.diffusion_step_encoder = diffusion_step_encoder\n",
    "        self.up_modules = up_modules\n",
    "        self.down_modules = down_modules\n",
    "        self.final_conv = final_conv\n",
    "\n",
    "        print(\"number of parameters: {:e}\".format(\n",
    "            sum(p.numel() for p in self.parameters()))\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "            sample: torch.Tensor,\n",
    "            timestep: Union[torch.Tensor, float, int],\n",
    "            global_cond=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,input_dim)\n",
    "        timestep: (B,) or int, diffusion step\n",
    "        global_cond: (B,global_cond_dim)\n",
    "        output: (B,T,input_dim)\n",
    "        \"\"\"\n",
    "        # (B,T,C)\n",
    "        sample = sample.moveaxis(-1,-2)\n",
    "        # (B,C,T)\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        global_feature = self.diffusion_step_encoder(timesteps)\n",
    "\n",
    "        if global_cond is not None:\n",
    "            global_feature = torch.cat([\n",
    "                global_feature, global_cond\n",
    "            ], axis=-1)\n",
    "\n",
    "        x = sample\n",
    "        h = []\n",
    "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        for mid_module in self.mid_modules:\n",
    "            x = mid_module(x, global_feature)\n",
    "\n",
    "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # (B,C,T)\n",
    "        x = x.moveaxis(-1,-2)\n",
    "        # (B,T,C)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================CHANGE=========================================\n",
    "env_id = 'PickCube-v1'\n",
    "#env_id = 'StackCube-v1'\n",
    "#env_id = 'PegInsertionSide-v2'\n",
    "#env_id = 'PlugCharger-v2'\n",
    "#env_id = 'PushCube-v1'\n",
    "obs_mode = 'state_dict'\n",
    "control_mode = 'pd_joint_delta_pos'\n",
    "\n",
    "pred_horizon = 16\n",
    "obs_horizon = 2\n",
    "action_horizon = 8\n",
    "\n",
    "#======================================CHANGE========================================\n",
    "\n",
    "task_id = {\n",
    "    'PickCube-v1': 0.0,\n",
    "    'StackCube-v1': 0.1,\n",
    "    'PegInsertionSide-v2': 0.2,\n",
    "    'PlugCharger-v2': 0.3,\n",
    "    'PushCube-v1': 0.4\n",
    "}\n",
    "\n",
    "base_path = '/content/drive/MyDrive/Data'\n",
    "generated_path = f'{base_path}/Generated/{env_id}/motionplanning'\n",
    "checkpoints_path = f'{base_path}/Checkpoints/{env_id}'\n",
    "results_path = f'{base_path}/Results/{env_id}'\n",
    "\n",
    "train_dataset_path = f'{generated_path}/training.{obs_mode}.{control_mode}.h5'\n",
    "val_dataset_path = f'{generated_path}/validation.{obs_mode}.{control_mode}.h5'\n",
    "model_path = f'{checkpoints_path}/model.pt'\n",
    "loss_path = f'{results_path}/loss.npz'\n",
    "plot_path = f'{results_path}/plot.png'\n",
    "animation_path = f'{results_path}/animation.gif'\n",
    "\n",
    "obs_dim = 40\n",
    "action_dim = 8\n",
    "print(\"obs_dim:\", obs_dim)\n",
    "print(\"action_dim:\", action_dim)\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim*obs_horizon\n",
    ")\n",
    "\n",
    "state_dict = torch.load(model_path, map_location='cuda')\n",
    "noise_pred_net.load_state_dict(state_dict['model_state_dict'])\n",
    "stats = state_dict['stats']\n",
    "print('Pretrained weights loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_id, obs_mode=obs_mode, control_mode=control_mode, render_mode='rgb_array')\n",
    "\n",
    "max_steps = 400\n",
    "\n",
    "num_episodes = 50\n",
    "mean_success = 0 \n",
    "mean_reward = 0\n",
    "rewards = []\n",
    "csv_file = f\"{results_path}/results.csv\"\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Episode', 'Max Reward', 'Success'])\n",
    "    print(f\"Opened file {csv_file} for writing.\")\n",
    "\n",
    "    with tqdm(range(num_episodes), desc='Epoch') as episodes:\n",
    "\n",
    "        for episode in episodes:\n",
    "            \n",
    "            # reset \n",
    "            obs, info = env.reset()\n",
    "            obs = get_observations(obs)\n",
    "            obs = convert_observation(obs, task_id[env_id])\n",
    "\n",
    "            # save observations\n",
    "            obs_deque = collections.deque([obs] * obs_horizon, maxlen=obs_horizon)\n",
    "\n",
    "            # save visualization\n",
    "            imgs = []\n",
    "            rewards = []\n",
    "            done = False\n",
    "            step_idx = 0\n",
    "            unsuccessful = False\n",
    "\n",
    "\n",
    "            with tqdm(total=max_steps, desc=\"Eval\", leave=False) as pbar:\n",
    "                while not done:\n",
    "                    B = 1\n",
    "                    # stack the last obs_horizon (2) number of observations\n",
    "                    obs_seq = np.stack(obs_deque)\n",
    "                \n",
    "                    nobs = normalize_batch({'obs': torch.tensor(obs_seq, dtype=torch.float32)}, min_vals, max_vals, exclude_features)\n",
    "                \n",
    "                    # device transfer\n",
    "                    #nobs = torch.from_numpy(nobs).to(device, dtype=torch.float32)\n",
    "                    nobs= nobs.to(device)\n",
    "\n",
    "                    # infer action\n",
    "                    with torch.no_grad():\n",
    "                        # reshape observation to (B,obs_horizon*obs_dim)\n",
    "                        obs_cond = nobs.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "                        # initialize action from Guassian noise\n",
    "                        noisy_action = torch.randn(\n",
    "                            (B, pred_horizon, action_dim), device=device)\n",
    "                        naction = noisy_action\n",
    "\n",
    "                        # init scheduler\n",
    "                        noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "\n",
    "                        for k in noise_scheduler.timesteps:\n",
    "                            # predict noise\n",
    "                            noise_pred = ema_noise_pred_net(\n",
    "                                sample=naction,\n",
    "                                timestep=k,\n",
    "                                global_cond=obs_cond\n",
    "                            )\n",
    "\n",
    "                            # inverse diffusion step (remove noise)\n",
    "                            naction = noise_scheduler.step(\n",
    "                                model_output=noise_pred,\n",
    "                                timestep=k,\n",
    "                                sample=naction\n",
    "                            ).prev_sample\n",
    "\n",
    "                    # unnormalize action\n",
    "                    naction = naction.detach().to('cpu').numpy()\n",
    "                    # (B, pred_horizon, action_dim)\n",
    "                    action_pred = naction[0] # we dont have to denormalize the action\n",
    "\n",
    "                    # only take action_horizon number of actions\n",
    "                    start = obs_horizon - 1\n",
    "                    end = start + action_horizon\n",
    "                    action = action_pred[start:end,:]\n",
    "\n",
    "                    # execute action_horizon number of steps\n",
    "                    # without replanning\n",
    "                    for i in range(len(action)):\n",
    "                        # stepping env\n",
    "                        obs, reward, done, _, info = env.step(action[i])\n",
    "\n",
    "                        # process observation\n",
    "                        # From the observation dictionary, we concatenate all the observations\n",
    "                        # as done in the training data\n",
    "                        obs = get_observations(obs)\n",
    "                        obs = convert_observation(obs, task_id[env_id])\n",
    "\n",
    "                        # save observations\n",
    "                        obs_deque.append(obs)\n",
    "\n",
    "                        # and reward/vis\n",
    "                        rewards.append(reward)\n",
    "                        imgs.append(env.render())\n",
    "\n",
    "                        # update progress bar\n",
    "                        step_idx += 1\n",
    "                        pbar.update(1)\n",
    "                        pbar.set_postfix(reward=reward)\n",
    "                        if step_idx > max_steps:\n",
    "                          \n",
    "                            done = True\n",
    "                            unsuccessful = True\n",
    "                        if done:\n",
    "                            break\n",
    "            \n",
    "            if not unsuccessful:\n",
    "                mean_success += 1\n",
    "            mean_reward += max(rewards)\n",
    "            writer.writerow([episode + 1, max(rewards), int(not unsuccessful)])\n",
    "            episodes.set_postfix(\n",
    "                reward=mean_reward / (episode + 1),\n",
    "                success=mean_success / (episode + 1)\n",
    "            )\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    print(\"Reward: \", mean_reward / num_episodes)\n",
    "    print(\"Success: \", mean_success/num_episodes)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
