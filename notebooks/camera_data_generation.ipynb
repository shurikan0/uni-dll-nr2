{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the package\n",
    "%pip install --upgrade mani_skill\n",
    "# install a version of torch that is compatible with your system\n",
    "%pip install torch torchvision torchaudio numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mani_skill.utils import sapien_utils\n",
    "from mani_skill.sensors.camera import CameraConfig\n",
    "import numpy as np\n",
    "\n",
    "pose = sapien_utils.look_at(eye=[0.3, 0, 0.6], target=[-0.1, 0, 0.1])\n",
    "pose2 = sapien_utils.look_at(eye=[0.0, -0.3, 0.6], target=[-0.1, 0, 0.1])\n",
    "camera1 = CameraConfig(\"front_camera\", pose, 128, 128, np.pi / 2, 0.01, 100)\n",
    "camera2 = CameraConfig(\"side_camera\", pose2, 128, 128, np.pi / 2, 0.01, 100)\n",
    "camera_list = [camera1, camera2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ManiSkill Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "import mani_skill.envs.utils.randomization as randomization\n",
    "from mani_skill.agents.robots import Fetch, Panda, Xmate3Robotiq\n",
    "from mani_skill.envs.sapien_env import BaseEnv\n",
    "from mani_skill.sensors.camera import CameraConfig\n",
    "from mani_skill.utils import sapien_utils\n",
    "from mani_skill.utils.building import actors\n",
    "from mani_skill.utils.registration import register_env\n",
    "from mani_skill.utils.scene_builder.table import TableSceneBuilder\n",
    "from mani_skill.utils.structs.pose import Pose\n",
    "env_id = \"CustomEnv\"\n",
    "\n",
    "@register_env(env_id, max_episode_steps=50)\n",
    "class CustomEnv(BaseEnv):\n",
    "    SUPPORTED_ROBOTS = [\"panda\", \"xmate3_robotiq\", \"fetch\"]\n",
    "    agent: Union[Panda, Xmate3Robotiq, Fetch]\n",
    "    cube_half_size = 0.02\n",
    "    goal_thresh = 0.025\n",
    "\n",
    "    def __init__(self, *args, robot_uids=\"panda\", robot_init_qpos_noise=0.02, **kwargs):\n",
    "        self.robot_init_qpos_noise = robot_init_qpos_noise\n",
    "        super().__init__(*args, robot_uids=robot_uids, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _default_sensor_configs(self):\n",
    "        return camera_list\n",
    "\n",
    "    @property\n",
    "    def _default_human_render_camera_configs(self):\n",
    "        pose = sapien_utils.look_at([0.6, 0.7, 0.6], [0.0, 0.0, 0.35])\n",
    "        return CameraConfig(\"render_camera\", pose, 512, 512, 1, 0.01, 100)\n",
    "\n",
    "    def _load_scene(self, options: dict):\n",
    "        self.table_scene = TableSceneBuilder(\n",
    "            self, robot_init_qpos_noise=self.robot_init_qpos_noise\n",
    "        )\n",
    "        self.table_scene.build()\n",
    "        self.cube = actors.build_cube(\n",
    "            self.scene, half_size=self.cube_half_size, color=[1, 0, 0, 1], name=\"cube\"\n",
    "        )\n",
    "        self.goal_site = actors.build_sphere(\n",
    "            self.scene,\n",
    "            radius=self.goal_thresh,\n",
    "            color=[0, 1, 0, 1],\n",
    "            name=\"goal_site\",\n",
    "            body_type=\"kinematic\",\n",
    "            add_collision=False,\n",
    "        )\n",
    "        self._hidden_objects.append(self.goal_site)\n",
    "\n",
    "    def _initialize_episode(self, env_idx: torch.Tensor, options: dict):\n",
    "        with torch.device(self.device):\n",
    "            b = len(env_idx)\n",
    "            self.table_scene.initialize(env_idx)\n",
    "            xyz = torch.zeros((b, 3))\n",
    "            xyz[:, :2] = torch.rand((b, 2)) * 0.2 - 0.1\n",
    "            xyz[:, 2] = self.cube_half_size\n",
    "            qs = randomization.random_quaternions(b, lock_x=True, lock_y=True)\n",
    "            self.cube.set_pose(Pose.create_from_pq(xyz, qs))\n",
    "\n",
    "            goal_xyz = torch.zeros((b, 3))\n",
    "            goal_xyz[:, :2] = torch.rand((b, 2)) * 0.2 - 0.1\n",
    "            goal_xyz[:, 2] = torch.rand((b)) * 0.3 + xyz[:, 2]\n",
    "            self.goal_site.set_pose(Pose.create_from_pq(goal_xyz))\n",
    "\n",
    "    def _get_obs_extra(self, info: Dict):\n",
    "        # in reality some people hack is_grasped into observations by checking if the gripper can close fully or not\n",
    "        obs = dict(\n",
    "            is_grasped=info[\"is_grasped\"],\n",
    "            tcp_pose=self.agent.tcp.pose.raw_pose,\n",
    "            goal_pos=self.goal_site.pose.p,\n",
    "        )\n",
    "        if \"state\" in self.obs_mode:\n",
    "            obs.update(\n",
    "                obj_pose=self.cube.pose.raw_pose,\n",
    "                tcp_to_obj_pos=self.cube.pose.p - self.agent.tcp.pose.p,\n",
    "                obj_to_goal_pos=self.goal_site.pose.p - self.cube.pose.p,\n",
    "            )\n",
    "        return obs\n",
    "\n",
    "    def evaluate(self):\n",
    "        is_obj_placed = (\n",
    "            torch.linalg.norm(self.goal_site.pose.p - self.cube.pose.p, axis=1)\n",
    "            <= self.goal_thresh\n",
    "        )\n",
    "        is_grasped = self.agent.is_grasping(self.cube)\n",
    "        is_robot_static = self.agent.is_static(0.2)\n",
    "        return {\n",
    "            \"success\": is_obj_placed & is_robot_static,\n",
    "            \"is_obj_placed\": is_obj_placed,\n",
    "            \"is_robot_static\": is_robot_static,\n",
    "            \"is_grasped\": is_grasped,\n",
    "        }\n",
    "\n",
    "    def compute_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):\n",
    "        tcp_to_obj_dist = torch.linalg.norm(\n",
    "            self.cube.pose.p - self.agent.tcp.pose.p, axis=1\n",
    "        )\n",
    "        reaching_reward = 1 - torch.tanh(5 * tcp_to_obj_dist)\n",
    "        reward = reaching_reward\n",
    "\n",
    "        is_grasped = info[\"is_grasped\"]\n",
    "        reward += is_grasped\n",
    "\n",
    "        obj_to_goal_dist = torch.linalg.norm(\n",
    "            self.goal_site.pose.p - self.cube.pose.p, axis=1\n",
    "        )\n",
    "        place_reward = 1 - torch.tanh(5 * obj_to_goal_dist)\n",
    "        reward += place_reward * is_grasped\n",
    "\n",
    "        static_reward = 1 - torch.tanh(\n",
    "            5 * torch.linalg.norm(self.agent.robot.get_qvel()[..., :-2], axis=1)\n",
    "        )\n",
    "        reward += static_reward * info[\"is_obj_placed\"]\n",
    "\n",
    "        reward[info[\"success\"]] = 5\n",
    "        return reward\n",
    "\n",
    "    def compute_normalized_dense_reward(\n",
    "        self, obs: Any, action: torch.Tensor, info: Dict\n",
    "    ):\n",
    "        return self.compute_dense_reward(obs=obs, action=action, info=info) / 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PANDA Planner (change for new env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sapien\n",
    "\n",
    "\n",
    "from mani_skill.examples.motionplanning.panda.motionplanner import \\\n",
    "    PandaArmMotionPlanningSolver\n",
    "from mani_skill.examples.motionplanning.panda.utils import (\n",
    "    compute_grasp_info_by_obb, get_actor_obb)\n",
    "\n",
    "def solve(env: CustomEnv, seed=None, debug=False, vis=False):\n",
    "    env.reset(seed=seed)\n",
    "    planner = PandaArmMotionPlanningSolver(\n",
    "        env,\n",
    "        debug=debug,\n",
    "        vis=vis,\n",
    "        base_pose=env.unwrapped.agent.robot.pose,\n",
    "        visualize_target_grasp_pose=vis,\n",
    "        print_env_info=False,\n",
    "    )\n",
    "\n",
    "    FINGER_LENGTH = 0.025\n",
    "    env = env.unwrapped\n",
    "\n",
    "    # retrieves the object oriented bounding box (trimesh box object)\n",
    "    obb = get_actor_obb(env.cube)\n",
    "\n",
    "    approaching = np.array([0, 0, -1])\n",
    "    # get transformation matrix of the tcp pose, is default batched and on torch\n",
    "    target_closing = env.agent.tcp.pose.to_transformation_matrix()[0, :3, 1].cpu().numpy()\n",
    "    # we can build a simple grasp pose using this information for Panda\n",
    "    grasp_info = compute_grasp_info_by_obb(\n",
    "        obb,\n",
    "        approaching=approaching,\n",
    "        target_closing=target_closing,\n",
    "        depth=FINGER_LENGTH,\n",
    "    )\n",
    "    closing, center = grasp_info[\"closing\"], grasp_info[\"center\"]\n",
    "    grasp_pose = env.agent.build_grasp_pose(approaching, closing, env.cube.pose.sp.p)\n",
    "\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # Reach\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    reach_pose = grasp_pose * sapien.Pose([0, 0, -0.05])\n",
    "    planner.move_to_pose_with_screw(reach_pose)\n",
    "\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # Grasp\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    planner.move_to_pose_with_screw(grasp_pose)\n",
    "    planner.close_gripper()\n",
    "\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # Move to goal pose\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    goal_pose = sapien.Pose(env.goal_site.pose.sp.p, grasp_pose.q)\n",
    "    res = planner.move_to_pose_with_screw(goal_pose)\n",
    "\n",
    "    planner.close()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os.path as osp\n",
    "from mani_skill.utils.wrappers.record import RecordEpisode\n",
    "\n",
    "\n",
    "def generate(num_traj, file_path, file_name, save_video):\n",
    "    file_name = file_name + str(num_traj)\n",
    "    env = gym.make(\n",
    "        env_id,\n",
    "        obs_mode=\"none\",\n",
    "        control_mode=\"pd_joint_pos\",\n",
    "        render_mode=\"rgb_array\",\n",
    "        reward_mode=\"dense\",\n",
    "        shader_dir=\"default\",\n",
    "        sim_backend=\"auto\"\n",
    "    )\n",
    "  \n",
    "    env = RecordEpisode(\n",
    "        env,\n",
    "        output_dir=osp.join(file_path),\n",
    "        trajectory_name=file_name, save_video=False,\n",
    "        source_type=\"motionplanning\",\n",
    "        source_desc=\"official motion planning solution from ManiSkill contributors\",\n",
    "        video_fps=30,\n",
    "        save_on_reset=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Motion Planning Running on {env_id}\")\n",
    "    pbar = tqdm(range(num_traj))\n",
    "    seed = 0\n",
    "    successes = []\n",
    "    passed = 0\n",
    "    while True:\n",
    "        res = solve(env, seed=seed, debug=False, vis=False)\n",
    "        if res == -1:\n",
    "            success = False\n",
    "        else:\n",
    "            success = res[-1][\"success\"].item()\n",
    "        successes.append(success)\n",
    "        if not success:\n",
    "            seed += 1\n",
    "            env.flush_trajectory(save=False)\n",
    "            if save_video:\n",
    "                env.flush_video(save=False)\n",
    "            continue\n",
    "        else:\n",
    "            env.flush_trajectory()\n",
    "            if save_video:\n",
    "                env.flush_video()\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(dict(success_rate=np.mean(successes)))\n",
    "            seed += 1\n",
    "            passed += 1\n",
    "            if passed == num_traj:\n",
    "                break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Replay the trajectory stored in HDF5.\n",
    "The replayed trajectory can use different observation modes and control modes.\n",
    "We support translating actions from certain controllers to a limited number of controllers.\n",
    "The script is only tested for Panda, and may include some Panda-specific hardcode.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Union\n",
    "\n",
    "import gymnasium as gym\n",
    "import h5py\n",
    "import numpy as np\n",
    "import sapien\n",
    "from tqdm.auto import tqdm\n",
    "from transforms3d.quaternions import quat2axangle\n",
    "\n",
    "import mani_skill.envs\n",
    "from mani_skill.agents.controllers import *\n",
    "from mani_skill.agents.controllers.base_controller import CombinedController\n",
    "from mani_skill.envs.sapien_env import BaseEnv\n",
    "from mani_skill.trajectory import utils as trajectory_utils\n",
    "from mani_skill.trajectory.merge_trajectory import merge_h5\n",
    "from mani_skill.utils import common, gym_utils, io_utils, wrappers\n",
    "from mani_skill.utils.structs.link import Link\n",
    "\n",
    "\n",
    "def qpos_to_pd_joint_delta_pos(controller: PDJointPosController, qpos):\n",
    "    assert type(controller) == PDJointPosController\n",
    "    assert controller.config.use_delta\n",
    "    assert controller.config.normalize_action\n",
    "    delta_qpos = qpos - controller.qpos.cpu().numpy()[0]\n",
    "    low, high = controller.config.lower, controller.config.upper\n",
    "    return gym_utils.inv_scale_action(delta_qpos, low, high)\n",
    "\n",
    "\n",
    "def qpos_to_pd_joint_target_delta_pos(controller: PDJointPosController, qpos):\n",
    "    assert type(controller) == PDJointPosController\n",
    "    assert controller.config.use_delta\n",
    "    assert controller.config.use_target\n",
    "    assert controller.config.normalize_action\n",
    "    delta_qpos = qpos - controller._target_qpos.cpu().numpy()[0]\n",
    "    low, high = controller.config.lower, controller.config.upper\n",
    "    return gym_utils.inv_scale_action(delta_qpos, low, high)\n",
    "\n",
    "\n",
    "def qpos_to_pd_joint_vel(controller: PDJointVelController, qpos):\n",
    "    assert type(controller) == PDJointVelController\n",
    "    assert controller.config.normalize_action\n",
    "    delta_qpos = qpos - controller.qpos.cpu().numpy()[0]\n",
    "    qvel = delta_qpos * controller._control_freq\n",
    "    low, high = controller.config.lower, controller.config.upper\n",
    "    return gym_utils.inv_scale_action(qvel, low, high)\n",
    "\n",
    "\n",
    "def compact_axis_angle_from_quaternion(quat: np.ndarray) -> np.ndarray:\n",
    "    theta, omega = quat2axangle(quat)\n",
    "    # - 2 * np.pi to make the angle symmetrical around 0\n",
    "    if omega > np.pi:\n",
    "        omega = omega - 2 * np.pi\n",
    "    return omega * theta\n",
    "\n",
    "\n",
    "def delta_pose_to_pd_ee_delta(\n",
    "    controller: Union[PDEEPoseController, PDEEPosController],\n",
    "    delta_pose: sapien.Pose,\n",
    "    pos_only=False,\n",
    "):\n",
    "    assert isinstance(controller, PDEEPosController)\n",
    "    assert controller.config.use_delta\n",
    "    assert controller.config.normalize_action\n",
    "    low, high = controller.action_space_low, controller.action_space_high\n",
    "    if pos_only:\n",
    "        return gym_utils.inv_scale_action(delta_pose.p, low, high)\n",
    "    delta_pose = np.r_[\n",
    "        delta_pose.p,\n",
    "        compact_axis_angle_from_quaternion(delta_pose.q),\n",
    "    ]\n",
    "    return gym_utils.inv_scale_action(delta_pose, low, high)\n",
    "\n",
    "\n",
    "def from_pd_joint_pos_to_ee(\n",
    "    output_mode: str,\n",
    "    ori_actions,\n",
    "    ori_env: BaseEnv,\n",
    "    env: BaseEnv,\n",
    "    render=False,\n",
    "    pbar=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    n = len(ori_actions)\n",
    "    if pbar is not None:\n",
    "        pbar.reset(total=n)\n",
    "\n",
    "    ori_controller: CombinedController = ori_env.agent.controller\n",
    "    controller: CombinedController = env.agent.controller\n",
    "    assert (\n",
    "        \"arm\" in ori_controller.controllers\n",
    "    ), \"Could not find the controller for the robot arm. This controller conversion tool requires there to be a key called 'arm' in the controller\"\n",
    "    ori_arm_controller: PDJointPosController = ori_controller.controllers[\"arm\"]\n",
    "    arm_controller: PDEEPoseController = controller.controllers[\"arm\"]\n",
    "    assert isinstance(arm_controller, PDEEPoseController) or isinstance(\n",
    "        arm_controller, PDEEPosController\n",
    "    ), \"the arm controller must inherit PDEEPoseController or PDEEPosController\"\n",
    "    assert arm_controller.config.frame in [\n",
    "        \"root_translation:root_aligned_body_rotation\",\n",
    "        \"root_translation\",\n",
    "    ], \"Currently only support the 'root_translation:root_aligned_body_rotation' ee control frame for delta pose control and 'root_translation' ee control frame for delta pos control\"\n",
    "    ori_ee_link = ori_env.agent.robot.links_map[arm_controller.ee_link.name]\n",
    "    ee_link: Link = arm_controller.ee_link\n",
    "    pos_only = arm_controller.config.frame == \"root_translation\"\n",
    "\n",
    "    info = {}\n",
    "\n",
    "    for t in range(n):\n",
    "        if pbar is not None:\n",
    "            pbar.update()\n",
    "\n",
    "        ori_action = ori_actions[t]\n",
    "        ori_action_dict = ori_controller.to_action_dict(ori_action)\n",
    "        output_action_dict = ori_action_dict.copy()  # do not in-place modify\n",
    "        ori_env.step(ori_action)\n",
    "        flag = True\n",
    "\n",
    "        for _ in range(4):\n",
    "            delta_q = [1, 0, 0, 0]\n",
    "            if \"root_translation\" in arm_controller.config.frame:\n",
    "                delta_position = ori_ee_link.pose.p - ee_link.pose.p\n",
    "            if \"root_aligned_body_rotation\" in arm_controller.config.frame:\n",
    "                delta_q = (ee_link.pose * ori_ee_link.pose.inv()).q.cpu().numpy()[0]\n",
    "            delta_pose = sapien.Pose(delta_position.cpu().numpy()[0], delta_q)\n",
    "            arm_action = delta_pose_to_pd_ee_delta(\n",
    "                arm_controller, delta_pose, pos_only=pos_only\n",
    "            )\n",
    "            if (np.abs(arm_action[:3])).max() > 1:  # position clipping\n",
    "                if verbose:\n",
    "                    tqdm.write(f\"Position action is clipped: {arm_action[:3]}\")\n",
    "                arm_action[:3] = np.clip(arm_action[:3], -1, 1)\n",
    "                flag = False\n",
    "            if not pos_only:\n",
    "                if np.linalg.norm(arm_action[3:]) > 1:  # rotation clipping\n",
    "                    if verbose:\n",
    "                        tqdm.write(f\"Rotation action is clipped: {arm_action[3:]}\")\n",
    "                    arm_action[3:] = arm_action[3:] / np.linalg.norm(arm_action[3:])\n",
    "                    flag = False\n",
    "\n",
    "            output_action_dict[\"arm\"] = arm_action\n",
    "            output_action = controller.from_action_dict(output_action_dict)\n",
    "\n",
    "            _, _, _, _, info = env.step(output_action)\n",
    "            if render:\n",
    "                env.render_human()\n",
    "\n",
    "            if flag:\n",
    "                break\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "def from_pd_joint_pos(\n",
    "    output_mode,\n",
    "    ori_actions,\n",
    "    ori_env: BaseEnv,\n",
    "    env: BaseEnv,\n",
    "    render=False,\n",
    "    pbar=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    if \"ee\" in output_mode:\n",
    "        return from_pd_joint_pos_to_ee(**locals())\n",
    "\n",
    "    n = len(ori_actions)\n",
    "    if pbar is not None:\n",
    "        pbar.reset(total=n)\n",
    "\n",
    "    ori_controller: CombinedController = ori_env.agent.controller\n",
    "    controller: CombinedController = env.agent.controller\n",
    "\n",
    "    info = {}\n",
    "\n",
    "    for t in range(n):\n",
    "        if pbar is not None:\n",
    "            pbar.update()\n",
    "\n",
    "        ori_action = ori_actions[t]\n",
    "        ori_action_dict = ori_controller.to_action_dict(ori_action)\n",
    "        output_action_dict = ori_action_dict.copy()  # do not in-place modify\n",
    "\n",
    "        ori_env.step(ori_action)\n",
    "        flag = True\n",
    "\n",
    "        for _ in range(2):\n",
    "            if output_mode == \"pd_joint_delta_pos\":\n",
    "                arm_action = qpos_to_pd_joint_delta_pos(\n",
    "                    controller.controllers[\"arm\"], ori_action_dict[\"arm\"]\n",
    "                )\n",
    "            elif output_mode == \"pd_joint_target_delta_pos\":\n",
    "                arm_action = qpos_to_pd_joint_target_delta_pos(\n",
    "                    controller.controllers[\"arm\"], ori_action_dict[\"arm\"]\n",
    "                )\n",
    "            elif output_mode == \"pd_joint_vel\":\n",
    "                arm_action = qpos_to_pd_joint_vel(\n",
    "                    controller.controllers[\"arm\"], ori_action_dict[\"arm\"]\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    f\"Does not support converting pd_joint_pos to {output_mode}\"\n",
    "                )\n",
    "\n",
    "            # Assume normalized action\n",
    "            if np.max(np.abs(arm_action)) > 1 + 1e-3:\n",
    "                if verbose:\n",
    "                    tqdm.write(f\"Arm action is clipped: {arm_action}\")\n",
    "                flag = False\n",
    "            arm_action = np.clip(arm_action, -1, 1)\n",
    "            output_action_dict[\"arm\"] = arm_action\n",
    "\n",
    "            output_action = controller.from_action_dict(output_action_dict)\n",
    "            _, _, _, _, info = env.step(output_action)\n",
    "            if render:\n",
    "                env.render_human()\n",
    "\n",
    "            if flag:\n",
    "                break\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "def from_pd_joint_delta_pos(\n",
    "    output_mode,\n",
    "    ori_actions,\n",
    "    ori_env: BaseEnv,\n",
    "    env: BaseEnv,\n",
    "    render=False,\n",
    "    pbar=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    n = len(ori_actions)\n",
    "    if pbar is not None:\n",
    "        pbar.reset(total=n)\n",
    "\n",
    "    ori_controller: CombinedController = ori_env.agent.controller\n",
    "    controller: CombinedController = env.agent.controller\n",
    "    ori_arm_controller: PDJointPosController = ori_controller.controllers[\"arm\"]\n",
    "\n",
    "    assert output_mode == \"pd_joint_pos\", output_mode\n",
    "    assert ori_arm_controller.config.normalize_action\n",
    "    low, high = ori_arm_controller.config.lower, ori_arm_controller.config.upper\n",
    "\n",
    "    info = {}\n",
    "\n",
    "    for t in range(n):\n",
    "        if pbar is not None:\n",
    "            pbar.update()\n",
    "\n",
    "        ori_action = ori_actions[t]\n",
    "        ori_action_dict = ori_controller.to_action_dict(ori_action)\n",
    "        output_action_dict = ori_action_dict.copy()  # do not in-place modify\n",
    "\n",
    "        prev_arm_qpos = ori_arm_controller.qpos\n",
    "        delta_qpos = gym_utils.clip_and_scale_action(ori_action_dict[\"arm\"], low, high)\n",
    "        arm_action = prev_arm_qpos + delta_qpos\n",
    "\n",
    "        ori_env.step(ori_action)\n",
    "\n",
    "        output_action_dict[\"arm\"] = arm_action\n",
    "        output_action = controller.from_action_dict(output_action_dict)\n",
    "        _, _, _, _, info = env.step(output_action)\n",
    "\n",
    "        if render:\n",
    "            env.render_human()\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "\n",
    "def _replay(args, proc_id: int = 0, num_procs=1, pbar=None):\n",
    "    pbar = tqdm(position=proc_id, leave=None, unit=\"step\", dynamic_ncols=True)\n",
    "\n",
    "    # Load HDF5 containing trajectories\n",
    "    traj_path = args.traj_path\n",
    "    ori_h5_file = h5py.File(traj_path, \"r\")\n",
    "\n",
    "    # Load associated json\n",
    "    json_path = traj_path.replace(\".h5\", \".json\")\n",
    "    json_data = io_utils.load_json(json_path)\n",
    "\n",
    "    env_info = json_data[\"env_info\"]\n",
    "    env_id = env_info[\"env_id\"]\n",
    "    ori_env_kwargs = env_info[\"env_kwargs\"]\n",
    "\n",
    "    # Create a twin env with the original kwargs\n",
    "    if args.target_control_mode is not None:\n",
    "        if args.sim_backend:\n",
    "            ori_env_kwargs[\"sim_backend\"] = args.sim_backend\n",
    "        ori_env = gym.make(env_id, **ori_env_kwargs)\n",
    "    else:\n",
    "        ori_env = None\n",
    "\n",
    "    # Create a main env for replay\n",
    "    target_obs_mode = args.obs_mode\n",
    "    target_control_mode = args.target_control_mode\n",
    "    env_kwargs = ori_env_kwargs.copy()\n",
    "    if target_obs_mode is not None:\n",
    "        env_kwargs[\"obs_mode\"] = target_obs_mode\n",
    "    if target_control_mode is not None:\n",
    "        env_kwargs[\"control_mode\"] = target_control_mode\n",
    "    env_kwargs[\"shader_dir\"] = args.shader\n",
    "    env_kwargs[\"reward_mode\"] = args.reward_mode\n",
    "    env_kwargs[\n",
    "        \"render_mode\"\n",
    "    ] = (\n",
    "        args.render_mode\n",
    "    )  # note this only affects the videos saved as RecordEpisode wrapper calls env.render\n",
    "\n",
    "    # handle warnings/errors for replaying trajectories generated during GPU simulation\n",
    "    if \"num_envs\" in env_kwargs:\n",
    "        if env_kwargs[\"num_envs\"] > 1:\n",
    "            raise RuntimeError(\n",
    "                \"\"\"Cannot replay trajectories that were generated in a GPU\n",
    "            simulation with more than one environment. To replay trajectories generated during GPU simulation,\n",
    "            make sure to set num_envs=1 and sim_backend=\"gpu\" in the env kwargs.\"\"\"\n",
    "            )\n",
    "        if \"sim_backend\" in env_kwargs:\n",
    "            # if sim backend is \"gpu\", we change it to CPU if ray tracing shader is used as RT is not supported yet on GPU sim backends\n",
    "            # TODO (stao): remove this if we ever support RT on GPU sim.\n",
    "            if args.shader[:2] == \"rt\":\n",
    "                env_kwargs[\"sim_backend\"] = \"cpu\"\n",
    "\n",
    "    if args.sim_backend:\n",
    "        env_kwargs[\"sim_backend\"] = args.sim_backend\n",
    "    env = gym.make(env_id, **env_kwargs)\n",
    "    if pbar is not None:\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"control_mode\": env_kwargs.get(\"control_mode\"),\n",
    "                \"obs_mode\": env_kwargs.get(\"obs_mode\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Prepare for recording\n",
    "    output_dir = os.path.dirname(traj_path)\n",
    "    ori_traj_name = os.path.splitext(os.path.basename(traj_path))[0]\n",
    "    suffix = \"{}.{}\".format(env.obs_mode, env.control_mode)\n",
    "    new_traj_name = ori_traj_name + \".\" + suffix\n",
    "    if num_procs > 1:\n",
    "        new_traj_name = new_traj_name + \".\" + str(proc_id)\n",
    "    env = wrappers.RecordEpisode(\n",
    "        env,\n",
    "        output_dir,\n",
    "        save_on_reset=False,\n",
    "        save_trajectory=args.save_traj,\n",
    "        trajectory_name=new_traj_name,\n",
    "        save_video=args.save_video,\n",
    "        video_fps=args.video_fps,\n",
    "        record_reward=args.record_rewards,\n",
    "    )\n",
    "\n",
    "    if env.save_trajectory:\n",
    "        output_h5_path = env._h5_file.filename\n",
    "        assert not os.path.samefile(output_h5_path, traj_path)\n",
    "    else:\n",
    "        output_h5_path = None\n",
    "\n",
    "    episodes = json_data[\"episodes\"][: args.count]\n",
    "    n_ep = len(episodes)\n",
    "    inds = np.arange(n_ep)\n",
    "    inds = np.array_split(inds, num_procs)[proc_id]\n",
    "\n",
    "    # Replay\n",
    "    for ind in inds:\n",
    "        ep = episodes[ind]\n",
    "        episode_id = ep[\"episode_id\"]\n",
    "        traj_id = f\"traj_{episode_id}\"\n",
    "        if pbar is not None:\n",
    "            pbar.set_description(f\"Replaying {traj_id}\")\n",
    "\n",
    "        if traj_id not in ori_h5_file:\n",
    "            tqdm.write(f\"{traj_id} does not exist in {traj_path}\")\n",
    "            continue\n",
    "\n",
    "        reset_kwargs = ep[\"reset_kwargs\"].copy()\n",
    "        if \"seed\" in reset_kwargs:\n",
    "            assert reset_kwargs[\"seed\"] == ep[\"episode_seed\"]\n",
    "        else:\n",
    "            reset_kwargs[\"seed\"] = ep[\"episode_seed\"]\n",
    "        seed = reset_kwargs.pop(\"seed\")\n",
    "\n",
    "        ori_control_mode = ep[\"control_mode\"]\n",
    "\n",
    "        for _ in range(args.max_retry + 1):\n",
    "            # Each trial for each trajectory to replay, we reset the environment\n",
    "            # and optionally set the first environment state\n",
    "            env.reset(seed=seed, **reset_kwargs)\n",
    "            if ori_env is not None:\n",
    "                ori_env.reset(seed=seed, **reset_kwargs)\n",
    "\n",
    "            # set first environment state and update recorded env state\n",
    "            if args.use_first_env_state or args.use_env_states:\n",
    "                ori_env_states = trajectory_utils.dict_to_list_of_dicts(\n",
    "                    ori_h5_file[traj_id][\"env_states\"]\n",
    "                )\n",
    "                if ori_env is not None:\n",
    "                    ori_env.set_state_dict(ori_env_states[0])\n",
    "                env.base_env.set_state_dict(ori_env_states[0])\n",
    "                ori_env_states = ori_env_states[1:]\n",
    "                if args.save_traj:\n",
    "                    # replace the first saved env state\n",
    "                    # since we set state earlier and RecordEpisode will save the reset to state.\n",
    "                    def recursive_replace(x, y):\n",
    "                        if isinstance(x, np.ndarray):\n",
    "                            x[-1, :] = y[-1, :]\n",
    "                        else:\n",
    "                            for k in x.keys():\n",
    "                                recursive_replace(x[k], y[k])\n",
    "\n",
    "                    recursive_replace(\n",
    "                        env._trajectory_buffer.state, common.batch(ori_env_states[0])\n",
    "                    )\n",
    "                    fixed_obs = env.base_env.get_obs()\n",
    "                    recursive_replace(\n",
    "                        env._trajectory_buffer.observation,\n",
    "                        common.to_numpy(common.batch(fixed_obs)),\n",
    "                    )\n",
    "            # Original actions to replay\n",
    "            ori_actions = ori_h5_file[traj_id][\"actions\"][:]\n",
    "            info = {}\n",
    "\n",
    "            # Without conversion between control modes\n",
    "            assert not (\n",
    "                target_control_mode is not None and args.use_env_states\n",
    "            ), \"Cannot use env states when trying to \\\n",
    "                convert from one control mode to another. This is because control mode conversion causes there to be changes \\\n",
    "                in how many actions are taken to achieve the same states\"\n",
    "            if target_control_mode is None:\n",
    "                n = len(ori_actions)\n",
    "                if pbar is not None:\n",
    "                    pbar.reset(total=n)\n",
    "                for t, a in enumerate(ori_actions):\n",
    "                    if pbar is not None:\n",
    "                        pbar.update()\n",
    "                    _, _, _, truncated, info = env.step(a)\n",
    "                    if args.use_env_states:\n",
    "                        env.base_env.set_state_dict(ori_env_states[t])\n",
    "                    if args.vis:\n",
    "                        env.base_env.render_human()\n",
    "\n",
    "            # From joint position to others\n",
    "            elif ori_control_mode == \"pd_joint_pos\":\n",
    "                info = from_pd_joint_pos(\n",
    "                    target_control_mode,\n",
    "                    ori_actions,\n",
    "                    ori_env,\n",
    "                    env,\n",
    "                    render=args.vis,\n",
    "                    pbar=pbar,\n",
    "                    verbose=args.verbose,\n",
    "                )\n",
    "\n",
    "            # From joint delta position to others\n",
    "            elif ori_control_mode == \"pd_joint_delta_pos\":\n",
    "                info = from_pd_joint_delta_pos(\n",
    "                    target_control_mode,\n",
    "                    ori_actions,\n",
    "                    ori_env,\n",
    "                    env,\n",
    "                    render=args.vis,\n",
    "                    pbar=pbar,\n",
    "                    verbose=args.verbose,\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    f\"Script currently does not support converting {ori_control_mode} to {target_control_mode}\"\n",
    "                )\n",
    "\n",
    "            success = info.get(\"success\", False)\n",
    "            if args.discard_timeout:\n",
    "                success = success and (not truncated)\n",
    "\n",
    "            if success or args.allow_failure:\n",
    "                if args.save_traj:\n",
    "                    env.flush_trajectory()\n",
    "                if args.save_video:\n",
    "                    env.flush_video(ignore_empty_transition=False)\n",
    "                break\n",
    "            else:\n",
    "                if args.verbose:\n",
    "                    print(\"info\", info)\n",
    "        else:\n",
    "            tqdm.write(f\"Episode {episode_id} is not replayed successfully. Skipping\")\n",
    "\n",
    "    # Cleanup\n",
    "    env.close()\n",
    "    if ori_env is not None:\n",
    "        ori_env.close()\n",
    "    ori_h5_file.close()\n",
    "\n",
    "    if pbar is not None:\n",
    "        pbar.close()\n",
    "\n",
    "    return output_h5_path\n",
    "\n",
    "\n",
    "def replay(args):\n",
    "    if args.num_procs > 1:\n",
    "        pool = mp.Pool(args.num_procs)\n",
    "        proc_args = [(deepcopy(args), i, args.num_procs) for i in range(args.num_procs)]\n",
    "        res = pool.starmap(_replay, proc_args)\n",
    "        pool.close()\n",
    "        if args.save_traj:\n",
    "            # A hack to find the path\n",
    "            output_path = res[0][: -len(\"0.h5\")] + \"h5\"\n",
    "            merge_h5(output_path, res)\n",
    "            for h5_path in res:\n",
    "                tqdm.write(f\"Remove {h5_path}\")\n",
    "                os.remove(h5_path)\n",
    "                json_path = h5_path.replace(\".h5\", \".json\")\n",
    "                tqdm.write(f\"Remove {json_path}\")\n",
    "                os.remove(json_path)\n",
    "    else:\n",
    "        _replay(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_traj = 10\n",
    "file_path = \"/content/drive/MyDrive/Data/Generated/\"\n",
    "file_name = \"pickcube2camera\"\n",
    "save_video = False\n",
    "\n",
    "generate(num_traj,file_path, file_name, save_video)\n",
    "\n",
    "traj_path = file_path + \"pickcube2camera10.h5\"\n",
    "\n",
    "args = {\n",
    "    \"traj_path\": \"/content/drive/MyDrive/Data/Generated/pickcube2camera10.h5\",\n",
    "    \"save_traj\": True,\n",
    "    \"obs_mode\": \"rgbd\",\n",
    "    \"sim_backend\": \"gpu\",\n",
    "    \"num_procs\": 1,\n",
    "    \"max_retry\": 0,\n",
    "    \"vis\": False,\n",
    "    \"verbose\": False,\n",
    "    \"count\": 10,\n",
    "    \"render_mode\": \"rgb_array\",\n",
    "}\n",
    "mp.set_start_method(\"spawn\")\n",
    "replay(args)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
